import time
from datetime import datetime, timedelta
from pydantic import BaseModel, Field
from typing import List, Dict, Any
import statistics
from bilibili_api import search, user, video, sync, Credential
from core.tool_registry import ToolResult

# ==========================================
# ğŸ‘‡ Bilibili å‡­è¯é…ç½®
# ==========================================
MY_SESSDATA = "131f13c1%2C1774179487%2C71799%2A92CjCW6GJ9HMc5hsa6xLNDpAkgFfU2sVsdN5QHM80H5FLxZjJK92balhkRVVZ46j6g0SVnZrT3pLREwzbUc3RFV0cFg1M0RLQjBKbGFlUnRFcVROOUUtLV9MY0lwWHlyMk9GN1F6RmRXWGhpRWdoWGZkZHFBeV9Mek04cVNxa0JENzh4c2dkOG9nIIEC"
MY_BUVID3 = "0AD3C626-7C49-A0E2-976E-C27C6E11DEA772471infoc"

class BilibiliSearchInput(BaseModel):
    keyword: str = Field(..., description="æœç´¢å…³é”®è¯")
    limit: int = Field(15, description="è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼ˆè¿”å›ç»™è°ƒç”¨è€…ï¼‰")
    sort_by: str = Field("comprehensive", description="æ’åºç­–ç•¥: comprehensive/click/dm/pubdate/stow/scores")
    days: int = Field(30, description="ä»…ä¿ç•™æœ€è¿‘Nå¤©å†…çš„æŠ•ç¨¿ï¼Œ0è¡¨ç¤ºä¸è¿‡æ»¤")
    fetch_size: int = Field(50, description="å®é™…ä»APIè·å–çš„æ•°é‡ï¼ˆè¿‡æ»¤å‰ï¼‰")

class BilibiliMonitorInput(BaseModel):
    user_id: str = Field(..., description="Bç«™ç”¨æˆ·ID (mid)")
    limit: int = Field(10, description="è¿”å›ç»“æœæ•°é‡é™åˆ¶")

class BilibiliAdapter:
    def __init__(self):
        # åˆå§‹åŒ–å‡­è¯
        if MY_SESSDATA:
            self.credential = Credential(sessdata=MY_SESSDATA, buvid3=MY_BUVID3)
            print("[Bilibili] âœ… å·²åŠ è½½ç”¨æˆ·å‡­è¯")
        else:
            self.credential = None
            print("[Bilibili] âš ï¸ è­¦å‘Šï¼šè¿è¡Œåœ¨æ¸¸å®¢æ¨¡å¼")

    def search_videos(self, params: BilibiliSearchInput) -> ToolResult:
        """
        ğŸ¯ æ™ºèƒ½åˆ†é¡µ + çˆ†æ¬¾è¯„åˆ†æœç´¢

        ç­–ç•¥:
        1. æ™ºèƒ½åˆ†é¡µï¼ˆæ’­æ”¾é‡ä¸‹é™å°±åœæ­¢ï¼‰
        2. è®¡ç®—çˆ†æ¬¾åˆ†å¹¶æ’åº
        3. å¯¹top Nè¡¥å……è¯¦ç»†ä¿¡æ¯

        Returns:
            ToolResult: çˆ†æ¬¾ä¼˜å…ˆçš„æœç´¢ç»“æœ
        """
        print(f"[Bilibili] ğŸ” æ™ºèƒ½æœç´¢: {params.keyword}")

        try:
            order_type = self._resolve_order(params.sort_by)

            # === é˜¶æ®µ1: æ™ºèƒ½åˆ†é¡µè·å– ===
            print(f"ğŸ“„ [é˜¶æ®µ1] æ™ºèƒ½åˆ†é¡µæ‰«æï¼ˆæœ€å¤š {params.fetch_size} æ¡ï¼‰...")
            raw_videos = self._smart_pagination(
                params.keyword,
                order_type,
                target_count=params.fetch_size
            )

            if not raw_videos:
                print("[Bilibili] âŒ æœªè·å–åˆ°æ•°æ®")
                return ToolResult(
                    status="success",
                    data=[],
                    summary=f"No videos found for '{params.keyword}'"
                )

            print(f"âœ… [é˜¶æ®µ1] æ‰«æåˆ° {len(raw_videos)} æ¡åŸºç¡€æ•°æ®")

            # === é˜¶æ®µ2: çˆ†æ¬¾è¯„åˆ†ä¸æ’åº ===
            print(f"ğŸ¯ [é˜¶æ®µ2] è®¡ç®—çˆ†æ¬¾åˆ†ï¼ˆè¯¦ç»†å¤„ç† {params.limit} æ¡ï¼‰...")
            scored_videos = self._score_and_rank_viral(raw_videos, params.days, params.keyword)  # ğŸ”‘ ä¼ é€’keyword

            if not scored_videos:
                print("[Bilibili] âš ï¸ æ‰€æœ‰è§†é¢‘è¢«è¿‡æ»¤")
                return ToolResult(
                    status="success",
                    data=[],
                    summary=f"All videos filtered out for '{params.keyword}'"
                )

            print(f"âœ… [é˜¶æ®µ2] çˆ†æ¬¾æ’åºå®Œæˆï¼Œtop {params.limit} è¯†åˆ«")

            # === é˜¶æ®µ3: è¯¦ç»†ä¿¡æ¯è¡¥å…… ===
            print(f"ğŸ“Š [é˜¶æ®µ3] è¡¥å…… top {params.limit} è¯¦ç»†ä¿¡æ¯...")
            top_videos = scored_videos[:params.limit]
            enriched_videos = self._enrich_details(top_videos)

            print(f"âœ… [Bilibili] å®Œæˆï¼æ‰«æ {len(raw_videos)} æ¡ â†’ è¿”å› {len(enriched_videos)} æ¡çˆ†æ¬¾")

            return ToolResult(
                status="success",
                data=enriched_videos,
                summary=f"Found {len(enriched_videos)} viral videos on Bilibili for '{params.keyword}' (scanned {len(raw_videos)} total)"
            )

        except Exception as e:
            print(f"[Bilibili] âŒ æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return ToolResult(status="error", error=str(e), summary=f"Bilibili search failed: {e}")

    def _smart_pagination(self, keyword: str, order_type, target_count: int = 50) -> List[Dict]:
        """
        æ™ºèƒ½åˆ†é¡µ: æ’­æ”¾é‡ä¸‹é™å°±åœæ­¢

        ç­–ç•¥:
        - å¦‚æœå½“å‰é¡µå¹³å‡æ’­æ”¾é‡ < ä¸Šä¸€é¡µ * 50%ï¼Œæå‰åœæ­¢
        - èŠ‚çœæ— æ•ˆè¯·æ±‚
        """
        collected = []
        current_page = 1
        max_pages = 8  # æœ€å¤š8é¡µ
        last_avg_views = None  # ğŸ”‘ ä¿®å¤: æ”¹ä¸ºNoneï¼Œé¿å…ç¬¬ä¸€é¡µè¢«è¯¯åˆ¤ä¸º"ä¸‹é™"

        while len(collected) < target_count and current_page <= max_pages:
            print(f"   æŠ“å–ç¬¬ {current_page} é¡µ...")

            try:
                # ğŸ”‘ ä¿®å¤: search_by_type ä¸æ¥å— credential å‚æ•°
                # æœç´¢åŠŸèƒ½åœ¨æ¸¸å®¢æ¨¡å¼ä¸‹ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ
                results = sync(search.search_by_type(
                    keyword=keyword,
                    search_type=search.SearchObjectType.VIDEO,
                    order_type=order_type,
                    page=current_page,
                    page_size=20
                ))
            except Exception as e:
                print(f"   âŒ ç¬¬ {current_page} é¡µè¯·æ±‚å¤±è´¥: {e}")
                break

            if 'result' not in results or not results['result']:
                print(f"   ğŸ›‘ ç¬¬ {current_page} é¡µæ— æ•°æ®ï¼Œåœæ­¢")
                break

            raw_list = results['result']
            page_videos = []

            for v in raw_list:
                if v.get('type') != 'video':
                    continue

                item = self._parse_basic_video(v)
                page_videos.append(item)
                collected.append(item)

            # è®¡ç®—æœ¬é¡µå¹³å‡æ’­æ”¾é‡
            if page_videos:
                current_avg = statistics.mean([v['view_count'] for v in page_videos])

                # ğŸ”‘ åªåœ¨æœ‰å†å²æ•°æ®æ—¶æ£€æŸ¥è´¨é‡ä¸‹é™
                if last_avg_views is not None:
                    # æ’­æ”¾é‡å¤§å¹…ä¸‹é™ï¼Œè¯´æ˜è´¨é‡å˜å·®
                    if current_avg < last_avg_views * 0.4:  # ä¸‹é™è¶…è¿‡60%
                        print(f"   âš ï¸ ç¬¬ {current_page} é¡µè´¨é‡ä¸‹é™ï¼ˆ{current_avg:.0f} vs {last_avg_views:.0f}ï¼‰ï¼Œæå‰åœæ­¢")
                        break

                last_avg_views = current_avg
                print(f"   âœ… ç¬¬ {current_page} é¡µè·å– {len(page_videos)} æ¡ï¼ˆå¹³å‡æ’­æ”¾: {current_avg:.0f}ï¼‰")

            current_page += 1

            # é˜²æ­¢é€Ÿç‡é™åˆ¶
            if current_page <= max_pages:
                time.sleep(1.0)  # 1ç§’å»¶è¿Ÿ

        print(f"   ğŸ“Š æ™ºèƒ½åˆ†é¡µå®Œæˆï¼š{current_page-1} é¡µï¼Œå…± {len(collected)} æ¡")
        return collected

    def _parse_basic_video(self, v: Dict) -> Dict:
        """è§£æå•æ¡è§†é¢‘çš„åŸºç¡€æ•°æ®"""
        pub_ts = v.get('pubdate', 0)
        pub_date = datetime.fromtimestamp(pub_ts).strftime('%Y-%m-%d')

        # æ¸…æ´—æ ‡é¢˜HTML
        raw_title = v.get('title', '')
        clean_title = raw_title.replace('<em class="keyword">', '').replace('</em>', '')

        # ğŸ”‘ ä¿®å¤: å¤„ç†durationå­—ç¬¦ä¸²æ ¼å¼ (å¦‚ "5:30" -> 330ç§’)
        duration_raw = v.get('duration', 0)
        duration_seconds = self._parse_duration(duration_raw)

        return {
            "platform": "bilibili",
            "source_type": "search",
            "title": clean_title,
            "url": f"https://www.bilibili.com/video/{v.get('bvid')}",
            "bvid": v.get('bvid'),
            "author_name": v.get('author', ''),
            "author_id": str(v.get('mid', '')),
            "publish_time": pub_date,
            "pub_ts": pub_ts,  # ä¿ç•™æ—¶é—´æˆ³ç”¨äºè®¡ç®—
            "view_count": v.get('play', 0),
            "interaction": v.get('favorites', 0) + v.get('review', 0),
            "duration": duration_seconds,  # è½¬æ¢ä¸ºç§’æ•°
            "raw_data": v
        }

    def _score_and_rank_viral(self, videos: List[Dict], days: int = 30, keyword: str = "") -> List[Dict]:
        """
        è®¡ç®—çˆ†æ¬¾åˆ†å¹¶æ’åº

        çˆ†æ¬¾åˆ†ç®—æ³•:
        viral_score = (æ’­æ”¾é‡ç›¸å¯¹è¡¨ç°) * (æ—¶é—´æ–°é²œåº¦) * (äº’åŠ¨ç‡) * (æ—¶é•¿æƒé‡) * (ç›¸å…³æ€§æƒé‡)
        """
        if not videos:
            return []

        # è®¡ç®—å¹³å‡æ’­æ”¾é‡
        valid_views = [v['view_count'] for v in videos if v['view_count'] > 0]
        avg_views = statistics.mean(valid_views) if valid_views else 1

        now = datetime.now()
        scored_videos = []

        for video in videos:
            # æ’­æ”¾é‡ç›¸å¯¹è¡¨ç°
            view_ratio = video['view_count'] / max(avg_views, 1)

            # æ—¶é—´æ–°é²œåº¦
            freshness = 1.0
            days_old = 0

            if video.get('pub_ts'):
                try:
                    pub_dt = datetime.fromtimestamp(video['pub_ts'])
                    days_old = (now - pub_dt).days

                    # è¶…è¿‡æ—¶é—´èŒƒå›´ï¼Œè·³è¿‡
                    if days_old > days:
                        continue

                    # æ—¶é—´è¡°å‡
                    if days_old <= 3:
                        freshness = 1.5
                    elif days_old <= 7:
                        freshness = 1.3
                    elif days_old <= 14:
                        freshness = 1.0
                    else:
                        freshness = 0.8

                except:
                    freshness = 1.0

            # äº’åŠ¨ç‡
            engagement_rate = 1.0
            if video['view_count'] > 0:
                engagement_rate = 1.0 + (video['interaction'] / video['view_count']) * 10

            # æ—¶é•¿æƒé‡ï¼ˆ5-15åˆ†é’Ÿæ˜¯é»„é‡‘æ—¶é•¿ï¼‰
            duration_weight = 1.0
            duration_min = video.get('duration', 0) / 60
            if 5 <= duration_min <= 15:
                duration_weight = 1.2
            elif duration_min < 2:
                duration_weight = 0.6
            elif duration_min > 30:
                duration_weight = 0.8

            # ğŸ”‘ æ–°å¢ï¼šç›¸å…³æ€§æƒé‡
            relevance_weight = self._calculate_relevance(video.get('title', ''), keyword)

            # ç»¼åˆçˆ†æ¬¾åˆ†ï¼ˆåŠ å…¥ç›¸å…³æ€§æƒé‡ï¼‰
            viral_score = view_ratio * freshness * engagement_rate * duration_weight * relevance_weight

            video['viral_score'] = viral_score
            video['days_old'] = days_old
            scored_videos.append(video)

        # æŒ‰çˆ†æ¬¾åˆ†æ’åº
        scored_videos.sort(key=lambda x: x['viral_score'], reverse=True)

        # è¾“å‡ºtop 3çš„çˆ†æ¬¾åˆ†
        if len(scored_videos) >= 3:
            print(f"   Top 3 çˆ†æ¬¾åˆ†: {scored_videos[0]['viral_score']:.2f}, {scored_videos[1]['viral_score']:.2f}, {scored_videos[2]['viral_score']:.2f}")

        return scored_videos

    def _enrich_details(self, videos: List[Dict]) -> List[Dict]:
        """
        ä¸ºtop Nè§†é¢‘è¡¥å……è¯¦ç»†ä¿¡æ¯

        è¡¥å……:
        - è¯¦ç»†ç»Ÿè®¡ï¼ˆç¡¬å¸ã€åˆ†äº«ã€æ”¶è—ï¼‰
        - æ ‡ç­¾
        - æè¿°
        """
        if not videos:
            return []

        enriched = []

        for i, vid in enumerate(videos):
            try:
                bvid = vid.get('bvid')
                if not bvid:
                    enriched.append(vid)
                    continue

                print(f"   è¡¥å……è¯¦æƒ… {i+1}/{len(videos)}: {vid['title'][:30]}...")

                # è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆinfo ä¸­å·²åŒ…å« stat æ•°æ®ï¼‰
                v = video.Video(bvid=bvid, credential=self.credential)
                info = sync(v.get_info())

                # ä» info ä¸­æå– statï¼ˆä¸å†å•ç‹¬è°ƒç”¨ get_statï¼Œå› ä¸ºè¯¥ API å·²è¢« Bç«™ç¦ç”¨ï¼‰
                stats = info.get('stat', {})

                # åˆå¹¶æ•°æ®
                enriched_video = {
                    **vid,  # ä¿ç•™åŸºç¡€æ•°æ®
                    "view_count": stats.get('view', vid['view_count']),
                    "likes": stats.get('like', 0),
                    "coins": stats.get('coin', 0),
                    "favorites": stats.get('favorite', 0),
                    "shares": stats.get('share', 0),
                    "comments": stats.get('reply', 0),
                    "danmaku": stats.get('danmaku', 0),
                    "interaction": stats.get('like', 0) + stats.get('coin', 0) + stats.get('favorite', 0),
                    "raw_data": {
                        "description": info.get('desc', '')[:200],
                        "tid": info.get('tid'),
                        "tname": info.get('tname'),
                        "pic": info.get('pic'),
                    }
                }

                # å°è¯•è·å–æ ‡ç­¾
                try:
                    tags = sync(v.get_tags())
                    enriched_video['raw_data']['tags'] = [t.get('tag_name', '') for t in tags[:10]]
                except:
                    enriched_video['raw_data']['tags'] = []

                enriched.append(enriched_video)

                # é€Ÿç‡æ§åˆ¶
                time.sleep(0.5)

            except Exception as e:
                print(f"   âš ï¸ è¡¥å……å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä¿ç•™åŸºç¡€æ•°æ®
                enriched.append(vid)

        return enriched

    def monitor_user(self, params: BilibiliMonitorInput) -> ToolResult:
        """
        ğŸ¯ åšä¸»è´¨é‡è¯„åˆ†ç›‘æ§

        ç­–ç•¥:
        1. è·å–æœ€è¿‘5ä¸ªè§†é¢‘
        2. è®¡ç®—åšä¸»è´¨é‡åˆ†
        3. é«˜åˆ†åšä¸»ä¿ç•™top 2ï¼Œä½åˆ†æ”¾å¼ƒ
        """
        print(f"[Bilibili] ğŸ“¡ ç›‘æ§UPä¸»: {params.user_id}")

        try:
            u = user.User(uid=int(params.user_id), credential=self.credential)

            # è·å–æœ€è¿‘5ä¸ªè§†é¢‘ç”¨äºè¯„åˆ†
            resp = sync(u.get_videos(ps=5))

            items = []
            if 'list' in resp and 'vlist' in resp['list']:
                vlist = resp['list']['vlist']
                for v in vlist:
                    pub_ts = v.get('created', 0)
                    pub_date = datetime.fromtimestamp(pub_ts).strftime('%Y-%m-%d')

                    item = {
                        "platform": "bilibili",
                        "source_type": "monitor",
                        "title": v.get('title', ''),
                        "url": f"https://www.bilibili.com/video/{v.get('bvid')}",
                        "bvid": v.get('bvid'),
                        "author_name": v.get('author', ''),
                        "author_id": str(v.get('mid', '')),
                        "publish_time": pub_date,
                        "pub_ts": pub_ts,
                        "view_count": v.get('play', 0),
                        "interaction": v.get('comment', 0),
                        "raw_data": v
                    }
                    items.append(item)

            if len(items) < 2:
                print(f"   âš ï¸ è§†é¢‘æ•°é‡ä¸è¶³")
                return ToolResult(status="success", data=[], summary="Not enough videos")

            # è®¡ç®—åšä¸»è´¨é‡åˆ†
            channel_score = self._score_channel_quality(items)
            print(f"   ğŸ“Š åšä¸»è´¨é‡åˆ†: {channel_score:.2f}")

            # ä½åˆ†åšä¸»æ”¾å¼ƒ
            if channel_score < 3.0:
                print(f"   âŒ è´¨é‡åˆ†è¿‡ä½ï¼Œæ”¾å¼ƒè¯¥åšä¸»")
                return ToolResult(status="success", data=[], summary="Low quality channel")

            # é«˜åˆ†åšä¸»ï¼šä¿ç•™æœ€æ–°2ä¸ªè§†é¢‘
            items.sort(key=lambda x: x.get('pub_ts', 0), reverse=True)
            top_videos = items[:2]

            print(f"   âœ… ä¿ç•™ {len(top_videos)} æ¡ç²¾å“å†…å®¹")

            return ToolResult(
                status="success",
                data=top_videos,
                summary=f"Retrieved {len(top_videos)} videos from high-quality Bilibili user {params.user_id}"
            )

        except Exception as e:
            print(f"[Bilibili] âŒ ç›‘æ§é”™è¯¯: {e}")
            return ToolResult(status="error", error=str(e), summary=f"Bilibili monitor failed: {e}")

    def _score_channel_quality(self, videos: List[Dict]) -> float:
        """
        è®¡ç®—åšä¸»è´¨é‡åˆ†

        æŒ‡æ ‡:
        1. å¹³å‡æ’­æ”¾é‡ï¼ˆç›¸å¯¹åŸºå‡†ï¼‰
        2. å¹³å‡äº’åŠ¨ç‡
        3. å‘å¸ƒé¢‘ç‡
        """
        if len(videos) < 2:
            return 0.0

        # 1. å¹³å‡æ’­æ”¾é‡è¯„åˆ†
        avg_views = statistics.mean([v.get('view_count', 0) for v in videos])
        view_score = min(avg_views / 10000, 30)  # æœ€é«˜30åˆ†

        # 2. å¹³å‡äº’åŠ¨ç‡è¯„åˆ†
        engagement_rates = []
        for v in videos:
            views = v.get('view_count', 0)
            if views > 0:
                engagement = v.get('interaction', 0) / views
                engagement_rates.append(engagement)

        avg_engagement = statistics.mean(engagement_rates) if engagement_rates else 0
        engagement_score = min(avg_engagement * 1000, 30)  # æœ€é«˜30åˆ†

        # 3. å‘å¸ƒé¢‘ç‡åŠ åˆ†
        frequency_bonus = 0
        now = datetime.now()
        for v in videos:
            pub_ts = v.get('pub_ts', 0)
            if pub_ts:
                try:
                    days_old = (now - datetime.fromtimestamp(pub_ts)).days
                    if days_old <= 7:
                        frequency_bonus = 20
                        break
                except:
                    pass

        total_score = view_score + engagement_score + frequency_bonus
        return total_score

    def _calculate_relevance(self, title: str, keyword: str) -> float:
        """
        è®¡ç®—æ ‡é¢˜ä¸æœç´¢è¯çš„ç›¸å…³æ€§æƒé‡

        ç­–ç•¥ï¼š
        1. æå–å…³é”®è¯ä¸­çš„æ ¸å¿ƒæœ¯è¯­
        2. è®¡ç®—æ ‡é¢˜ä¸­åŒ¹é…çš„æ¯”ä¾‹
        3. ä½ç›¸å…³æ€§å¤§å¹…é™æƒï¼Œé«˜ç›¸å…³æ€§åŠ æƒ

        Returns:
            0.2 - 2.0 ä¹‹é—´çš„æƒé‡å€¼
        """
        if not keyword or not title:
            return 1.0

        # æ ‡å‡†åŒ–å¤„ç†
        title_lower = title.lower()
        keyword_lower = keyword.lower()

        # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
            'latest', '2025', '2024', '11', '10', '12',
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
        }

        # æå–å…³é”®è¯ä¸­çš„æ ¸å¿ƒè¯
        keyword_terms = [term for term in keyword_lower.split() if term not in stop_words and len(term) > 1]

        if not keyword_terms:
            return 1.0

        # è®¡ç®—åŒ¹é…åº¦
        matched_count = sum(1 for term in keyword_terms if term in title_lower)
        match_ratio = matched_count / len(keyword_terms)

        # ç›¸å…³æ€§æƒé‡æ˜ å°„
        if match_ratio >= 0.8:
            return 2.0  # é«˜åº¦ç›¸å…³ï¼Œç¿»å€
        elif match_ratio >= 0.5:
            return 1.5  # ä¸­åº¦ç›¸å…³ï¼ŒåŠ æƒ50%
        elif match_ratio >= 0.3:
            return 1.0  # åŸºæœ¬ç›¸å…³ï¼Œä¿æŒ
        elif match_ratio >= 0.1:
            return 0.5  # ä½ç›¸å…³æ€§ï¼Œå‡åŠ
        else:
            return 0.2  # å‡ ä¹ä¸ç›¸å…³ï¼Œå¤§å¹…é™æƒ

    def _parse_duration(self, duration_raw) -> int:
        """
        è§£æBilibili durationå­—æ®µ

        æ”¯æŒæ ¼å¼:
        - å­—ç¬¦ä¸² "5:30" -> 330ç§’
        - å­—ç¬¦ä¸² "1:05:30" -> 3930ç§’
        - æ•´æ•° 330 -> 330ç§’

        Returns:
            int: æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        if isinstance(duration_raw, int):
            return duration_raw

        if isinstance(duration_raw, str):
            try:
                parts = duration_raw.split(':')
                if len(parts) == 2:  # MM:SS
                    return int(parts[0]) * 60 + int(parts[1])
                elif len(parts) == 3:  # HH:MM:SS
                    return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
                else:
                    return 0
            except:
                return 0

        return 0

    def _resolve_order(self, sort_by: str):
        """æ’åºå‚æ•°æ˜ å°„"""
        mapping = {
            "comprehensive": search.OrderVideo.TOTALRANK,
            "click": search.OrderVideo.CLICK,
            "dm": search.OrderVideo.DM,
            "pubdate": search.OrderVideo.PUBDATE,
            "stow": search.OrderVideo.STOW,
            "scores": search.OrderVideo.SCORES
        }
        key = (sort_by or "comprehensive").lower()
        return mapping.get(key, search.OrderVideo.TOTALRANK)
