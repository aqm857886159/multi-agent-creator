import json
import subprocess
import shutil
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import statistics
import scrapetube

class YoutubeScout:
    """
    YouTube æ•°æ®é‡‡é›†å™¨ (v3.0 - çˆ†æ¬¾ä¼˜å…ˆç‰ˆ)
    æ ¸å¿ƒç­–ç•¥: ä¸‰é˜¶æ®µç­›é€‰ + åšä¸»è´¨é‡è¯„åˆ†
    1. å¿«é€Ÿæ‰«æå¤§é‡å€™é€‰
    2. è®¡ç®—çˆ†æ¬¾åˆ†ï¼Œæ™ºèƒ½æ’åº
    3. åªå¯¹é«˜åˆ†å†…å®¹æå–è¯¦ç»†ä¿¡æ¯
    """
    def __init__(self):
        self.yt_dlp_cmd = self._get_yt_dlp_cmd()

    def _get_yt_dlp_cmd(self):
        """æ£€æŸ¥ yt-dlp æ˜¯å¦å¯ç”¨"""
        if shutil.which('yt-dlp'):
            return 'yt-dlp'
        return 'yt-dlp'

    def search_videos(self, keyword: str, limit: int = 15, days: int = 30, sort_by: str = "relevance", scan_limit: int = 0) -> List[Dict[str, Any]]:
        """
        ğŸ¯ ä¸‰é˜¶æ®µçˆ†æ¬¾ç­›é€‰æœç´¢

        é˜¶æ®µ1: å¿«é€Ÿæ‰«æ (scan_limit æ¡åŸºç¡€æ•°æ®)
        é˜¶æ®µ2: è®¡ç®—çˆ†æ¬¾åˆ†ï¼Œæ’åº
        é˜¶æ®µ3: åªå¯¹ top N æå–è¯¦ç»†ä¿¡æ¯

        Args:
            keyword: æœç´¢å…³é”®è¯
            limit: æœ€ç»ˆè¿”å›æ•°é‡ (é»˜è®¤15)
            days: æ—¶é—´èŒƒå›´ (é»˜è®¤30å¤©)
            sort_by: æ’åºæ–¹å¼ (relevance/date)
            scan_limit: å¿«é€Ÿæ‰«ææ•°é‡ (0=è‡ªåŠ¨è®¡ç®—ä¸ºlimit*3)

        Returns:
            List[Dict]: æŒ‰çˆ†æ¬¾åˆ†æ’åºçš„é«˜è´¨é‡è§†é¢‘åˆ—è¡¨
        """
        print(f"ğŸ” [YouTube] ä¸‰é˜¶æ®µæœç´¢: {keyword}")

        # === é˜¶æ®µ1: å¿«é€Ÿæ‰«æ ===
        scan_count = scan_limit if scan_limit > 0 else limit * 3  # ğŸ”‘ æ”¯æŒè‡ªå®šä¹‰æ‰«ææ•°é‡
        print(f"ğŸ“„ [é˜¶æ®µ1] å¿«é€Ÿæ‰«æ {scan_count} æ¡ï¼ˆè¯¦ç»†å¤„ç† {limit} æ¡ï¼‰...")

        basic_videos = self._fast_scan(keyword, scan_count, sort_by)
        if not basic_videos:
            print("âŒ å¿«é€Ÿæ‰«ææœªè·å–åˆ°æ•°æ®")
            return []

        print(f"âœ… [é˜¶æ®µ1] æ‰«æåˆ° {len(basic_videos)} æ¡åŸºç¡€æ•°æ®")

        # === é˜¶æ®µ2: çˆ†æ¬¾è¯„åˆ†ä¸æ’åº ===
        print(f"ğŸ¯ [é˜¶æ®µ2] è®¡ç®—çˆ†æ¬¾åˆ†...")

        scored_videos = self._score_and_rank_viral(basic_videos, days, keyword)
        print(f"âœ… [é˜¶æ®µ2] æ’åºå®Œæˆï¼Œtop {limit} çˆ†æ¬¾è¯†åˆ«")

        # === é˜¶æ®µ3: è¯¦ç»†ä¿¡æ¯æå– ===
        print(f"ğŸ“Š [é˜¶æ®µ3] æå– top {limit} è¯¦ç»†ä¿¡æ¯...")

        top_videos = scored_videos[:limit]
        detailed_videos = self._enrich_details(top_videos)

        print(f"âœ… [YouTube] å®Œæˆï¼è¿”å› {len(detailed_videos)} æ¡çˆ†æ¬¾å†…å®¹")
        return detailed_videos

    def _fast_scan(self, keyword: str, count: int, sort_by: str = "relevance") -> List[Dict[str, Any]]:
        """
        é˜¶æ®µ1: å¿«é€Ÿæ‰«æ
        ä½¿ç”¨ --flat-playlist åªè·å–åŸºç¡€å…ƒæ•°æ®
        """
        search_prefix = "ytsearch"
        if (sort_by or "").lower() == "date":
            search_prefix = "ytsearchdate"

        cmd = [
            self.yt_dlp_cmd,
            f"{search_prefix}{count}:{keyword}",
            "--flat-playlist",  # ğŸ”‘ å¿«é€Ÿæ¨¡å¼
            "--dump-json",
            "--no-warnings",
            "--ignore-errors",
            "--skip-download"
        ]

        videos = []
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore'
            )

            if result.returncode != 0:
                print(f"âš ï¸ yt-dlp æœç´¢å¤±è´¥: {result.stderr}")
                return []

            # è§£ææ¯è¡ŒJSON
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                try:
                    data = json.loads(line)

                    # æå–åŸºç¡€æ•°æ®
                    video_id = data.get('id') or data.get('url')
                    if not video_id:
                        continue

                    # æ„é€ URL
                    if len(video_id) == 11 and '.' not in video_id:
                        url = f"https://www.youtube.com/watch?v={video_id}"
                    else:
                        url = data.get('url') or data.get('webpage_url') or f"https://www.youtube.com/watch?v={video_id}"

                    # åŸºç¡€æ•°æ®å­—å…¸
                    video = {
                        'url': url,
                        'id': video_id,
                        'title': data.get('title', ''),
                        'view_count': data.get('view_count', 0),
                        'upload_date': data.get('upload_date'),  # YYYYMMDDæ ¼å¼
                        'duration': data.get('duration', 0),
                        'channel': data.get('channel', ''),
                        'channel_id': data.get('channel_id', ''),
                    }
                    videos.append(video)

                except Exception as e:
                    continue

        except Exception as e:
            print(f"âš ï¸ å¿«é€Ÿæ‰«æå¼‚å¸¸: {e}")
            return []

        return videos

    def _score_and_rank_viral(self, videos: List[Dict], days: int = 30, keyword: str = "") -> List[Dict]:
        """
        é˜¶æ®µ2: è®¡ç®—çˆ†æ¬¾åˆ†å¹¶æ’åº

        çˆ†æ¬¾åˆ†ç®—æ³•:
        viral_score = (æ’­æ”¾é‡ç›¸å¯¹è¡¨ç°) * (æ—¶é—´æ–°é²œåº¦) * (æ—¶é•¿æƒé‡) * (ç›¸å…³æ€§æƒé‡)

        è¿”å›æŒ‰çˆ†æ¬¾åˆ†æ’åºçš„è§†é¢‘åˆ—è¡¨
        """
        if not videos:
            return []

        # è®¡ç®—å¹³å‡æ’­æ”¾é‡ï¼ˆç”¨äºç›¸å¯¹æ¯”è¾ƒï¼‰
        valid_views = [v['view_count'] for v in videos if v['view_count'] > 0]
        avg_views = statistics.mean(valid_views) if valid_views else 1

        now = datetime.now()
        scored_videos = []

        for video in videos:
            # æ’­æ”¾é‡ç›¸å¯¹è¡¨ç°
            view_ratio = video['view_count'] / max(avg_views, 1)

            # æ—¶é—´æ–°é²œåº¦è®¡ç®—
            freshness = 1.0
            if video.get('upload_date'):
                try:
                    upload_dt = datetime.strptime(video['upload_date'], "%Y%m%d")
                    days_old = (now - upload_dt).days

                    # è¶…è¿‡æ—¶é—´èŒƒå›´çš„ç›´æ¥è·³è¿‡
                    if days_old > days:
                        continue

                    # æ—¶é—´è¡°å‡: è¶Šæ–°è¶Šå¥½
                    if days_old <= 3:
                        freshness = 1.5  # 3å¤©å†…åŠ åˆ†
                    elif days_old <= 7:
                        freshness = 1.2  # 7å¤©å†…åŠ åˆ†
                    elif days_old <= 14:
                        freshness = 1.0  # 14å¤©å†…æ­£å¸¸
                    else:
                        freshness = 0.8  # è¶…è¿‡14å¤©é™åˆ†

                except:
                    freshness = 1.0

            # æ—¶é•¿æƒé‡ï¼ˆ3-20åˆ†é’Ÿæ˜¯é»„é‡‘æ—¶é•¿ï¼‰
            duration_weight = 1.0
            duration_min = video.get('duration', 0) / 60
            if 3 <= duration_min <= 20:
                duration_weight = 1.2
            elif duration_min < 1:
                duration_weight = 0.5  # å¤ªçŸ­çš„è§†é¢‘é™åˆ†
            elif duration_min > 60:
                duration_weight = 0.8  # å¤ªé•¿çš„è§†é¢‘é™åˆ†

            # ğŸ”‘ æ–°å¢ï¼šç›¸å…³æ€§æƒé‡
            relevance_weight = self._calculate_relevance(video.get('title', ''), keyword)

            # ç»¼åˆçˆ†æ¬¾åˆ†ï¼ˆåŠ å…¥ç›¸å…³æ€§æƒé‡ï¼‰
            viral_score = view_ratio * freshness * duration_weight * relevance_weight

            video['viral_score'] = viral_score
            video['days_old'] = days_old if video.get('upload_date') else 999
            scored_videos.append(video)

        # æŒ‰çˆ†æ¬¾åˆ†æ’åºï¼ˆé™åºï¼‰
        scored_videos.sort(key=lambda x: x['viral_score'], reverse=True)

        # è¾“å‡ºtop 3çš„çˆ†æ¬¾åˆ†ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if len(scored_videos) >= 3:
            print(f"   Top 3 çˆ†æ¬¾åˆ†: {scored_videos[0]['viral_score']:.2f}, {scored_videos[1]['viral_score']:.2f}, {scored_videos[2]['viral_score']:.2f}")

        return scored_videos

    def _calculate_relevance(self, title: str, keyword: str) -> float:
        """
        è®¡ç®—æ ‡é¢˜ä¸æœç´¢è¯çš„ç›¸å…³æ€§æƒé‡

        ç­–ç•¥ï¼š
        1. æå–å…³é”®è¯ä¸­çš„æ ¸å¿ƒæœ¯è¯­
        2. è®¡ç®—æ ‡é¢˜ä¸­åŒ¹é…çš„æ¯”ä¾‹
        3. ä½ç›¸å…³æ€§å¤§å¹…é™æƒï¼Œé«˜ç›¸å…³æ€§åŠ æƒ

        Returns:
            0.2 - 2.0 ä¹‹é—´çš„æƒé‡å€¼
        """
        if not keyword or not title:
            return 1.0

        # æ ‡å‡†åŒ–å¤„ç†
        title_lower = title.lower()
        keyword_lower = keyword.lower()

        # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'latest', '2025', '2024', '11', '10', '12'}

        # æå–å…³é”®è¯ä¸­çš„æ ¸å¿ƒè¯
        keyword_terms = [term for term in keyword_lower.split() if term not in stop_words and len(term) > 2]

        if not keyword_terms:
            return 1.0

        # è®¡ç®—åŒ¹é…åº¦
        matched_count = sum(1 for term in keyword_terms if term in title_lower)
        match_ratio = matched_count / len(keyword_terms)

        # ç›¸å…³æ€§æƒé‡æ˜ å°„
        if match_ratio >= 0.8:
            return 2.0  # é«˜åº¦ç›¸å…³ï¼Œç¿»å€
        elif match_ratio >= 0.5:
            return 1.5  # ä¸­åº¦ç›¸å…³ï¼ŒåŠ æƒ50%
        elif match_ratio >= 0.3:
            return 1.0  # åŸºæœ¬ç›¸å…³ï¼Œä¿æŒ
        elif match_ratio >= 0.1:
            return 0.5  # ä½ç›¸å…³æ€§ï¼Œå‡åŠ
        else:
            return 0.2  # å‡ ä¹ä¸ç›¸å…³ï¼Œå¤§å¹…é™æƒ

    def _enrich_details(self, videos: List[Dict]) -> List[Dict]:
        """
        é˜¶æ®µ3: ä¸ºtop Nè§†é¢‘è¡¥å……è¯¦ç»†ä¿¡æ¯

        è¡¥å……å­—æ®µ:
        - ç‚¹èµæ•°ã€è¯„è®ºæ•°
        - ä½œè€…ä¿¡æ¯
        - æ ‡ç­¾ï¼ˆå‰10ä¸ªï¼‰
        - æè¿°ï¼ˆå‰200å­—ï¼‰
        - åˆ†ç±»
        """
        if not videos:
            return []

        import yt_dlp
        ydl_opts = {
            'quiet': True,
            'skip_download': True,
            'ignoreerrors': True,
            'no_warnings': True,
            'socket_timeout': 15,
        }

        enriched = []

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            for i, video in enumerate(videos):
                try:
                    print(f"   æå–è¯¦æƒ… {i+1}/{len(videos)}: {video['title'][:40]}...")

                    # è·å–å®Œæ•´ä¿¡æ¯
                    info = ydl.extract_info(video['url'], download=False)
                    if not info:
                        continue

                    # åˆå¹¶æ•°æ®
                    enriched_video = {
                        # åŸºç¡€æ•°æ®ï¼ˆä¿ç•™ï¼‰
                        "title": info.get('title', video['title']),
                        "url": video['url'],
                        "platform": "youtube",

                        # æ•°æ®æŒ‡æ ‡
                        "view_count": info.get('view_count', video['view_count']),
                        "likes": info.get('like_count', 0),
                        "comments": info.get('comment_count', 0),
                        "interaction": (info.get('like_count') or 0) + (info.get('comment_count') or 0),

                        # ä½œè€…ä¿¡æ¯
                        "author_name": info.get('uploader', video.get('channel', 'Unknown')),
                        "author_id": info.get('channel_id', video.get('channel_id', '')),
                        "author_fans": info.get('channel_follower_count', 0),

                        # æ—¶é—´ä¿¡æ¯
                        "publish_time": info.get('upload_date', video.get('upload_date')),
                        "duration": info.get('duration', video.get('duration', 0)),

                        # çˆ†æ¬¾åˆ†ï¼ˆç”¨äºåç»­åˆ†æï¼‰
                        "viral_score": video.get('viral_score', 0),
                        "days_old": video.get('days_old', 0),

                        # è¯¦ç»†ä¿¡æ¯
                        "raw_data": {
                            "description": (info.get('description') or "")[:200],  # å‰200å­—
                            "tags": info.get('tags', [])[:10],  # å‰10ä¸ªæ ‡ç­¾
                            "category": info.get('categories', []),
                            "language": info.get('language'),
                        }
                    }

                    enriched.append(enriched_video)

                except Exception as e:
                    # å•ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“
                    print(f"   âš ï¸ æå–å¤±è´¥: {e}")
                    continue

        return enriched

    def get_channel_videos(self, channel_url: str, days: int = 30) -> List[Dict[str, Any]]:
        """
        ğŸ¯ åšä¸»è´¨é‡è¯„åˆ†ç›‘æ§

        ç­–ç•¥:
        1. è·å–æœ€è¿‘5ä¸ªè§†é¢‘
        2. è®¡ç®—åšä¸»"è¿‘æœŸè¡¨ç°åˆ†"
        3. é«˜åˆ†åšä¸»ä¿ç•™top 2è§†é¢‘ï¼Œä½åˆ†åšä¸»æ”¾å¼ƒ

        Returns:
            List[Dict]: é«˜è´¨é‡åšä¸»çš„ç²¾é€‰å†…å®¹ï¼ˆæœ€å¤š2æ¡ï¼‰
        """
        print(f"ğŸ“¡ [YouTube] ç›‘æ§åšä¸»: {channel_url}")

        # è·å–æœ€è¿‘5ä¸ªè§†é¢‘ç”¨äºè¯„åˆ†
        recent_urls = self._get_channel_recent_urls(channel_url, limit=5)
        if not recent_urls:
            print(f"   âŒ æœªè·å–åˆ°è§†é¢‘")
            return []

        # è·å–è¯¦ç»†ä¿¡æ¯
        videos = self._hydrate_urls(recent_urls, check_date_days=None)  # ä¸è¿‡æ»¤æ—¶é—´
        if len(videos) < 2:
            print(f"   âš ï¸ è§†é¢‘æ•°é‡ä¸è¶³ï¼Œè·³è¿‡è¯„åˆ†")
            return []

        # è®¡ç®—åšä¸»è´¨é‡åˆ†
        channel_score = self._score_channel_quality(videos)
        print(f"   ğŸ“Š åšä¸»è´¨é‡åˆ†: {channel_score:.2f}")

        # ä½åˆ†åšä¸»ç›´æ¥æ”¾å¼ƒ
        if channel_score < 5.0:  # é˜ˆå€¼å¯è°ƒ
            print(f"   âŒ è´¨é‡åˆ†è¿‡ä½ï¼Œæ”¾å¼ƒè¯¥åšä¸»")
            return []

        # é«˜åˆ†åšä¸»ï¼šä¿ç•™æœ€æ–°2ä¸ªè§†é¢‘
        # æŒ‰æ—¶é—´æ’åº
        videos.sort(key=lambda x: x.get('publish_time', ''), reverse=True)
        top_videos = videos[:2]

        print(f"   âœ… ä¿ç•™ {len(top_videos)} æ¡ç²¾å“å†…å®¹")
        return top_videos

    def _get_channel_recent_urls(self, channel_url: str, limit: int = 5) -> List[str]:
        """è·å–é¢‘é“æœ€è¿‘è§†é¢‘URLs"""
        urls = []

        # ä¼˜å…ˆä½¿ç”¨ scrapetube (å¿«é€Ÿ)
        try:
            videos = scrapetube.get_channel(channel_url=channel_url, limit=limit)
            for v in videos:
                v_id = v.get('videoId')
                if v_id:
                    urls.append(f"https://www.youtube.com/watch?v={v_id}")
        except Exception as e:
            print(f"   âš ï¸ Scrapetube å¤±è´¥ï¼Œå›é€€åˆ° yt-dlp: {e}")

            # å›é€€åˆ° yt-dlp
            cmd = [
                self.yt_dlp_cmd,
                channel_url,
                f"--playlist-end", str(limit),
                "--flat-playlist",
                "--dump-json",
                "--no-warnings",
                "--ignore-errors",
                "--skip-download"
            ]
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                for line in result.stdout.strip().split('\n'):
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        url = data.get('url') or data.get('webpage_url')
                        if url:
                            urls.append(url)
                    except:
                        pass
            except Exception as inner_e:
                print(f"   âŒ yt-dlp ä¹Ÿå¤±è´¥: {inner_e}")

        return urls

    def _score_channel_quality(self, videos: List[Dict]) -> float:
        """
        è®¡ç®—åšä¸»è´¨é‡åˆ†

        æ ¸å¿ƒæŒ‡æ ‡:
        1. å¹³å‡æ’­æ”¾é‡ (ç›¸å¯¹ç²‰ä¸æ•°)
        2. å¹³å‡äº’åŠ¨ç‡
        3. å‘å¸ƒé¢‘ç‡

        Returns:
            float: è´¨é‡åˆ† (0-100)
        """
        if len(videos) < 2:
            return 0.0

        # æå–ç²‰ä¸æ•°ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘çš„ï¼‰
        author_fans = videos[0].get('author_fans', 0)
        if author_fans == 0:
            author_fans = 100000  # é»˜è®¤å‡è®¾10ä¸‡ç²‰ä¸

        # 1. å¹³å‡æ’­æ”¾é‡ç›¸å¯¹ç²‰ä¸æ•°
        avg_views = statistics.mean([v.get('view_count', 0) for v in videos])
        view_ratio = avg_views / author_fans  # æ’­æ”¾é‡/ç²‰ä¸æ•°æ¯”ä¾‹
        view_score = min(view_ratio * 100, 50)  # æœ€é«˜50åˆ†

        # 2. å¹³å‡äº’åŠ¨ç‡
        engagement_rates = []
        for v in videos:
            views = v.get('view_count', 0)
            if views > 0:
                engagement = (v.get('likes', 0) + v.get('comments', 0)) / views
                engagement_rates.append(engagement)

        avg_engagement = statistics.mean(engagement_rates) if engagement_rates else 0
        engagement_score = min(avg_engagement * 1000, 30)  # æœ€é«˜30åˆ†

        # 3. å‘å¸ƒé¢‘ç‡åŠ åˆ†
        frequency_bonus = 0
        now = datetime.now()
        for v in videos:
            upload_date = v.get('publish_time')
            if upload_date:
                try:
                    dt = datetime.strptime(upload_date, "%Y%m%d")
                    days_old = (now - dt).days
                    if days_old <= 7:
                        frequency_bonus = 20  # 7å¤©å†…æœ‰è§†é¢‘ï¼ŒåŠ 20åˆ†
                        break
                except:
                    pass

        # ç»¼åˆè¯„åˆ†
        total_score = view_score + engagement_score + frequency_bonus
        return total_score

    def _hydrate_urls(self, urls: List[str], check_date_days: int = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æå–è§†é¢‘è¯¦ç»†ä¿¡æ¯
        """
        if not urls:
            return []

        final_data = []
        now = datetime.now()
        skipped_count = 0

        import yt_dlp
        ydl_opts = {
            'quiet': True,
            'skip_download': True,
            'ignoreerrors': True,
            'no_warnings': True,
            'socket_timeout': 10,
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            for url in urls:
                try:
                    info = ydl.extract_info(url, download=False)
                    if not info:
                        continue

                    # æ—¥æœŸè¿‡æ»¤
                    upload_date_str = info.get('upload_date')
                    is_valid_date = True

                    if check_date_days and upload_date_str:
                        try:
                            d = datetime.strptime(upload_date_str, "%Y%m%d")
                            delta_days = (now - d).days
                            if delta_days > check_date_days:
                                skipped_count += 1
                                continue
                        except:
                            pass

                    # æ•°æ®ç»„è£…
                    item = {
                        "title": info.get('title'),
                        "url": info.get('webpage_url', url),
                        "view_count": info.get('view_count', 0),
                        "interaction": (info.get('like_count') or 0) + (info.get('comment_count') or 0),
                        "likes": info.get('like_count', 0),
                        "comments": info.get('comment_count', 0),
                        "author_name": info.get('uploader', 'Unknown'),
                        "author_id": info.get('channel_id', ''),
                        "author_fans": info.get('channel_follower_count', 0),
                        "publish_time": upload_date_str,
                        "platform": "youtube",
                        "raw_data": {
                            "description": (info.get('description') or "")[:200],
                            "tags": info.get('tags', [])[:10]
                        }
                    }
                    final_data.append(item)

                except Exception as e:
                    pass

        if skipped_count > 0:
            print(f"  ğŸ§¹ [YouTube] è¿‡æ»¤æ‰ {skipped_count} æ¡è¿‡æœŸè§†é¢‘ (>{check_date_days}å¤©)")

        return final_data
