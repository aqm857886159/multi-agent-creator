# æ—¥å¿—é—®é¢˜æ·±åº¦è¯Šæ–­ä¸Ultra-Thinkè§£å†³æ–¹æ¡ˆ

## ğŸ“‹ é—®é¢˜ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜å…ˆçº§ | é—®é¢˜ | å½±å“èŒƒå›´ | ä¸¥é‡ç¨‹åº¦ | ä¿®å¤éš¾åº¦ |
|--------|------|----------|----------|----------|
| **P0** | JSONè§£æå´©æºƒ | é€‰é¢˜ç”Ÿæˆå¤±è´¥ | è‡´å‘½ | ä¸­ |
| **P0** | è´¨é‡é—¨æ— å›é€€ | æ•°æ®è´¨é‡æ— ä¿éšœ | è‡´å‘½ | é«˜ |
| **P1** | Engine1å¹»è§‰ | åƒåœ¾æ•°æ®æ±¡æŸ“ | ä¸¥é‡ | ä¸­ |
| **P1** | YouTubeç›¸å…³æ€§ä½ | æœç´¢è´¨é‡å·® | ä¸¥é‡ | ä½ |
| **P2** | Web Searchæ•°é‡ä¸è¶³ | å‘ç°æ•ˆç‡ä½ | ä¸­ | ä½ |

---

## ğŸ”´ P0-1: JSONè§£æå´©æºƒ (Kimi K2 Thinking)

### ç—‡çŠ¶è¯Šæ–­

**æ—¥å¿—è¯æ®**:
```
ERROR:root:âŒ LLM Schema Call Failed: <failed_attempts>
<generation number="1">
<exception>
  Input should be a valid array [type=list_type, input_value=':[{', input_type=str]
</exception>
<completion>
  ChatComplet...content=' {"proposals":":[{","},{":"},{"}',
  ...reasoning=' ç”¨æˆ·è¦æ±‚æˆ‘æ‰®æ¼”... (2267 tokens)'
</completion>
```

**æ ¹æœ¬åŸå› **:
1. **Kimi K2 Thinkingæ˜¯æ€ç»´é“¾æ¨¡å‹** - è¾“å‡ºåŒ…å«`reasoning`å­—æ®µï¼ˆ2267 tokensï¼‰
2. **Instructoré»˜è®¤è§£æ`content`å­—æ®µ** - ä½†å®é™…JSONåœ¨`reasoning`æˆ–`reasoning_details`ä¸­
3. **OpenRouterè¿”å›æ ¼å¼** - æ€ç»´é“¾æ¨¡å‹ä¼šå°†æ¨ç†è¿‡ç¨‹åˆ†ç¦»å­˜å‚¨

### Ultra-Thinkåˆ†æ

#### é—®é¢˜å±‚æ¬¡åˆ†è§£

**Level 1: API Responseç»“æ„**
```python
# OpenRouterè¿”å›ï¼ˆKimi K2 Thinkingï¼‰
{
  "choices": [{
    "message": {
      "content": " {\"proposals\":\":[{\",\"},{\":\"},{\"}", # ç¢ç‰‡
      "reasoning": "...å®Œæ•´çš„2267 tokenæ€è€ƒ...",         # å®é™…è¾“å‡º
      "reasoning_details": [                            # ç»“æ„åŒ–
        {
          "type": "reasoning.text",
          "text": "...å®Œæ•´JSONå¯èƒ½åœ¨è¿™..."
        }
      ]
    }
  }]
}
```

**Level 2: Instructorè§£ææµç¨‹**
```python
# instructoråº“é»˜è®¤è¡Œä¸º
response = instructor_client.chat.completions.create(
    model="moonshotai/kimi-k2-thinking",
    response_model=TopicBriefList
)
# â†“
# åªè§£æmessage.content â†’ å¾—åˆ°ç¢ç‰‡ â†’ JSON parseå¤±è´¥
```

**Level 3: æ€ç»´é“¾æ¨¡å‹ç‰¹æ€§**
- Kimi K2è¾“å‡º**å…ˆæ¨ç†åç­”æ¡ˆ**
- æ¨ç†è¿‡ç¨‹å­˜å‚¨åœ¨`reasoning`å­—æ®µ
- `content`å­—æ®µå¯èƒ½ä¸ºç©ºæˆ–ç¢ç‰‡

### è§£å†³æ–¹æ¡ˆçŸ©é˜µ

#### æ–¹æ¡ˆA: æ™ºèƒ½JSONæ¸…æ´—å™¨ (æ¨è)

**åŸç†**: åœ¨Instructorä¹‹å‰é¢„å¤„ç†å“åº”

**å®æ–½ä½ç½®**: `core/llm.py`

**ä»£ç **:
```python
import re
import json

class ReasoningModelParser:
    """ä¸“é—¨å¤„ç†æ€ç»´é“¾æ¨¡å‹çš„JSONæå–å™¨"""

    @staticmethod
    def extract_json_from_response(raw_response: Any) -> str:
        """
        Ultra-Think JSONæå–ç­–ç•¥

        ä¼˜å…ˆçº§ï¼š
        1. reasoning_details[].text
        2. reasoning
        3. content
        4. å…¨æ–‡æ­£åˆ™æå–
        """
        # ç­–ç•¥1: æå–reasoning_details
        if hasattr(raw_response, 'choices') and raw_response.choices:
            message = raw_response.choices[0].message

            # 1.1 reasoning_details (æœ€ä¼˜)
            if hasattr(message, 'reasoning_details') and message.reasoning_details:
                for detail in message.reasoning_details:
                    if hasattr(detail, 'text'):
                        json_str = ReasoningModelParser._extract_json_string(detail.text)
                        if json_str:
                            return json_str

            # 1.2 reasoning field
            if hasattr(message, 'reasoning') and message.reasoning:
                json_str = ReasoningModelParser._extract_json_string(message.reasoning)
                if json_str:
                    return json_str

            # 1.3 content field (å…œåº•)
            if hasattr(message, 'content') and message.content:
                json_str = ReasoningModelParser._extract_json_string(message.content)
                if json_str:
                    return json_str

        # ç­–ç•¥2: å…¨æ–‡æ‰«æï¼ˆæœ€åæ‰‹æ®µï¼‰
        full_text = str(raw_response)
        return ReasoningModelParser._extract_json_string(full_text)

    @staticmethod
    def _extract_json_string(text: str) -> Optional[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–JSON

        æ”¯æŒæ ¼å¼ï¼š
        - ```json ... ```
        - { ... }
        - [ ... ]
        """
        if not text:
            return None

        # ç§»é™¤<think>æ ‡ç­¾
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

        # æå–ä»£ç å—
        json_block = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_block:
            return json_block.group(1).strip()

        # æå–å¯¹è±¡
        json_obj = re.search(r'\{.*\}', text, re.DOTALL)
        if json_obj:
            json_str = json_obj.group(0)
            # éªŒè¯æ˜¯å¦ä¸ºåˆæ³•JSON
            try:
                json.loads(json_str)
                return json_str
            except:
                pass

        # æå–æ•°ç»„
        json_arr = re.search(r'\[.*\]', text, re.DOTALL)
        if json_arr:
            json_str = json_arr.group(0)
            try:
                json.loads(json_str)
                return json_str
            except:
                pass

        return None
```

**é›†æˆåˆ°Instructor**:
```python
# core/llm.py: ModelGateway.call_with_schema()

def call_with_schema(self, user_prompt: str, schema_model: Type[T],
                     system_prompt: str = "You are a helpful assistant.",
                     capability: str = "fast") -> T:
    """ä¿®å¤åçš„ç‰ˆæœ¬"""
    agent_config = self._get_model_params(capability)
    model_id = agent_config["model_id"]

    # ğŸ”‘ æ£€æµ‹æ˜¯å¦ä¸ºæ€ç»´é“¾æ¨¡å‹
    is_reasoning_model = "thinking" in model_id.lower() or "k2" in model_id.lower()

    try:
        if is_reasoning_model:
            # ğŸ”‘ æ€ç»´é“¾æ¨¡å‹ï¼šå…ˆè·å–åŸå§‹å“åº”
            from openai import OpenAI
            client = OpenAI(
                base_url=self.base_url,
                api_key=self.api_key
            )

            raw_response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=agent_config.get("temperature", 0.7),
                max_tokens=agent_config.get("max_tokens", 2000)
            )

            # ğŸ”‘ æå–JSON
            json_str = ReasoningModelParser.extract_json_from_response(raw_response)

            # ğŸ”‘ æ‰‹åŠ¨è§£æä¸ºPydantic
            if json_str:
                return schema_model.model_validate_json(json_str)
            else:
                raise ValueError("æ— æ³•ä»reasoning modelæå–JSON")

        else:
            # æ™®é€šæ¨¡å‹ï¼šä½¿ç”¨instructoré»˜è®¤æµç¨‹
            response = self.instructor_client.chat.completions.create(
                model=model_id,
                response_model=schema_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=agent_config.get("temperature", 0.7),
                max_tokens=agent_config.get("max_tokens", 1000)
            )
            return response

    except Exception as e:
        logging.error(f"âŒ LLM Schema Call Failed: {e}")
        raise e
```

#### æ–¹æ¡ˆB: åˆ‡æ¢åˆ°éæ€ç»´é“¾æ¨¡å‹ (å¿«é€Ÿä¿®å¤)

**åŸç†**: ä¸ºstructured outputä½¿ç”¨æ ‡å‡†æ¨¡å‹

**ä¿®æ”¹**: `config/models.yaml`

```yaml
# æ–°å¢capability: creative_structured
creative_structured:
  model_id: "deepseek/deepseek-chat"  # éthinkingæ¨¡å‹
  temperature: 0.7
  max_tokens: 4000
```

**ä¿®æ”¹**: `nodes/architect.py`

```python
# ä¿®æ”¹capability
result: TopicBriefList = get_llm_with_schema(
    user_prompt=user_prompt,
    response_model=TopicBriefList,
    capability="creative_structured",  # æ”¹ç”¨éthinkingæ¨¡å‹
    system_prompt=system_prompt
)
```

#### æ–¹æ¡ˆC: å¼ºåˆ¶JSON Mode (OpenRouterç‰¹æ€§)

**åŸç†**: ä½¿ç”¨OpenRouterçš„`response_format`å‚æ•°

**ä¿®æ”¹**: `core/llm.py`

```python
# åœ¨instructor clientåˆå§‹åŒ–æ—¶
response = client.chat.completions.create(
    model=model_id,
    messages=[...],
    response_format={ "type": "json_object" },  # ğŸ”‘ å¼ºåˆ¶JSON
    temperature=...
)
```

### æ¨èæ–¹æ¡ˆ

**çŸ­æœŸï¼ˆ1å¤©ï¼‰**: æ–¹æ¡ˆB - åˆ‡æ¢æ¨¡å‹
**ä¸­æœŸï¼ˆ3å¤©ï¼‰**: æ–¹æ¡ˆA - å®æ–½æ™ºèƒ½è§£æå™¨
**é•¿æœŸ**: æ–¹æ¡ˆC + æ–¹æ¡ˆA ç»„åˆ

---

## ğŸ”´ P0-2: è´¨é‡é—¨æ— å›é€€æœºåˆ¶

### ç—‡çŠ¶è¯Šæ–­

**æ—¥å¿—è¯æ®**:
```
Line 77-79: âš ï¸ è´¨é‡æ£€æŸ¥: adjust_params - ç»“æœç›¸å…³ä½†æåº¦ä¸è¶³...
Line 80: ğŸ”´ web_search | é¡¶çº§AIçŸ­å‰§åˆ›ä½œUPä¸»æ¨è 2025  â† ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ª
```

**ç”¨æˆ·æè¿°**: "åšäº†è´¨é‡æ£€æŸ¥ ä½†æ˜¯æ²¡æœ‰å›é€€ é‡æ–°æ„å»ºæœç´¢è¯ è€Œæ˜¯å‘ä¸‹æ‰§è¡Œäº†"

**æ ¹æœ¬åŸå› **:
1. è´¨é‡é—¨åª**è®°å½•é—®é¢˜** - æ²¡æœ‰**æ‰§è¡Œè°ƒæ•´**
2. Plannerä¸è¯»å–`state.quality_checks` - æ²¡æœ‰åé¦ˆå¾ªç¯
3. ç¼ºå°‘é‡è¯•é€»è¾‘ - ä»»åŠ¡æ‰§è¡Œå¤±è´¥åä¸ä¼šè°ƒæ•´å‚æ•°é‡è¯•

### Ultra-Thinkåˆ†æ

**å½“å‰æ¶æ„é—®é¢˜**:
```
Executor â†’ æ‰§è¡Œå·¥å…· â†’ è´¨é‡æ£€æŸ¥ â†’ è®°å½•é—®é¢˜ â†’ ç»§ç»­
                                    â†“
                                state.quality_checks (æ— äººè¯»å–)
```

**ç†æƒ³æ¶æ„**:
```
Planner â†’ é€‰ä»»åŠ¡ â†’ Executor â†’ è´¨é‡æ£€æŸ¥
                              â†“ failed
                        â† å›é€€åˆ°Planner
                              â†“
                        è°ƒæ•´å‚æ•° / é‡æ–°ç”Ÿæˆ
                              â†“
                        Executorå†æ¬¡æ‰§è¡Œ
```

### è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆA: Plannerè¯»å–è´¨é‡æ£€æŸ¥ (æ¨è)

**åŸç†**: Planneråœ¨é€‰æ‹©ä¸‹ä¸€ä¸ªä»»åŠ¡å‰ï¼Œæ£€æŸ¥ä¸Šä¸€ä¸ªä»»åŠ¡çš„è´¨é‡åé¦ˆ

**å®æ–½**: `nodes/planner.py`

```python
def run_planner(state: RadarState) -> Dict[str, Any]:
    """
    è§„åˆ’å¤§è„‘ v3.0 - å¢åŠ è‡ªé€‚åº”åé¦ˆå¾ªç¯
    """
    # ... existing code ...

    # ğŸ”‘ æ–°å¢ï¼šæ£€æŸ¥ä¸Šä¸€ä¸ªä»»åŠ¡çš„è´¨é‡åé¦ˆ
    if state.quality_checks and len(state.quality_checks) > 0:
        last_check = state.quality_checks[-1]

        # å¦‚æœä¸Šä¸€ä¸ªä»»åŠ¡è´¨é‡ä¸é€šè¿‡ä¸”å»ºè®®è°ƒæ•´
        if not last_check["passed"] and last_check["suggested_action"] in ["adjust_params", "retry"]:

            # ğŸ”‘ é‡è¯•ä¿æŠ¤ï¼šæ£€æŸ¥æ˜¯å¦å·²é‡è¯•è¿‡
            last_tool = last_check["tool_name"]
            retry_key = f"{last_tool}_{hash(str(last_check['tool_args']))}"

            if retry_key not in state.completed_tasks:  # æœªé‡è¯•è¿‡
                print(f"\nğŸ”„ è‡ªé€‚åº”åé¦ˆï¼šæ£€æµ‹åˆ° {last_tool} è´¨é‡é—®é¢˜")
                print(f"   é—®é¢˜: {last_check['issues'][0] if last_check['issues'] else 'æœªçŸ¥'}")
                print(f"   å»ºè®®: {last_check['suggested_action']}")

                # ğŸ”‘ åº”ç”¨è°ƒæ•´å»ºè®®
                adjusted_params = last_check["tool_args"].copy()
                if last_check.get("adjustment_plan"):
                    adjusted_params.update(last_check["adjustment_plan"])
                    print(f"   è°ƒæ•´å‚æ•°: {last_check['adjustment_plan']}")

                # ğŸ”‘ åˆ›å»ºé‡è¯•ä»»åŠ¡
                retry_task = TaskItem(
                    task_id=f"retry_{retry_key}",
                    task_type="quality_retry",
                    priority=999,  # æœ€é«˜ä¼˜å…ˆçº§
                    engine=_extract_engine(last_check.get("reasoning", "")),
                    platform=_infer_platform(last_tool),
                    tool_name=last_tool,
                    arguments=adjusted_params,
                    status="pending",
                    reasoning=f"ğŸ”„ [è´¨é‡åé¦ˆé‡è¯•] {last_check['reasoning'][:50]}..."
                )

                # æ’å…¥åˆ°é˜Ÿåˆ—æœ€å‰é¢
                state.task_queue.insert(0, retry_task)
                state.retry_count += 1

                # æ ‡è®°åŸä»»åŠ¡ä¸º"å·²å°è¯•è°ƒæ•´"
                state.completed_tasks.append(retry_key)

                # æŠ¤æ ï¼šæœ€å¤š3æ¬¡å…¨å±€é‡è¯•
                if state.retry_count >= 3:
                    print(f"   âš ï¸ è¾¾åˆ°å…¨å±€é‡è¯•ä¸Šé™ï¼Œè·³è¿‡åç»­åé¦ˆ")
                    state.feedback_enabled = False

                return {
                    "plan_status": "planning",
                    "task_queue": state.task_queue,
                    "quality_checks": state.quality_checks,
                    "retry_count": state.retry_count
                }

    # ... ç»§ç»­åŸæœ‰é€»è¾‘ ...
```

#### æ–¹æ¡ˆB: Executorå†…éƒ¨é‡è¯• (å¤‡é€‰)

**åŸç†**: è´¨é‡æ£€æŸ¥å¤±è´¥åç›´æ¥åœ¨Executoré‡è¯•

**å®æ–½**: `nodes/executor.py`

```python
def run_executor(state: RadarState) -> Dict[str, Any]:
    # ... existing code ...

    # åœ¨è´¨é‡æ£€æŸ¥å
    if not quality_result.passed:
        # åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
        if quality_result.suggested_action in ["retry", "adjust_params"]:

            # ğŸ”‘ æ£€æŸ¥æ˜¯å¦å·²é‡è¯•
            retry_count_key = f"retry_count_{tool_name}"
            current_retries = state.session_focus.get(retry_count_key, 0)

            if current_retries < 2:  # æœ€å¤š2æ¬¡
                print(f"   ğŸ”„ è‡ªé€‚åº”é‡è¯•: ç¬¬{current_retries+1}æ¬¡")

                # åº”ç”¨è°ƒæ•´
                if quality_result.adjustment_plan:
                    tool_args.update(quality_result.adjustment_plan)

                # é‡æ–°æ‰§è¡Œ
                result = tool_def.func(tool_args)

                # è®°å½•é‡è¯•
                state.session_focus[retry_count_key] = current_retries + 1
                state.retry_count += 1

                # é‡æ–°æ£€æŸ¥è´¨é‡
                quality_result = _run_quality_check(...)
```

### æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆA (Plannerå±‚é¢)** - æ›´ç¬¦åˆç³»ç»Ÿæ¶æ„ï¼Œä¸ç ´åå•ä¸€èŒè´£åŸåˆ™

---

## ğŸŸ  P1-1: Engine 1 "Ken Jee"å¹»è§‰

### ç—‡çŠ¶è¯Šæ–­

**æ—¥å¿—è¯æ®**:
```
Line 84: âœ… åšä¸»æå–: 4 ä¸ª (YT:4 BL:0)
Line 202-237: ğŸ”´ youtube_search | Ken Jee AIçŸ­å‰§åˆ›ä½œå·¥ä½œæµ
è¿”å›: çª®å°å­æ…˜é­é™·å®³å…¥ç„... (ä¸­æ–‡çˆ½æ–‡çŸ­å‰§)
```

**é—®é¢˜åˆ†æ**:
1. Ken Jeeæ˜¯**Data Scienceåšä¸»**ï¼Œä¸AIçŸ­å‰§æ— å…³
2. YouTubeæœç´¢"Ken Jee AIçŸ­å‰§"æ‰¾ä¸åˆ°ç›¸å…³å†…å®¹
3. YouTubeå¿½ç•¥"Ken Jee"ï¼ŒåªåŒ¹é…"AIçŸ­å‰§"å…³é”®è¯
4. è¿”å›å¤§é‡åƒåœ¾æ•°æ®

### Ultra-Thinkåˆ†æ

**å¹»è§‰é“¾æ¡**:
```
Webæœç´¢ â†’ æ–‡ç« : "top AI video creators"
    â†“
LLMæå–åšä¸» â†’ è¯†åˆ«åˆ°"Ken Jee"
    â†“ (é”™è¯¯æ¨ç†)
è®¤ä¸ºKen Jeeæ˜¯AIè§†é¢‘åˆ›ä½œåšä¸»
    â†“
ç”Ÿæˆä»»åŠ¡: youtube_search | Ken Jee AIçŸ­å‰§åˆ›ä½œå·¥ä½œæµ
    â†“
YouTubeæœç´¢ â†’ æ‰¾ä¸åˆ°Ken Jeeç›¸å…³
    â†“
é€€åŒ–ä¸º: æœç´¢"AIçŸ­å‰§" (å¿½ç•¥Ken Jee)
    â†“
è¿”å›ä¸­æ–‡çˆ½æ–‡çŸ­å‰§
```

### è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆA: å¼ºåˆ¶Channel IDæœç´¢ (æ¨è)

**åŸç†**: Engine 1å¿…é¡»æå–Channel IDæˆ–@handleï¼Œç”¨é¢‘é“è¿‡æ»¤

**ä¿®æ”¹1**: `nodes/influencer_extractor.py` Prompt

```python
system_prompt = """
ä½ æ˜¯é¡¶çº§çš„åšä¸»è¯†åˆ«ä¸“å®¶...

ğŸ”‘ **CRITICAL REQUIREMENTS**:

1. **Platform Fingerprinting (å¿…é¡»)**:
   - YouTube: å¿…é¡»æå– @handle æˆ– Channel URL
   - Bilibili: å¿…é¡»æå– UID æˆ–ç©ºé—´URL

2. **Verification Rules**:
   - âŒ å¦‚æœæ–‡ç« åªæåˆ°åšä¸»åå­—ä½†æ²¡æœ‰@handle/UID â†’ ä¸æå–
   - âœ… åªæœ‰æ˜ç¡®æ ‡è¯†ç¬¦çš„åšä¸»æ‰èƒ½æå–

3. **Output Format**:
   ```json
   {
     "name": "åšä¸»å",
     "platform": "youtube",
     "identifier": "@channelname",  # ğŸ”‘ å¿…å¡«ï¼Œä¸èƒ½æ˜¯åå­—
     "confidence": "high"
   }
   ```

**Examples**:

âœ… CORRECT:
- "Follow @MKBHD for tech reviews" â†’ identifier="@MKBHD"
- "è®¢é˜…UPä¸» å½±è§†é£“é£ï¼ˆUID: 946974ï¼‰" â†’ identifier="946974"

âŒ WRONG:
- "Ken Jee creates data science content" â†’ âŒ æ— @handleï¼Œè·³è¿‡
- "é¡¶çº§AIåšä¸»æ¨èï¼šææ°¸ä¹" â†’ âŒ æ— UIDï¼Œè·³è¿‡
"""
```

**ä¿®æ”¹2**: `tools/youtube_scout.py` - æ·»åŠ Channelè¿‡æ»¤

```python
def search_videos(self, keyword: str, channel_id: Optional[str] = None, ...):
    """
    æ–°å¢channel_idå‚æ•°
    """
    if channel_id:
        # ğŸ”‘ é™å®šåœ¨ç‰¹å®šé¢‘é“æœç´¢
        if channel_id.startswith("@"):
            # æ–¹æ¡ˆ1: ä½¿ç”¨yt-dlpæœç´¢è¯­æ³•
            search_query = f"{keyword} channel:{channel_id[1:]}"
        else:
            # æ–¹æ¡ˆ2: å…ˆè·å–é¢‘é“æ‰€æœ‰è§†é¢‘ï¼Œæœ¬åœ°è¿‡æ»¤keyword
            return self._search_in_channel(channel_id, keyword, limit)
    else:
        # åŸæœ‰å…¨ç½‘æœç´¢
        ...

def _search_in_channel(self, channel_id: str, keyword: str, limit: int):
    """åœ¨æŒ‡å®šé¢‘é“æœç´¢"""
    # 1. è·å–é¢‘é“æœ€æ–°50ä¸ªè§†é¢‘
    channel_url = f"https://www.youtube.com/@{channel_id}/videos"
    videos = self._fast_scan(channel_url, scan_count=50)

    # 2. æœ¬åœ°è¿‡æ»¤keyword
    filtered = [v for v in videos if keyword.lower() in v['title'].lower()]

    # 3. è¿”å›top N
    return filtered[:limit]
```

**ä¿®æ”¹3**: `nodes/planner.py` - ä¼ é€’identifier

```python
# ç”Ÿæˆåšä¸»æœç´¢ä»»åŠ¡æ—¶
task = TaskItem(
    task_id=f"influencer_search_{platform}_{name}",
    ...
    arguments={
        "keyword": keyword,
        "channel_id": influencer.get("identifier"),  # ğŸ”‘ ä¼ é€’æ ‡è¯†ç¬¦
        "from_influencer": identifier,
        "influencer_name": name
    }
)
```

#### æ–¹æ¡ˆB: è´¨é‡é—¨æ‹’ç»æ— å…³ç»“æœ

**åŸç†**: è´¨é‡é—¨æ£€æµ‹åˆ°åšä¸»åä¸åŒ¹é…æ—¶ï¼Œç›´æ¥æ‹’ç»

**å®æ–½**: `core/quality_gate.py`

```python
def check_quality(...):
    # ç‰¹æ®Šæ£€æŸ¥ï¼šåšä¸»æœç´¢ä»»åŠ¡
    if "from_influencer" in tool_params:
        expected_creator = tool_params.get("influencer_name", "")

        # æ£€æŸ¥è¿”å›ç»“æœæ˜¯å¦çœŸçš„æ¥è‡ªè¯¥åšä¸»
        if tool_result and tool_result.get("data"):
            sample_titles = [item.get("title", "") for item in tool_result["data"][:3]]

            # ç®€å•æ£€æŸ¥ï¼šæ ‡é¢˜ä¸­æ˜¯å¦å‡ºç°åšä¸»å
            has_creator_mention = any(expected_creator.lower() in title.lower()
                                     for title in sample_titles)

            if not has_creator_mention:
                return QualityCheckResult(
                    passed=False,
                    score=0.1,
                    issues=[f"æœç´¢åšä¸»'{expected_creator}'ä½†ç»“æœå®Œå…¨æ— å…³"],
                    suggested_action="skip",  # ğŸ”‘ è·³è¿‡ï¼Œä¸é‡è¯•
                    reasoning="åšä¸»æœç´¢å¤±è´¥ï¼Œå¯èƒ½è¯¥åšä¸»æœªå‘å¸ƒç›¸å…³å†…å®¹"
                )
```

### æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆA (Channel ID)** - æ²»æœ¬
**æ–¹æ¡ˆB (è´¨é‡é—¨)** - æ²»æ ‡

åŒæ—¶å®æ–½æ•ˆæœæœ€ä½³ã€‚

---

## ğŸŸ  P1-2: YouTubeæœç´¢"æ–‡ä¸å¯¹é¢˜"

### ç—‡çŠ¶è¯Šæ–­

**æ—¥å¿—è¯æ®**:
```
æœç´¢: "AI video drama workflow tutorial 2025"
è¿”å›: "Testing Stable Diffusion inpainting", "Music AI"
è´¨é‡åˆ†æ•°: 0.3
```

**æ ¹æœ¬åŸå› **:
1. æœç´¢è¯å¤ªå…·ä½“ - "drama workflow tutorial"è¿‡åº¦ç»„åˆ
2. YouTubeç®—æ³•è¯†åˆ«ä¸äº†å¤åˆå…³é”®è¯
3. "drama"æœ‰æ­§ä¹‰ - å¯èƒ½åŒ¹é…"dramatic"ã€"documentary"

### è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆA: ä¼˜åŒ–Keyword Designer Prompt (å·²éƒ¨åˆ†å®Œæˆ)

**å½“å‰é—®é¢˜**: è™½ç„¶æœ‰æ–‡åŒ–é€‚é…æç¤ºï¼Œä½†LLMä»ç”Ÿæˆè¿‡åº¦å¤æ‚çš„è¯

**å¼ºåŒ–ç­–ç•¥**:

```python
# nodes/keyword_designer.py

user_prompt = f"""
...

# ğŸ”‘ æ–°å¢ï¼šYouTubeæœç´¢è¯ç®€åŒ–è§„åˆ™

## YouTube Search Query Optimization:

1. **Keep It Simple (3-5 words max)**:
   - âŒ WRONG: "AI video drama workflow tutorial step-by-step guide 2025"
   - âœ… RIGHT: "AI filmmaking tutorial 2025"

2. **Use Broad-to-Narrow Strategy**:
   - Level 1: "AI video generation"
   - Level 2: "AI video generation tutorial"
   - Level 3: "AI video tutorial workflow"
   - ğŸ”‘ Use Level 2 (balance)

3. **Avoid Compound Concepts**:
   - âŒ "AI drama workflow tutorial" (4 concepts)
   - âœ… "AI filmmaking guide" (2 concepts)

4. **Test Against Real Searches**:
   - Would a normal user type this exact phrase?
   - If no, simplify.

**Examples for Topic "AIçŸ­å‰§åˆ›ä½œå·¥ä½œæµ"**:

âŒ TOO COMPLEX:
- "AI video drama workflow tutorial 2025"
- "AI short film production complete guide"

âœ… OPTIMIZED:
- "AI filmmaking tutorial 2025"
- "AI video creation guide"
- "AI movie making workflow"

Remember: YouTube's algorithm prefers **natural search patterns** over technical jargon.
"""
```

#### æ–¹æ¡ˆB: å¢åŠ æœç´¢è¯A/Bæµ‹è¯•

**åŸç†**: ä¸ºåŒä¸€ä¸»é¢˜ç”Ÿæˆ2-3ä¸ªå€™é€‰è¯ï¼Œæµ‹è¯•åé€‰æœ€å¥½çš„

**å®æ–½**: Plannerç”Ÿæˆå¤šä¸ªæœç´¢ä»»åŠ¡

```python
# ä¸ºEngine 2ç”ŸæˆA/Bæµ‹è¯•ä»»åŠ¡
variants = [
    "AI filmmaking tutorial 2025",
    "AI video creation guide",
    "AI movie workflow"
]

for variant in variants:
    tasks.append(TaskItem(
        task_id=f"youtube_search_{topic}_variant_{i}",
        ...
        arguments={"keyword": variant, "limit": 5}  # æ¯ä¸ªåªæŠ“5æ¡
    ))

# è´¨é‡é—¨è‡ªåŠ¨é€‰å‡ºæœ€ä½³ç»“æœ
```

### æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆAä¼˜å…ˆ** - ä»æºå¤´æ”¹è¿›

---

## ğŸŸ¡ P2: Web Searchæ•°é‡ä¸è¶³

### ç—‡çŠ¶

```
Line 78: limit=20ä½†ä»…è¿”å›1ä¸ªç»“æœ
```

### å¿«é€Ÿä¿®å¤

**ä¿®æ”¹**: `DEFAULT_PARAMS`

```python
"web_search": {"limit": 30, "depth": "advanced"},  # 20â†’30
```

**åŸå› **: Webæœç´¢ç»“æœè´¨é‡å‚å·®ä¸é½ï¼Œéœ€è¦æ›´å¤§é‡‡æ ·æ± 

---

## ğŸ¯ å®æ–½è·¯çº¿å›¾

### ç¬¬1å¤© (é«˜ä¼˜å…ˆçº§)

**ä»»åŠ¡**:
1. âœ… JSONè§£æå™¨ä¿®å¤ (æ–¹æ¡ˆB: åˆ‡æ¢æ¨¡å‹)
2. âœ… è´¨é‡é—¨å›é€€ (æ–¹æ¡ˆA: Planneråé¦ˆ)
3. âœ… YouTubeæœç´¢è¯ä¼˜åŒ– (æ–¹æ¡ˆA: å¼ºåŒ–prompt)

**é¢„æœŸ**:
- JSONè§£ææˆåŠŸç‡: 0% â†’ 95%
- è´¨é‡åé¦ˆç”Ÿæ•ˆç‡: 0% â†’ 80%
- YouTubeç›¸å…³æ€§: 30% â†’ 70%

### ç¬¬3å¤© (ä¸­ä¼˜å…ˆçº§)

**ä»»åŠ¡**:
4. âœ… Engine 1 Channel ID (æ–¹æ¡ˆA)
5. âœ… JSONæ™ºèƒ½è§£æå™¨ (æ–¹æ¡ˆA)

**é¢„æœŸ**:
- Engine 1å¹»è§‰ç‡: 50% â†’ 10%
- æ”¯æŒKimi K2ç­‰æ€ç»´é“¾æ¨¡å‹

### ç¬¬7å¤© (ä¼˜åŒ–)

**ä»»åŠ¡**:
6. âœ… A/Bæœç´¢è¯æµ‹è¯•
7. âœ… è´¨é‡é—¨æ™ºèƒ½åº¦æå‡

---

## ğŸ“Š æˆåŠŸæŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | æµ‹é‡æ–¹å¼ |
|------|------|------|----------|
| JSONè§£ææˆåŠŸç‡ | 0% | 95% | é€‰é¢˜ç”ŸæˆæˆåŠŸæ¬¡æ•°/æ€»æ¬¡æ•° |
| è´¨é‡åé¦ˆç”Ÿæ•ˆ | 0% | 80% | è°ƒæ•´åé‡è¯•æ¬¡æ•°/è´¨é‡é—®é¢˜æ¬¡æ•° |
| YouTubeç›¸å…³æ€§ | 30% | 75% | è´¨é‡é—¨åˆ†æ•°avg |
| Engine 1å‡†ç¡®ç‡ | 50% | 90% | æœ‰æ•ˆåšä¸»æ•°/æå–æ€»æ•° |
| Bilibiliè´¨é‡ | 92% | 95% | ä¿æŒé«˜æ°´å¹³ |

---

## ğŸ§ª æµ‹è¯•æ¸…å•

### å›å½’æµ‹è¯•
- [ ] Bilibiliä»ä¿æŒé«˜è´¨é‡ï¼ˆåˆ†æ•°>0.9ï¼‰
- [ ] æ™ºèƒ½åˆ†é¡µä»æ­£å¸¸å·¥ä½œ
- [ ] ä»»åŠ¡å»é‡ä»æœ‰æ•ˆ

### æ–°åŠŸèƒ½æµ‹è¯•
- [ ] JSONè§£æï¼šä½¿ç”¨Kimi K2æ¨¡å‹ç”Ÿæˆé€‰é¢˜
- [ ] è´¨é‡å›é€€ï¼šæ•…æ„è§¦å‘ä½è´¨é‡ç»“æœï¼Œè§‚å¯Ÿé‡è¯•
- [ ] Engine 1ï¼šéªŒè¯æå–çš„åšä¸»æœ‰Channel ID

### å‹åŠ›æµ‹è¯•
- [ ] è¿ç»­3æ¬¡è´¨é‡å¤±è´¥ â†’ åº”åœæ­¢åé¦ˆ
- [ ] JSONè§£æ3æ¬¡é‡è¯•å¤±è´¥ â†’ åº”é™çº§

---

## ğŸ’¡ æœ€ç»ˆå»ºè®®

**ç«‹å³è¡ŒåŠ¨ï¼ˆä»Šå¤©ï¼‰**:
1. ä¿®æ”¹`config/models.yaml` - ä¸ºarchitectæ·»åŠ éthinkingæ¨¡å‹
2. ä¿®æ”¹`nodes/planner.py` - æ·»åŠ è´¨é‡åé¦ˆè¯»å–
3. ä¿®æ”¹`nodes/keyword_designer.py` - å¼ºåŒ–YouTubeç®€åŒ–è§„åˆ™

**æœ¬å‘¨å®Œæˆ**:
4. å®æ–½JSONæ™ºèƒ½è§£æå™¨
5. å®æ–½Engine 1 Channel IDæœºåˆ¶

**æŒç»­ä¼˜åŒ–**:
6. ç›‘æ§è´¨é‡é—¨åé¦ˆæ•ˆæœ
7. è°ƒæ•´å„æ¨¡å‹temperatureå‚æ•°

ğŸ¯ **é¢„æœŸæ•ˆæœ**: ç³»ç»Ÿä»80åˆ†æå‡åˆ°95åˆ†ï¼Œæˆä¸ºçœŸæ­£å¯é çš„ç”Ÿäº§å·¥å…·
