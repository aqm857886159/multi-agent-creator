"""
ğŸ”‘ P1æ–¹æ¡ˆ: æœç´¢è´¨é‡éªŒè¯å™¨
è‡ªåŠ¨æ£€æµ‹æœç´¢ç»“æœç›¸å…³æ€§å¹¶æä¾›é™çº§å»ºè®®
"""

from typing import Dict, Any, List, Tuple
import re
from difflib import SequenceMatcher


class SearchValidator:
    """æœç´¢ç»“æœè´¨é‡éªŒè¯å™¨"""

    def __init__(self, relevance_threshold: float = 0.3):
        """
        å‚æ•°:
            relevance_threshold: ç›¸å…³æ€§é˜ˆå€¼ (0-1)ï¼Œä½äºæ­¤å€¼è§¦å‘è­¦å‘Š
        """
        self.relevance_threshold = relevance_threshold

    def validate_results(
        self,
        query: str,
        results: List[Dict[str, Any]],
        result_limit: int = 10
    ) -> Dict[str, Any]:
        """
        éªŒè¯æœç´¢ç»“æœè´¨é‡

        å‚æ•°:
            query: æœç´¢å…³é”®è¯
            results: æœç´¢ç»“æœåˆ—è¡¨ (åŒ…å« title å­—æ®µ)
            result_limit: æ£€æŸ¥å‰Nä¸ªç»“æœ

        è¿”å›:
            {
                "is_valid": bool,
                "relevance_score": float (0-1),
                "matched_count": int,
                "total_checked": int,
                "core_entities": List[str],
                "matched_keywords": List[str],
                "issues": List[str],
                "suggestions": List[str]
            }
        """
        if not results:
            return {
                "is_valid": False,
                "relevance_score": 0.0,
                "matched_count": 0,
                "total_checked": 0,
                "core_entities": [],
                "matched_keywords": [],
                "issues": ["æœç´¢è¿”å›0ä¸ªç»“æœ"],
                "suggestions": ["æ‰©å¤§æœç´¢èŒƒå›´æˆ–è°ƒæ•´å…³é”®è¯"]
            }

        # æå–æ ¸å¿ƒå®ä½“
        core_entities = self._extract_core_entities(query)

        # è®¡ç®—ç›¸å…³æ€§
        relevance_score, matched_count, matched_keywords = self._calculate_relevance(
            core_entities,
            results[:result_limit]
        )

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        is_valid = relevance_score >= self.relevance_threshold

        # ç”Ÿæˆé—®é¢˜å’Œå»ºè®®
        issues = []
        suggestions = []

        if not is_valid:
            issues.append(f"ä»…{matched_count}/{min(result_limit, len(results))}ç»“æœåŒ…å«æ ¸å¿ƒè¯'{core_entities}'")
            issues.append(f"ç›¸å…³æ€§{relevance_score:.1%} < é˜ˆå€¼{self.relevance_threshold:.0%}")

            # ç”Ÿæˆé™çº§å»ºè®®
            suggestions.extend(self._generate_fallback_suggestions(query, core_entities, matched_keywords))

        return {
            "is_valid": is_valid,
            "relevance_score": relevance_score,
            "matched_count": matched_count,
            "total_checked": min(result_limit, len(results)),
            "core_entities": core_entities,
            "matched_keywords": matched_keywords,
            "issues": issues,
            "suggestions": suggestions
        }

    def _extract_core_entities(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–æ ¸å¿ƒå®ä½“è¯

        ç¤ºä¾‹:
            "AIå…¬å¸manusä¸ºä»€ä¹ˆæˆåŠŸ" â†’ ["ai", "å…¬å¸", "manus", "æˆåŠŸ"]
            "why Manus AI succeeded" â†’ ["manus", "ai", "succeeded"]
        """
        stopwords = {
            'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›', 'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'å—',
            'why', 'how', 'what', 'when', 'where', 'who', 'which', 'the', 'a', 'an', 'is', 'are', 'was', 'were',
            'be', 'been', 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'should'
        }

        # åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        words = re.findall(r'[\w]+', query.lower())

        # è¿‡æ»¤åœç”¨è¯å’Œè¿‡çŸ­è¯
        core_entities = [w for w in words if w not in stopwords and len(w) > 1]

        return core_entities

    def _calculate_relevance(
        self,
        core_entities: List[str],
        results: List[Dict[str, Any]]
    ) -> Tuple[float, int, List[str]]:
        """
        è®¡ç®—æœç´¢ç»“æœä¸æ ¸å¿ƒå®ä½“çš„ç›¸å…³æ€§

        è¿”å›:
            (å¹³å‡ç›¸å…³æ€§åˆ†æ•°, åŒ¹é…åˆ°çš„ç»“æœæ•°, åŒ¹é…åˆ°çš„å…³é”®è¯åˆ—è¡¨)
        """
        if not core_entities or not results:
            return 0.0, 0, []

        matched_count = 0
        matched_keywords = set()

        for result in results:
            # æå–æ ‡é¢˜
            title = result.get('title', '').lower()
            if not title:
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ ¸å¿ƒå®ä½“
            has_match = False
            for entity in core_entities:
                # ç²¾ç¡®åŒ¹é…
                if entity in title:
                    has_match = True
                    matched_keywords.add(entity)
                    continue

                # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†æ‹¼å†™å˜ä½“ï¼‰
                for word in re.findall(r'[\w]+', title):
                    similarity = SequenceMatcher(None, entity, word).ratio()
                    if similarity > 0.85:  # 85%ç›¸ä¼¼åº¦
                        has_match = True
                        matched_keywords.add(entity)
                        break

            if has_match:
                matched_count += 1

        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        relevance_score = matched_count / len(results) if results else 0.0

        return relevance_score, matched_count, list(matched_keywords)

    def _generate_fallback_suggestions(
        self,
        original_query: str,
        core_entities: List[str],
        matched_keywords: List[str]
    ) -> List[str]:
        """
        ç”Ÿæˆé™çº§æœç´¢å»ºè®®

        ç­–ç•¥:
        1. Layer 1: åªç”¨å“ç‰Œå/æ ¸å¿ƒå®ä½“ (ç²¾å‡†åŒ¹é…)
        2. Layer 2: æ ¸å¿ƒå®ä½“ + åŠŸèƒ½æè¿°è¯
        3. Layer 3: æ³›åŒ–åˆ°é¢†åŸŸè¯
        """
        suggestions = []

        # Layer 1: ç²¾å‡†åŒ¹é… - æå–æœ€æ ¸å¿ƒçš„å®ä½“ï¼ˆé€šå¸¸æ˜¯å“ç‰Œå/äº§å“åï¼‰
        if core_entities:
            # æ‰¾åˆ°æœ€å¯èƒ½æ˜¯ä¸“æœ‰åè¯çš„å®ä½“ï¼ˆé¦–å­—æ¯å¤§å†™æˆ–æ··åˆå¤§å°å†™ï¼‰
            original_words = re.findall(r'[\w]+', original_query)
            proper_nouns = [w for w in original_words if w[0].isupper() or any(c.isupper() for c in w[1:])]

            if proper_nouns:
                suggestions.append(f'å°è¯•ç²¾å‡†åŒ¹é…: "{proper_nouns[0]}"')
            else:
                # å¦åˆ™ç”¨ç¬¬ä¸€ä¸ªå®ä½“
                suggestions.append(f'ç®€åŒ–ä¸ºæ ¸å¿ƒè¯: "{core_entities[0]}"')

        # Layer 2: åŠŸèƒ½æè¿°
        if matched_keywords:
            # å¦‚æœæœ‰åŒ¹é…å…³é”®è¯ï¼Œè¯´æ˜å¯èƒ½éœ€è¦æ·»åŠ åŠŸèƒ½æè¿°
            functional_words = ['tutorial', 'review', 'guide', 'æ•™ç¨‹', 'è¯„æµ‹', 'ä½¿ç”¨']
            suggestions.append(f'æ·»åŠ åŠŸèƒ½è¯: "{matched_keywords[0]} {functional_words[0]}"')
        elif core_entities:
            suggestions.append(f'æ·»åŠ åŠŸèƒ½è¯: "{core_entities[0]} tutorial" æˆ– "{core_entities[0]} æ•™ç¨‹"')

        # Layer 3: æ³›åŒ–å»ºè®®
        # æ£€æµ‹é¢†åŸŸè¯ï¼ˆAI, tech, businessç­‰ï¼‰
        domain_keywords = {
            'ai': ['artificial intelligence', 'machine learning', 'AIå·¥å…·'],
            'tech': ['technology', 'software', 'ç§‘æŠ€'],
            'business': ['startup', 'company', 'åˆ›ä¸šå…¬å¸']
        }

        for domain, alternatives in domain_keywords.items():
            if domain in core_entities:
                suggestions.append(f'æ³›åŒ–æœç´¢: "{alternatives[0]}"')
                break

        # å¦‚æœä»¥ä¸Šéƒ½æ²¡æœ‰ï¼Œç»™å‡ºé€šç”¨å»ºè®®
        if not suggestions:
            suggestions.append("æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®")
            suggestions.append("è¯¥ä¸»é¢˜å¯èƒ½æ˜¯æ–°å…´/å°ä¼—è¯é¢˜ï¼Œæ•°æ®ä¸è¶³")

        return suggestions


# å…¨å±€å•ä¾‹
_validator = SearchValidator(relevance_threshold=0.3)


def validate_search_results(query: str, results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šéªŒè¯æœç´¢ç»“æœè´¨é‡

    ç¤ºä¾‹:
        result = validate_search_results(
            query="Manus AIæˆåŠŸç§˜è¯€",
            results=[{"title": "AIå·¥å…·æ¨è"}, {"title": "Manus AIæ•™ç¨‹"}]
        )
        if not result["is_valid"]:
            print(f"æœç´¢è´¨é‡ä¸ä½³: {result['issues']}")
            print(f"å»ºè®®: {result['suggestions']}")
    """
    return _validator.validate_results(query, results)


if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹1: é«˜ç›¸å…³æ€§
    print("=== æµ‹è¯•ç”¨ä¾‹1: é«˜ç›¸å…³æ€§ ===")
    result1 = validate_search_results(
        query="Manus AI tutorial",
        results=[
            {"title": "Manus AI Complete Tutorial"},
            {"title": "How to use Manus AI"},
            {"title": "Manus AI Review"}
        ]
    )
    print(f"ç»“æœ: {result1['is_valid']}, ç›¸å…³æ€§: {result1['relevance_score']:.1%}")
    print(f"åŒ¹é…: {result1['matched_count']}/{result1['total_checked']}")
    print()

    # æµ‹è¯•ç”¨ä¾‹2: ä½ç›¸å…³æ€§ï¼ˆæœç´¢æ¼‚ç§»ï¼‰
    print("=== æµ‹è¯•ç”¨ä¾‹2: ä½ç›¸å…³æ€§ï¼ˆæœç´¢æ¼‚ç§»ï¼‰===")
    result2 = validate_search_results(
        query="Manus AIæˆåŠŸç§˜è¯€",
        results=[
            {"title": "å½“AIå¾—çŸ¥è‡ªå·±æ˜¯AI"},
            {"title": "äºšé©¬é€ŠAIå¸ƒå±€"},
            {"title": "5ä¸ªAIç”Ÿæ„"},
            {"title": "AIå·¥å…·æ¨è"},
            {"title": "Manus AIæ•™ç¨‹"}  # ä»…ç¬¬5ä¸ªç›¸å…³
        ]
    )
    print(f"ç»“æœ: {result2['is_valid']}, ç›¸å…³æ€§: {result2['relevance_score']:.1%}")
    print(f"åŒ¹é…: {result2['matched_count']}/{result2['total_checked']}")
    print(f"é—®é¢˜: {result2['issues']}")
    print(f"å»ºè®®: {result2['suggestions']}")
    print()

    # æµ‹è¯•ç”¨ä¾‹3: æ— ç»“æœ
    print("=== æµ‹è¯•ç”¨ä¾‹3: æ— ç»“æœ ===")
    result3 = validate_search_results(
        query="XYZ123 è¶…çº§æ–°äº§å“",
        results=[]
    )
    print(f"ç»“æœ: {result3['is_valid']}")
    print(f"é—®é¢˜: {result3['issues']}")
    print(f"å»ºè®®: {result3['suggestions']}")
