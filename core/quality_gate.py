"""
è‡ªé€‚åº”è´¨é‡é—¨ç³»ç»Ÿ (Adaptive Quality Gate)

æ ¸å¿ƒè®¾è®¡åŸåˆ™:
1. é€šç”¨æ€§ - é€‚ç”¨äºä»»ä½•å·¥å…·ç»“æœçš„è´¨é‡æ£€æŸ¥
2. æ™ºèƒ½æ€§ - ä½¿ç”¨LLMè‡ªä¸»åˆ¤æ–­å’Œå†³ç­–
3. æŠ¤æ æœºåˆ¶ - é˜²æ­¢æ­»å¾ªç¯å’Œæˆæœ¬å¤±æ§
ğŸ”‘ P1å¢å¼º - é›†æˆæœç´¢ç»“æœç›¸å…³æ€§éªŒè¯
"""

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from core.llm import get_llm_with_schema
from core.search_validator import validate_search_results  # ğŸ”‘ P1æ–°å¢


class QualityCheckResult(BaseModel):
    """è´¨é‡æ£€æŸ¥ç»“æœï¼ˆé€šç”¨ç»“æ„ï¼‰"""
    passed: bool = Field(..., description="æ˜¯å¦é€šè¿‡è´¨é‡æ£€æŸ¥ï¼ˆtrue=ç»§ç»­ï¼Œfalse=éœ€è¦è°ƒæ•´ï¼‰")
    confidence: float = Field(..., description="åˆ¤æ–­çš„ç½®ä¿¡åº¦ 0-1ï¼Œè¶Šé«˜è¶Šç¡®å®š")
    score: float = Field(..., description="ç»“æœè´¨é‡åˆ†æ•° 0-1ï¼Œ0=å®Œå…¨ä¸ç¬¦åˆé¢„æœŸï¼Œ1=å®Œå…¨ç¬¦åˆ")

    # æ™ºèƒ½è¯Šæ–­
    issues: List[str] = Field(default_factory=list, description="å‘ç°çš„å…·ä½“é—®é¢˜ï¼ˆè®©LLMè‡ªç”±æè¿°ï¼‰")
    root_cause: Optional[str] = Field(None, description="æ ¹æœ¬åŸå› åˆ†æ")

    # æ™ºèƒ½å»ºè®®
    suggested_action: str = Field(..., description="å»ºè®®çš„è¡ŒåŠ¨: continue | retry | adjust_params | change_strategy | skip")
    adjustment_plan: Optional[Dict[str, Any]] = Field(None, description="å…·ä½“çš„è°ƒæ•´æ–¹æ¡ˆï¼ˆå‚æ•°ä¿®æ”¹ã€ç­–ç•¥å˜æ›´ç­‰ï¼‰")
    reasoning: str = Field(..., description="ä¸ºä»€ä¹ˆåšå‡ºè¿™ä¸ªåˆ¤æ–­ï¼ˆå¯è§£é‡Šæ€§ï¼‰")


class FeedbackLoopGuard(BaseModel):
    """åé¦ˆå¾ªç¯æŠ¤æ """
    tool_name: str
    original_params: Dict[str, Any]
    retry_count: int = 0
    max_retries: int = 2  # æœ€å¤šé‡è¯•2æ¬¡
    total_cost_estimate: float = 0.0  # ç´¯è®¡æˆæœ¬ä¼°ç®—
    max_cost: float = 1.0  # æœ€å¤§å…è®¸æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    feedback_history: List[Dict[str, Any]] = Field(default_factory=list)

    def can_retry(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­é‡è¯•"""
        if self.retry_count >= self.max_retries:
            return False
        if self.total_cost_estimate >= self.max_cost:
            return False
        return True

    def record_attempt(self, result: QualityCheckResult, cost: float = 0.0):
        """è®°å½•ä¸€æ¬¡å°è¯•"""
        self.retry_count += 1
        self.total_cost_estimate += cost
        self.feedback_history.append({
            "attempt": self.retry_count,
            "passed": result.passed,
            "score": result.score,
            "action": result.suggested_action,
            "issues": result.issues,
            "cost": cost
        })


class AdaptiveQualityGate:
    """
    è‡ªé€‚åº”è´¨é‡é—¨

    æ ¸å¿ƒç‰¹ç‚¹:
    1. é€šç”¨æ€§ - ä¸é’ˆå¯¹ç‰¹å®šå·¥å…·ï¼Œé€šç”¨è´¨é‡æ£€æŸ¥æ¡†æ¶
    2. æ™ºèƒ½æ€§ - LLMè‡ªä¸»åˆ†æé—®é¢˜å’Œç»™å‡ºæ–¹æ¡ˆ
    3. è‡ªé€‚åº” - æ ¹æ®å†å²åé¦ˆè°ƒæ•´åˆ¤æ–­æ ‡å‡†
    """

    def __init__(self, use_fast_model: bool = True):
        """
        Args:
            use_fast_model: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å‹ï¼ˆhaikuï¼‰é™ä½æˆæœ¬
        """
        self.use_fast_model = use_fast_model
        self.capability = "base" if use_fast_model else "reasoning"

    def check_quality(
        self,
        tool_name: str,
        tool_params: Dict[str, Any],
        tool_result: Any,
        expectation: str,
        context: Optional[Dict[str, Any]] = None
    ) -> QualityCheckResult:
        """
        é€šç”¨è´¨é‡æ£€æŸ¥ï¼ˆæ™ºèƒ½åˆ¤æ–­ï¼‰
        ğŸ”‘ P1å¢å¼º: å¯¹æœç´¢å·¥å…·å¢åŠ å¿«é€Ÿç›¸å…³æ€§é¢„æ£€æŸ¥

        Args:
            tool_name: å·¥å…·åç§°ï¼ˆå¦‚ youtube_search, bilibili_searchï¼‰
            tool_params: å·¥å…·è°ƒç”¨å‚æ•°
            tool_result: å·¥å…·è¿”å›ç»“æœï¼ˆå¯ä»¥æ˜¯ä»»ä½•ç±»å‹ï¼‰
            expectation: æœŸæœ›çš„ç»“æœæè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            QualityCheckResult: LLMæ™ºèƒ½åˆ¤æ–­çš„è´¨é‡æ£€æŸ¥ç»“æœ
        """

        # ğŸ”‘ P1: å¿«é€Ÿé¢„æ£€æŸ¥ - æœç´¢å·¥å…·çš„å…³é”®è¯ç›¸å…³æ€§éªŒè¯
        if tool_name in ['youtube_search', 'bilibili_search', 'web_search']:
            query = tool_params.get('query', '')
            if query and hasattr(tool_result, 'data') and isinstance(tool_result.data, list):
                # è¿è¡Œå¿«é€Ÿç›¸å…³æ€§æ£€æŸ¥
                validation_result = validate_search_results(query, tool_result.data)

                if not validation_result['is_valid']:
                    # ç›¸å…³æ€§ä¸è¶³ï¼Œç›´æ¥è¿”å›å¤±è´¥ï¼ˆè·³è¿‡LLMè°ƒç”¨ï¼ŒèŠ‚çœæˆæœ¬ï¼‰
                    return QualityCheckResult(
                        passed=False,
                        confidence=0.9,  # è§„åˆ™æ£€æŸ¥ç½®ä¿¡åº¦é«˜
                        score=validation_result['relevance_score'],
                        issues=validation_result['issues'] + [
                            f"æ ¸å¿ƒå®ä½“: {validation_result['core_entities']}",
                            f"ä»…åŒ¹é…: {validation_result['matched_keywords']}"
                        ],
                        root_cause=f"æœç´¢å¼•æ“å°†'{query}'é™çº§ä¸ºæ³›åŒ–è¯'{validation_result['core_entities']}'ï¼Œå¿½ç•¥æ ¸å¿ƒå®ä½“",
                        suggested_action="adjust_params",
                        adjustment_plan={
                            "original_query": query,
                            "fallback_queries": validation_result['suggestions'],
                            "action": "try_simpler_keyword"
                        },
                        reasoning=f"å…³é”®è¯ç›¸å…³æ€§æ£€æŸ¥: {validation_result['relevance_score']:.1%} < 30%é˜ˆå€¼ï¼Œå»ºè®®é™çº§ä¸ºæ›´ç®€æ´çš„å…³é”®è¯"
                    )

        # æ„å»ºæ™ºèƒ½æç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è´¨é‡æ£€æŸ¥ä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°å·¥å…·æ‰§è¡Œç»“æœçš„è´¨é‡ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ¤æ–­ç»“æœæ˜¯å¦ç¬¦åˆé¢„æœŸ
2. è¯Šæ–­å­˜åœ¨çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
3. ç»™å‡ºæ™ºèƒ½çš„æ”¹è¿›å»ºè®®

è¯„åˆ¤åŸåˆ™ï¼š
- ç›¸å…³æ€§ï¼šç»“æœæ˜¯å¦ä¸é¢„æœŸä¸»é¢˜ç›¸å…³
- æ•°é‡ï¼šæ˜¯å¦è·å–åˆ°è¶³å¤Ÿçš„æ•°æ®
- è´¨é‡ï¼šæ•°æ®æ˜¯å¦æœ‰ä»·å€¼ï¼ˆéåƒåœ¾å†…å®¹ï¼‰
- å¤šæ ·æ€§ï¼šæ˜¯å¦æœ‰é‡å¤æˆ–å•ä¸€æ¥æº

è¡ŒåŠ¨å»ºè®®ç±»å‹ï¼š
- continue: è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¸‹ä¸€æ­¥
- retry: ä¸´æ—¶é—®é¢˜ï¼Œé‡è¯•ç›¸åŒå‚æ•°
- adjust_params: éœ€è¦è°ƒæ•´å‚æ•°ï¼ˆå¦‚å¢åŠ limitã€ä¿®æ”¹å…³é”®è¯ï¼‰
- change_strategy: ç­–ç•¥æ€§é—®é¢˜ï¼Œéœ€è¦æ¢ä¸ªæ–¹å‘
- skip: æ— æ³•ä¿®å¤ï¼Œè·³è¿‡è¿™ä¸ªä»»åŠ¡
"""

        # å‡†å¤‡ç»“æœæ‘˜è¦ï¼ˆé¿å…tokenè¿‡å¤šï¼‰
        result_summary = self._summarize_result(tool_result)

        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context_str = ""
        if context:
            context_str = f"\n\nè¡¥å……ä¸Šä¸‹æ–‡:\n{self._format_context(context)}"

        user_prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹å·¥å…·æ‰§è¡Œç»“æœçš„è´¨é‡ï¼š

ã€å·¥å…·ã€‘: {tool_name}
ã€å‚æ•°ã€‘: {tool_params}
ã€é¢„æœŸã€‘: {expectation}

ã€å®é™…ç»“æœã€‘:
{result_summary}
{context_str}

è¯·åˆ†æï¼š
1. è¿™ä¸ªç»“æœæ˜¯å¦ç¬¦åˆé¢„æœŸï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. å­˜åœ¨ä»€ä¹ˆé—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Ÿ
3. å»ºè®®é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼Ÿ
4. å¦‚æœéœ€è¦è°ƒæ•´ï¼Œå…·ä½“åº”è¯¥æ€ä¹ˆæ”¹ï¼Ÿ

è¯·ç»™å‡ºä½ çš„ä¸“ä¸šåˆ¤æ–­ã€‚
"""

        try:
            result: QualityCheckResult = get_llm_with_schema(
                user_prompt=user_prompt,
                response_model=QualityCheckResult,
                capability=self.capability,
                system_prompt=system_prompt
            )
            return result

        except Exception as e:
            # å…œåº•ï¼šæ£€æŸ¥å¤±è´¥æ—¶é»˜è®¤é€šè¿‡ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            print(f"âš ï¸ è´¨é‡æ£€æŸ¥å¤±è´¥: {e}ï¼Œé»˜è®¤é€šè¿‡")
            return QualityCheckResult(
                passed=True,
                confidence=0.5,
                score=0.7,
                issues=[f"è´¨é‡æ£€æŸ¥å¤±è´¥: {e}"],
                suggested_action="continue",
                reasoning="è´¨é‡æ£€æŸ¥ç³»ç»Ÿå¼‚å¸¸ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥é»˜è®¤é€šè¿‡"
            )

    def _summarize_result(self, result: Any) -> str:
        """æ€»ç»“ç»“æœï¼ˆé¿å…tokenè¿‡å¤šï¼‰"""
        if result is None:
            return "ç»“æœä¸ºç©º (None)"

        if isinstance(result, dict):
            if "data" in result and isinstance(result["data"], list):
                items = result["data"]
                count = len(items)
                if count == 0:
                    return f"è¿”å›0æ¡æ•°æ®"

                # æ˜¾ç¤ºå‰3æ¡çš„æ ‡é¢˜
                sample_titles = []
                for item in items[:3]:
                    if isinstance(item, dict) and "title" in item:
                        sample_titles.append(item["title"])

                summary = f"è¿”å›{count}æ¡æ•°æ®\nå‰3æ¡æ ‡é¢˜:\n"
                for i, title in enumerate(sample_titles, 1):
                    summary += f"  {i}. {title}\n"

                return summary
            else:
                return f"å­—å…¸ç»“æœ: {str(result)[:300]}..."

        if isinstance(result, list):
            count = len(result)
            if count == 0:
                return "è¿”å›ç©ºåˆ—è¡¨"

            # å°è¯•æå–æ ‡é¢˜
            sample_titles = []
            for item in result[:3]:
                if isinstance(item, dict) and "title" in item:
                    sample_titles.append(item["title"])

            if sample_titles:
                summary = f"è¿”å›{count}æ¡æ•°æ®\nå‰3æ¡æ ‡é¢˜:\n"
                for i, title in enumerate(sample_titles, 1):
                    summary += f"  {i}. {title}\n"
                return summary
            else:
                return f"è¿”å›{count}æ¡æ•°æ®ï¼ˆæ— æ³•æå–æ ‡é¢˜ï¼‰"

        # å…¶ä»–ç±»å‹
        return f"{type(result).__name__}: {str(result)[:200]}..."

    def _format_context(self, context: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        lines = []
        for key, value in context.items():
            if isinstance(value, (list, dict)):
                lines.append(f"- {key}: {len(value)} é¡¹")
            else:
                lines.append(f"- {key}: {value}")
        return "\n".join(lines)


class FeedbackLoopManager:
    """
    åé¦ˆå¾ªç¯ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. ç®¡ç†é‡è¯•é€»è¾‘
    2. é˜²æ­¢æ­»å¾ªç¯
    3. æˆæœ¬æ§åˆ¶
    4. è‡ªåŠ¨åº”ç”¨è°ƒæ•´å»ºè®®
    """

    def __init__(self, max_retries: int = 2, max_cost: float = 1.0):
        self.max_retries = max_retries
        self.max_cost = max_cost
        self.active_guards: Dict[str, FeedbackLoopGuard] = {}

    def create_guard(self, tool_name: str, params: Dict[str, Any]) -> FeedbackLoopGuard:
        """ä¸ºä»»åŠ¡åˆ›å»ºæŠ¤æ """
        guard_id = f"{tool_name}_{hash(str(params))}"

        guard = FeedbackLoopGuard(
            tool_name=tool_name,
            original_params=params.copy(),
            max_retries=self.max_retries,
            max_cost=self.max_cost
        )

        self.active_guards[guard_id] = guard
        return guard

    def should_retry(
        self,
        guard: FeedbackLoopGuard,
        quality_result: QualityCheckResult
    ) -> bool:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•

        ç»¼åˆè€ƒè™‘ï¼š
        1. è´¨é‡æ£€æŸ¥å»ºè®®
        2. æŠ¤æ é™åˆ¶
        3. å†å²åé¦ˆ
        """
        # æŠ¤æ æ£€æŸ¥
        if not guard.can_retry():
            print(f"   ğŸ›‘ æŠ¤æ é˜»æ­¢: å·²é‡è¯•{guard.retry_count}æ¬¡ æˆ– æˆæœ¬${guard.total_cost_estimate:.2f}")
            return False

        # LLMå»ºè®®æ£€æŸ¥
        if quality_result.suggested_action in ["continue", "skip"]:
            return False

        if quality_result.suggested_action in ["retry", "adjust_params", "change_strategy"]:
            # æ£€æŸ¥æ˜¯å¦åœ¨é‡å¤åŒæ ·çš„é—®é¢˜
            if self._is_repeating_issue(guard, quality_result):
                print(f"   ğŸ” æ£€æµ‹åˆ°é‡å¤é—®é¢˜ï¼Œåœæ­¢é‡è¯•")
                return False

            return True

        return False

    def _is_repeating_issue(
        self,
        guard: FeedbackLoopGuard,
        current_result: QualityCheckResult
    ) -> bool:
        """æ£€æµ‹æ˜¯å¦åœ¨é‡å¤é‡åˆ°ç›¸åŒé—®é¢˜"""
        if len(guard.feedback_history) < 2:
            return False

        # ç®€å•æ£€æŸ¥ï¼šå¦‚æœæœ€è¿‘ä¸¤æ¬¡çš„é—®é¢˜ç›¸ä¼¼ï¼Œè®¤ä¸ºæ˜¯é‡å¤
        last_issues = guard.feedback_history[-1].get("issues", [])
        current_issues = current_result.issues

        if not last_issues or not current_issues:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„é—®é¢˜æè¿°
        for last_issue in last_issues:
            for current_issue in current_issues:
                if last_issue.lower() in current_issue.lower() or current_issue.lower() in last_issue.lower():
                    return True

        return False

    def apply_adjustment(
        self,
        original_params: Dict[str, Any],
        quality_result: QualityCheckResult
    ) -> Dict[str, Any]:
        """
        åº”ç”¨LLMå»ºè®®çš„è°ƒæ•´

        æ™ºèƒ½åˆå¹¶åŸå‚æ•°å’Œè°ƒæ•´å»ºè®®
        """
        if not quality_result.adjustment_plan:
            return original_params.copy()

        adjusted = original_params.copy()

        # åº”ç”¨è°ƒæ•´å»ºè®®
        for key, value in quality_result.adjustment_plan.items():
            adjusted[key] = value

        print(f"   ğŸ”§ å‚æ•°è°ƒæ•´: {quality_result.adjustment_plan}")

        return adjusted
