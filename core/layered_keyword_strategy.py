"""
ğŸ”‘ P2æ–¹æ¡ˆ: åˆ†å±‚å…³é”®è¯ç­–ç•¥
å½“ç²¾å‡†æœç´¢å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨é™çº§åˆ°æ›´ç®€æ´çš„å…³é”®è¯
"""

from typing import List, Dict, Any
import re


class LayeredKeywordGenerator:
    """
    åˆ†å±‚å…³é”®è¯ç”Ÿæˆå™¨

    ç­–ç•¥:
    - Layer 1 (Precise): ç²¾å‡†åŒ¹é… - å“ç‰Œå/ä¸“æœ‰åè¯
    - Layer 2 (Functional): åŠŸèƒ½æè¿° - æ ¸å¿ƒå®ä½“ + åŠ¨ä½œè¯
    - Layer 3 (Generic): æ³›åŒ–æœç´¢ - é¢†åŸŸè¯
    """

    def generate_fallback_keywords(
        self,
        original_query: str,
        platform: str = "youtube"  # youtube | bilibili
    ) -> Dict[str, List[str]]:
        """
        ä¸ºåŸå§‹æŸ¥è¯¢ç”Ÿæˆåˆ†å±‚é™çº§å…³é”®è¯

        å‚æ•°:
            original_query: åŸå§‹æœç´¢è¯ (å¦‚ "why Manus AI succeeded 2025")
            platform: å¹³å° (youtube | bilibili)

        è¿”å›:
            {
                "layer1_precise": ["Manus AI", "Manus"],
                "layer2_functional": ["Manus AI tutorial", "Manusä½¿ç”¨æ•™ç¨‹"],
                "layer3_generic": ["AI automation tools", "AIä»£ç†å·¥å…·"]
            }
        """
        # æå–æ ¸å¿ƒå®ä½“
        core_entities = self._extract_entities(original_query)

        # æ£€æµ‹æŸ¥è¯¢è¯­è¨€
        is_chinese = self._is_chinese(original_query)

        layers = {
            "layer1_precise": [],
            "layer2_functional": [],
            "layer3_generic": []
        }

        if not core_entities:
            # æ— æ³•æå–å®ä½“ï¼Œè¿”å›åŸå§‹æŸ¥è¯¢
            layers["layer1_precise"] = [original_query]
            return layers

        # Layer 1: ç²¾å‡†åŒ¹é…
        # æå–ä¸“æœ‰åè¯ï¼ˆå¤§å†™å¼€å¤´æˆ–æ··åˆå¤§å°å†™ï¼‰
        proper_nouns = self._extract_proper_nouns(original_query)

        if proper_nouns:
            # ä½¿ç”¨ä¸“æœ‰åè¯
            layers["layer1_precise"] = [
                proper_nouns[0],  # å•ç‹¬å“ç‰Œå
                f'"{proper_nouns[0]}"'  # åŠ å¼•å·å¼ºåˆ¶ç²¾å‡†åŒ¹é…
            ]
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ¸å¿ƒå®ä½“
            layers["layer1_precise"] = [core_entities[0]]

        # Layer 2: åŠŸèƒ½æè¿°
        functional_keywords = self._generate_functional_keywords(
            core_entities,
            proper_nouns,
            platform,
            is_chinese
        )
        layers["layer2_functional"] = functional_keywords

        # Layer 3: æ³›åŒ–æœç´¢
        generic_keywords = self._generate_generic_keywords(
            original_query,
            core_entities,
            platform,
            is_chinese
        )
        layers["layer3_generic"] = generic_keywords

        return layers

    def _extract_entities(self, query: str) -> List[str]:
        """æå–æ ¸å¿ƒå®ä½“ï¼ˆå»é™¤åœç”¨è¯å’ŒåŠ¨ä½œè¯ï¼‰"""
        stopwords = {
            'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'å“ªä¸ª', 'çš„', 'äº†', 'æ˜¯', 'å’Œ', 'ä¸º',
            'æ•™ç¨‹', 'æŒ‡å—', 'è¯„æµ‹', 'è§£æ', 'åŠ¨æ€', 'åˆ¶ä½œ', 'æ·±åº¦', 'ä¿å§†çº§', 'æœ€æ–°',
            'why', 'how', 'what', 'when', 'where', 'who', 'the', 'a', 'an',
            'is', 'are', 'was', 'were', 'do', 'does', 'did',
            'tutorial', 'guide', 'review', 'analysis', 'making', 'latest'
        }

        words = re.findall(r'[\w]+', query.lower())
        entities = [w for w in words if w not in stopwords and len(w) > 1]

        return entities

    def _extract_proper_nouns(self, query: str) -> List[str]:
        """æå–ä¸“æœ‰åè¯ï¼ˆå¤§å†™å¼€å¤´æˆ–æ··åˆå¤§å°å†™ï¼‰"""
        words = re.findall(r'[\w]+', query)

        proper_nouns = []
        for word in words:
            # æ£€æŸ¥æ˜¯å¦é¦–å­—æ¯å¤§å†™æˆ–åŒ…å«å¤§å†™å­—æ¯
            if word[0].isupper() or any(c.isupper() for c in word[1:]):
                proper_nouns.append(word)

        return proper_nouns

    def _is_chinese(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡"""
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        return len(chinese_chars) > len(text) * 0.3  # ä¸­æ–‡å­—ç¬¦å æ¯” > 30%

    def _generate_functional_keywords(
        self,
        core_entities: List[str],
        proper_nouns: List[str],
        platform: str,
        is_chinese: bool
    ) -> List[str]:
        """ç”ŸæˆåŠŸèƒ½æ€§å…³é”®è¯ï¼ˆå®ä½“ + åŠ¨ä½œè¯ï¼‰"""
        functional = []

        # ç¡®å®šä¸»å®ä½“ - ä¼˜å…ˆç”¨ä¸“æœ‰åè¯ï¼Œå¦åˆ™ç”¨ç¬¬ä¸€ä¸ªéåŠ¨ä½œè¯çš„å®ä½“
        if proper_nouns:
            main_entity = proper_nouns[0]
        else:
            # è¿‡æ»¤æ‰æ˜æ˜¾çš„åŠ¨ä½œè¯/æè¿°è¯
            filter_words = {'æ•™ç¨‹', 'æŒ‡å—', 'è¯„æµ‹', 'è§£æ', 'åŠ¨æ€', 'åˆ¶ä½œ', 'tutorial', 'guide', 'review', 'making'}
            candidates = [e for e in core_entities if e not in filter_words]
            main_entity = candidates[0] if candidates else "AI"  # é»˜è®¤ç”¨AI

        if platform == "youtube":
            if is_chinese:
                # ä¸­æ–‡YouTubeæœç´¢ï¼ˆå°‘è§ï¼Œä½†å¤„ç†ï¼‰
                functional = [
                    f"{main_entity} æ•™ç¨‹",
                    f"{main_entity} ä½¿ç”¨æŒ‡å—",
                    f"{main_entity} è¯„æµ‹"
                ]
            else:
                # è‹±æ–‡YouTubeæœç´¢
                functional = [
                    f"{main_entity} tutorial",
                    f"{main_entity} guide",
                    f"{main_entity} review",
                    f"how to use {main_entity}"
                ]

        elif platform == "bilibili":
            if is_chinese:
                # ä¸­æ–‡Bç«™æœç´¢ï¼ˆä¸»æµï¼‰
                functional = [
                    f"{main_entity} ä¿å§†çº§æ•™ç¨‹",
                    f"{main_entity} ä½¿ç”¨æ•™ç¨‹",
                    f"{main_entity} æ·±åº¦è¯„æµ‹",
                    f"{main_entity} å®æ“æ¼”ç¤º"
                ]
            else:
                # è‹±æ–‡å®ä½“ï¼ŒBç«™ç”¨ä¸­æ–‡æè¿°
                functional = [
                    f"{main_entity} æ•™ç¨‹",
                    f"{main_entity} ä½¿ç”¨æŒ‡å—",
                    f"{main_entity} è¯„æµ‹"
                ]

        return functional[:3]  # æœ€å¤š3ä¸ª

    def _generate_generic_keywords(
        self,
        original_query: str,
        core_entities: List[str],
        platform: str,
        is_chinese: bool
    ) -> List[str]:
        """ç”Ÿæˆæ³›åŒ–å…³é”®è¯ï¼ˆé¢†åŸŸè¯ï¼‰"""
        generic = []

        # æ£€æµ‹é¢†åŸŸå…³é”®è¯
        domain_map = {
            # è‹±æ–‡
            'ai': 'AI automation tools',
            'manus': 'AI agent platforms',  # Manusæ˜¯AI Agentå·¥å…·
            'chatgpt': 'AI chatbot tools',
            'python': 'programming tutorials',
            'react': 'web development frameworks',

            # ä¸­æ–‡
            'äººå·¥æ™ºèƒ½': 'AIå·¥å…·',
            'aiå·¥å…·': 'AIè‡ªåŠ¨åŒ–',
            'ç¼–ç¨‹': 'ç¼–ç¨‹æ•™ç¨‹',
            'å‰ç«¯': 'Webå¼€å‘'
        }

        # æŸ¥æ‰¾åŒ¹é…çš„é¢†åŸŸ
        for entity in core_entities:
            if entity in domain_map:
                if is_chinese and platform == "bilibili":
                    # ä¸­æ–‡æ³›åŒ–
                    generic.append(f"{domain_map[entity]} æœ€æ–°")
                else:
                    # è‹±æ–‡æ³›åŒ–
                    generic.append(domain_map[entity])
                break

        # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œç”Ÿæˆé€šç”¨æ³›åŒ–è¯
        if not generic:
            if is_chinese:
                # æ ¹æ®æ ¸å¿ƒå®ä½“çŒœæµ‹é¢†åŸŸ
                if any(word in core_entities for word in ['ai', 'æ™ºèƒ½', 'agent']):
                    generic.append("AIå·¥å…·æ¨è æœ€æ–°")
                elif any(word in core_entities for word in ['ç¼–ç¨‹', 'python', 'code']):
                    generic.append("ç¼–ç¨‹æ•™ç¨‹ æœ€æ–°")
                else:
                    generic.append(f"{core_entities[0]} ç›¸å…³å†…å®¹")
            else:
                if any(word in core_entities for word in ['ai', 'artificial', 'intelligence']):
                    generic.append("AI tools 2025")
                elif any(word in core_entities for word in ['code', 'programming', 'python']):
                    generic.append("programming tutorials")
                else:
                    generic.append(f"{core_entities[0]} related")

        return generic[:2]  # æœ€å¤š2ä¸ª


# å…¨å±€å•ä¾‹
_generator = LayeredKeywordGenerator()


def generate_fallback_keywords(original_query: str, platform: str = "youtube") -> Dict[str, List[str]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šç”Ÿæˆåˆ†å±‚é™çº§å…³é”®è¯

    ç¤ºä¾‹:
        layers = generate_fallback_keywords(
            original_query="why Manus AI succeeded 2025",
            platform="youtube"
        )

        # ç»“æœ:
        # {
        #     "layer1_precise": ["Manus"],
        #     "layer2_functional": ["Manus tutorial", "Manus guide"],
        #     "layer3_generic": ["AI agent platforms"]
        # }

        # ä½¿ç”¨ç­–ç•¥:
        # 1. å…ˆè¯• layer1_precise[0]
        # 2. å¦‚æœå¤±è´¥ï¼Œè¯• layer2_functional[0]
        # 3. æœ€åè¯• layer3_generic[0]
    """
    return _generator.generate_fallback_keywords(original_query, platform)


if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹1: Manus AI (è‹±æ–‡)
    print("=== æµ‹è¯•ç”¨ä¾‹1: Manus AI ===")
    result1 = generate_fallback_keywords("why Manus AI succeeded 2025", "youtube")
    for layer, keywords in result1.items():
        print(f"{layer}: {keywords}")
    print()

    # æµ‹è¯•ç”¨ä¾‹2: Bç«™ä¸­æ–‡æŸ¥è¯¢
    print("=== æµ‹è¯•ç”¨ä¾‹2: Bç«™ä¸­æ–‡ ===")
    result2 = generate_fallback_keywords("Manus AIæˆåŠŸç§˜è¯€æ·±åº¦è§£æ æœ€æ–°", "bilibili")
    for layer, keywords in result2.items():
        print(f"{layer}: {keywords}")
    print()

    # æµ‹è¯•ç”¨ä¾‹3: é€šç”¨AIæŸ¥è¯¢
    print("=== æµ‹è¯•ç”¨ä¾‹3: é€šç”¨AI ===")
    result3 = generate_fallback_keywords("AIè§†é¢‘åˆ¶ä½œæ•™ç¨‹", "bilibili")
    for layer, keywords in result3.items():
        print(f"{layer}: {keywords}")
