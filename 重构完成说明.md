# 系统重构完成说明

> **重构日期**: 2025-11-26
> **版本**: v2.0 (任务队列化 + 双引擎平衡)

---

## ✅ 已完成的重构

### 1. core/state.py ✅
**新增模型**:
- `TopicSearchQueries`: 结构化搜索词（每个主题4个搜索词）
- `TaskItem`: 任务队列项

**新增字段**:
- `topic_queries`: 结构化搜索词列表
- `task_queue`: 待执行任务队列
- `completed_tasks`: 已完成任务ID列表
- `current_phase`: 当前阶段 (init/discovery/collection/filtering)
- `engine_progress`: 引擎进度追踪

### 2. utils/logger.py ✅ (新文件)
**功能**:
- `print_phase_header()`: 打印阶段标题
- `print_progress_dashboard()`: 双引擎进度仪表盘
- `print_task_selected()`: 打印选中的任务
- `print_influencer_extraction_result()`: 博主提取结果
- `print_filter_result()`: 筛选结果

### 3. nodes/keyword_designer.py ✅ (完全重写)
**核心改进**:
- 每个主题生成配对的中英文搜索词
- discovery_query_en/zh + content_query_en/zh
- 整合用户输入 (`session_focus["priority_topics"]`)
- 输出 `TopicSearchQueries` 结构化数据

### 4. nodes/planner.py ✅ (完全重写 - 任务调度器)
**核心改进**:
- 任务队列化管理
- 智能任务选择策略：
  - 平台平衡（YouTube vs Bilibili）
  - 引擎平衡（引擎1 vs 引擎2）
  - 优先级排序
- 动态生成"顺藤摸瓜"任务
- 结构化日志输出

**关键函数**:
- `_initialize_task_queue()`: 初始化任务队列
- `_select_next_task()`: 智能任务选择
- `_generate_influencer_search_tasks()`: 生成博主搜索任务

### 5. nodes/executor.py ✅ (部分修改)
**新增功能**:
- 标记引擎来源 (`raw_data["engine"]`)
- 标记顺藤摸瓜 (`raw_data["from_influencer_search"]`)
- 更新引擎进度 (`engine_progress`)
- 标记任务完成

**新增函数**:
- `_extract_task_id()`: 提取任务ID
- `_extract_engine()`: 提取引擎标识
- `_mark_task_completed()`: 标记任务完成

---

## 🔧 待手动完成的修改

### 6. nodes/influencer_extractor.py (必须修复)
**位置**: 第135行前

**添加去重函数**:

```python
def _deduplicate_influencers(influencers: List) -> List:
    """去重博主，合并相同的"""
    deduped = {}
    for inf in influencers:
        identifier = inf.get("identifier", "").strip().lower() if inf.get("identifier") else inf.get("name", "").lower()
        key = f"{inf.get('platform')}:{identifier}"

        if key in deduped:
            # 合并：累加提及次数，保留最高置信度
            deduped[key]["mention_count"] += inf.get("mention_count", 1)
            if inf.get("confidence") == "high":
                deduped[key]["confidence"] = "high"
        else:
            deduped[key] = inf

    return list(deduped.values())

# 在第135行附近，排序后调用:
sorted_influencers = sorted(result.influencers, key=lambda x: ..., reverse=True)
sorted_influencers = _deduplicate_influencers([inf.model_dump() for inf in sorted_influencers])  # 🔑 新增
print(f"   去重后: {len(sorted_influencers)} 个博主")

# 然后再转换为字典
influencer_dicts = sorted_influencers  # 已经是字典了
```

**同时修改日志输出**:

在文件开头添加:
```python
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from utils.logger import print_influencer_extraction_result
```

在第104行附近替换现有的打印逻辑为:
```python
print_influencer_extraction_result(sorted_influencers, len(web_results))
```

---

### 7. nodes/filter.py (必须修复)
**位置**: 第29-34行

**修改分组逻辑**:

```python
# 🔑 修复: 按引擎分组，不是按source_type
monitor_items = [i for i in state.candidates if
                 i.raw_data.get("engine") == "engine1" or
                 i.raw_data.get("from_influencer_search") or
                 i.source_type in ["youtube_monitor", "bilibili_monitor"]]

hunter_items = [i for i in state.candidates if
                i.raw_data.get("engine") == "engine2" and
                not i.raw_data.get("from_influencer_search")]

print(f"📊 数据分类: 🔴引擎1(顺藤摸瓜) {len(monitor_items)} 条, 🔵引擎2(关键词) {len(hunter_items)} 条")
```

**修改日志输出** (第135行):

在文件开头添加:
```python
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from utils.logger import print_filter_result
```

替换第135行附近的打印为:
```python
print_filter_result(len(state.candidates), len(monitor_items), len(hunter_items), len(top_items))
```

---

### 8. core/graph.py (可选优化)
**位置**: 第32-48行

**修改 planner_router** (可选,现有逻辑也能工作):

```python
def planner_router(state: RadarState):
    """Planner 节点之后的路由"""

    # 🔑 优先检查：如果有 Web 搜索结果且还没有提取过博主，立即提取
    if state.leads and not state.discovered_influencers:
        print("🔄 检测到 Web 搜索结果，准备提取博主...")
        return "influencer_extractor"

    # 如果正在执行工具，去 Executor
    if state.plan_status == "executing":
        return "executor"

    # 如果规划完成（收集到足够数据），进入筛选
    elif state.plan_status == "finished":
        return "filter"

    # 继续规划
    else:
        return "planner"
```

---

## 🚀 测试步骤

### 1. 测试运行
```bash
cd /mnt/c/Users/23732/Desktop/multi-agent-create
python main.py
```

### 2. 观察要点

**初始化阶段**:
```
╔══════════════════════════════════════════════════════════╗
║  系统初始化                                              ║
╚══════════════════════════════════════════════════════════╝

--- 节点: 搜索词设计师 v2.0 ---
✅ 搜索词设计完成:
   主题数量: 2

   [1] 主题: AI nano banana
       🔴 发现博主:
          EN: best AI nano banana channels 2025
          ZH: 顶级AI nano banana博主推荐 2025
       🔵 搜索内容:
          EN: AI nano banana tutorial 2025-11
          ZH: AI nano banana 深度评测 2025

🔧 初始化任务队列...
✅ 初始化 8 个任务:
   🔴 引擎1 (发现博主): 4 个
   🔵 引擎2 (关键词搜索): 4 个
   📺 YouTube: 4 个
   📺 Bilibili: 4 个
```

**执行阶段**:
```
╔══════════════════════════════════════════════════════════╗
║  🔴 引擎1 - 阶段1: 发现博主                              ║
╚══════════════════════════════════════════════════════════╝

📊 双引擎进度仪表盘
────────────────────────────────────────────────────────────
├─ 🔴 引擎1 (头部博主监控):
│  ├─ 发现博主: 0 个
│  │  ├─ YouTube: 0 个
│  │  └─ Bilibili: 0 个
│  └─ 收集内容: 0 条
│
├─ 🔵 引擎2 (关键词搜索):
│  └─ 收集内容: 0 条
│
├─ 📦 平台分布:
│  ├─ YouTube: 0 条
│  └─ Bilibili: 0 条
│
└─ 🎯 总进度: 0/50 条 (0%)
────────────────────────────────────────────────────────────

🔴 选中任务: [engine1] discovery
   任务ID: web_search_AI nano banana_en
   平台: youtube
   工具: web_search
   优先级: 100
   理由: 🔴 [引擎1-发现博主] Web搜索: AI nano banana (英文) → 发现YouTube博主
```

**博主提取**:
```
✅ 博主提取完成:
   分析文章数: 15
   发现博主数: 8 个 (去重后)
   ├─ YouTube: 6 个
   └─ Bilibili: 2 个

   YouTube 博主 (Top 5):
   1. Matt Wolfe (提及4次, 置信度:high)
   2. AI Explained (提及2次, 置信度:high)
   ...

   Bilibili UP主 (Top 2):
   1. 李永乐老师 (提及1次, 置信度:high)
   ...
```

**平台平衡**:
```
⚖️ 平台平衡: YouTube(15) > Bilibili(0), 优先Bilibili任务
🔵 选中任务: [engine2] content_search
   任务ID: bilibili_search_AI nano banana
   平台: bilibili
   ...
```

**最终筛选**:
```
🧹 智能筛选完成:
   输入: 55 条
   ├─ 🔴 引擎1数据: 30 条
   └─ 🔵 引擎2数据: 25 条
   输出: 12 条优质内容
```

### 3. 验证要点

- [ ] 每个主题都有中英文搜索词
- [ ] YouTube和Bilibili都有数据
- [ ] 引擎1和引擎2都有数据
- [ ] 没有重复搜索同一个博主
- [ ] 筛选器有输出（不是0条）
- [ ] 日志清晰可读，能看出双引擎流程

---

## 📝 核心改进总结

### 问题 → 解决方案

| 问题 | 原因 | 解决方案 |
|------|------|---------|
| Bilibili完全没有数据 | 只搜索第一个英文query | 结构化搜索词 + 任务队列 |
| 重复搜索同一博主 | 博主提取未去重 | 添加去重函数 |
| 筛选器过滤所有内容 | 横向标准筛选纵向数据 | 按引擎分组，分开筛选 |
| 顺藤摸瓜锁死工作流 | 确定性逻辑优先级过高 | 任务队列 + 智能调度 |
| 日志混乱 | 缺少结构化输出 | logger工具 + 阶段标识 |
| 用户输入被忽略 | Keyword Designer未使用 | 整合session_focus |

---

## 🎯 预期效果

**重构前**:
```
收集: 60条 (全是YouTube)
去重: 19条
筛选: 0条 ❌
输出: 0个选题 ❌
```

**重构后**:
```
收集: 55条 (YouTube 30 + Bilibili 25)
去重: 45条
筛选: 12条 ✅
  ├─ 引擎1: 7条 (顺藤摸瓜发现的爆款)
  └─ 引擎2: 5条 (关键词搜索的黑马)
输出: 5-8个选题 ✅
```

---

## 🔧 故障排查

### 如果出现导入错误
```
ModuleNotFoundError: No module named 'utils'
```

**解决**: 在每个节点文件开头添加:
```python
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
```

### 如果任务队列为空
检查 `state.topic_queries` 是否生成。在 planner.py 添加调试:
```python
print(f"DEBUG: topic_queries = {len(state.topic_queries)}")
```

### 如果博主重复
检查 influencer_extractor.py 是否添加了去重函数。

---

## 📞 下一步

1. **立即测试**: 运行 `python main.py` 验证基本功能
2. **调整参数**: 根据实际效果调整：
   - planner.py: `TARGET_TOTAL_ITEMS` (当前50)
   - executor.py: `DEFAULT_PARAMS` 中的 limit
   - filter.py: 筛选阈值
3. **监控日志**: 确认双引擎都在工作
4. **反馈问题**: 如有错误，提供完整的错误日志

---

**重构完成！** 🎉

如有问题，请查看 `项目完整文档.md` 或提供错误日志。
