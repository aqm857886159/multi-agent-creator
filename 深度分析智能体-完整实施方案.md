# 深度分析智能体 (Analyst Agent) - 完整实施方案

**版本**: v1.0
**日期**: 2025-11-27
**目标**: 构建模拟顶级研究员的AI智能体，输出深度洞察而非信息堆砌

---

## 📋 执行总览

### 核心定位

**在内容流水线中的角色**: "中央情报局" + "首席分析师"

```
Planner (广度扫描) → Filter (筛选爆款) → 【Analyst (深度洞察)】 → Writer (内容生成)
                                           ↑
                                    你现在在这里
```

**两大核心职责**:
1. **降噪 (De-noise)**: 过滤营销号二手信息，只保留一手硬核事实
2. **升维 (Elevate)**: 将信息通过第一性原理转化为洞察力观点

---

## 🎯 方案设计三大原则

### 1. 通用性 (Generalizability)

**问题**: 选题类型多样（AI技术、商业模式、心理学、财经）

**解决方案**: **动态路由机制**
```python
if topic_category == "技术/AI":
    search_strategy = ["arxiv", "github", "官方文档", "技术博客"]
elif topic_category == "商业/财经":
    search_strategy = ["财报", "研报", "行业数据", "专家访谈"]
elif topic_category == "社会/认知":
    search_strategy = ["书籍", "心理学研究", "访谈", "实验数据"]
```

### 2. 智能性 (Intelligence)

**问题**: 不能只是信息搬运工

**解决方案**: **三级火箭架构**
```
Level 1 (规划): 像研究员一样思考 "去哪找源头"
Level 2 (萃取): 智能清洗，提取金句和数据
Level 3 (分析): 第一性原理 + 思维模型重构
```

### 3. 鲁棒性 (Robustness)

**问题**: 不能因为搜不到资料就崩溃

**解决方案**: **多层降级策略**
```
尝试1: 搜索一手资料（论文/财报）
    ↓ 失败
尝试2: 搜索权威二手（Nature解读/投行研报）
    ↓ 失败
尝试3: 搜索高质量三手（顶级博客/专家访谈）
    ↓ 失败
兜底: 使用现有素材 + LLM推理
```

---

## 🏗️ 技术架构设计

### 总体流程图

```
输入: 3个选题简报 (TopicBrief)
    ↓
┌───────────────────────────────────────┐
│  For Each 选题 (并行处理)              │
│                                       │
│  Step 1: 领域识别 & 动态规划          │
│  ├─ Fast Model判断类型                │
│  └─ 生成ResearchPlan (3-5个搜索任务)  │
│          ↓                            │
│  Step 2: 多源挖掘 & 智能萃取          │
│  ├─ 执行搜索 (Tavily/Arxiv/Scholar)  │
│  ├─ Fast Model清洗原始文本            │
│  └─ 输出KeyInsight卡片                │
│          ↓                            │
│  Step 3: 深度认知重构                │
│  ├─ Reasoning Model推理               │
│  ├─ 第一性原理分析                    │
│  ├─ 冲突挖掘                          │
│  └─ 思维模型匹配                      │
│          ↓                            │
│  输出: DeepAnalysisReport             │
└───────────────────────────────────────┘
    ↓
聚合: 3份深度研报 → Writer Agent
```

### 关键节点详解

#### **节点1: 领域识别 & 动态规划 (The Scout)**

**目标**: 根据选题特征，智能规划搜索策略

**模型选型**: **Fast Model (DeepSeek V3 / GPT-4o-mini)**
- 理由: 简单分类任务，不需要深度推理
- 成本: ~$0.0001/次
- 延迟: <1秒

**输入**:
```python
{
    "title": "3分钟AI海报秒变Sora大片",
    "keywords": ["AI生图", "Sora", "图生视频"],
    "existing_sources": ["B站教程视频", "YouTube评测"]
}
```

**Prompt策略**:
```
Role: Research Planning Expert

Task: Analyze the topic and design a targeted research plan.

Topic: {title}
Keywords: {keywords}

Classification Rules:
1. Technology/AI: Mentions models, algorithms, code, benchmarks
   → Search: Arxiv, GitHub, official docs, tech blogs

2. Business/Finance: Mentions revenue, market, competition, investment
   → Search: Financial reports, analyst reports, market data

3. Social/Psychology: Mentions behavior, cognition, culture, trends
   → Search: Books, academic papers, interviews, surveys

Output Format:
{
  "category": "Technology/AI",
  "reasoning": "Contains AI models (Sora), technical workflow",
  "search_plan": [
    {
      "query": "Sora technical architecture arxiv",
      "source_type": "arxiv",
      "priority": "high",
      "fallback": "Sora technical blog OpenAI"
    },
    {
      "query": "image to video generation state of art 2025",
      "source_type": "scholar",
      "priority": "medium"
    },
    {
      "query": "Sora vs Runway vs Pika comparison",
      "source_type": "web",
      "priority": "low"
    }
  ]
}

Key Principle: 3-5 targeted searches > 20 generic searches
```

**输出**:
```python
class ResearchPlan(BaseModel):
    category: str  # "Technology", "Business", "Social"
    reasoning: str
    search_tasks: List[SearchTask]  # 3-5个精准任务
```

#### **节点2: 多源挖掘 & 智能萃取 (The Excavator)**

**目标**: 执行搜索并清洗数据，提取高信噪比情报

**模型选型**: **Fast Model (DeepSeek V3 / Gemini Flash)**
- 理由: 长文本处理 + 低成本 + 高吞吐
- 成本: ~$0.0005/次 (处理20k tokens)
- 上下文: 128k-256k

**2.1 搜索执行**

调用工具:
```python
# Arxiv搜索
results = arxiv_search(query="Sora video generation", max_results=5)

# Web搜索 (Tavily)
results = tavily_search(
    query="Sora technical analysis",
    include_raw_content=True,  # 关键：获取完整网页文本
    max_results=10
)

# Scholar搜索 (如果有)
results = scholar_search(query="text to video diffusion model")
```

**2.2 智能萃取 (ETL Pipeline)**

**输入**: 20k tokens的杂乱原始文本

**Prompt策略**:
```
Role: Information Extraction Expert

Context: You are processing raw web content to extract key insights.

Raw Text:
{raw_content[:20000]}  # 截断到20k

Research Goal: {topic_title}

Extraction Rules:
1. Focus ONLY on content related to the research goal
2. Ignore:
   - Advertisements, navigation menus, comments
   - Generic claims without data/sources
   - Marketing fluff and buzzwords

3. Extract:
   - Concrete data (numbers, dates, metrics)
   - Direct quotes from experts/papers
   - First-hand facts (not opinions)
   - Contradictions or conflicts with mainstream views

4. Mark source credibility:
   - Primary: Official docs, papers, company reports
   - Secondary: Reputable news, analyst reports
   - Tertiary: Blogs, forums (use cautiously)

Output Format:
{
  "source_name": "OpenAI Technical Report",
  "source_url": "https://...",
  "credibility": "primary",  # primary/secondary/tertiary
  "key_facts": [
    {
      "fact": "Sora uses 3D VAE with 16x temporal compression",
      "quote": "Original text from source...",
      "page_section": "Section 3.2"
    }
  ],
  "conflicts": [
    {
      "mainstream_claim": "Video generation requires 100GB VRAM",
      "counter_evidence": "Sora paper shows 24GB is sufficient with optimization"
    }
  ],
  "data_points": [
    {"metric": "training cost", "value": "$10M", "source": "..."}
  ]
}

Principle: One hard fact with source > Ten vague claims
```

**输出**:
```python
class KeyInsight(BaseModel):
    source_name: str
    source_url: str
    credibility: str  # "primary", "secondary", "tertiary"
    key_facts: List[FactItem]
    conflicts: List[ConflictItem]
    data_points: List[DataPoint]
    relevance_score: float  # 0-1, AI自评
```

#### **节点3: 深度认知重构 (The Philosopher)**

**目标**: 基于事实进行逻辑外科手术，输出洞察

**模型选型**: **Reasoning Model (DeepSeek-R1 / Kimi K2 / Claude 3.5 Sonnet)**
- 理由: 需要深度推理、多步思考、逻辑链条
- 成本: ~$0.01-0.03/次
- 温度: 0.7 (允许一定创造性)

**2025研究洞察**:
- DeepSeek-R1: AIME数学题71%准确率，成本降低96%
- OpenAI o3: 多步推理能力最强
- Kimi K2: 256k上下文 + 长思维链

**Prompt策略** (基于最佳实践):

```
Role: Senior Research Analyst & Strategic Thinker

You are analyzing research materials to produce deep insights, not summaries.

=== INPUT DATA ===

Topic: {topic_title}

Extracted Insights:
{json.dumps(key_insights, indent=2)}

Existing Materials (for reference):
{existing_video_titles}

=== ANALYSIS FRAMEWORK ===

## Phase 1: Fact Verification
- Cross-reference data points across sources
- Flag any contradictions or unverified claims
- Establish what we KNOW for certain

## Phase 2: First Principles Thinking
Apply "5 Whys" to find root causes:

Example for "Why is Sora better at video generation?":
- Why? Better temporal consistency
- Why? Uses 3D VAE instead of 2D
- Why? Captures motion in latent space
- Why? Fundamental physics of video = 3D (x,y,t)
- Root Cause: Architecture matches problem structure

Your Task: Find the PHYSICAL or HUMAN NATURE root cause

## Phase 3: Conflict Mining
Dialectic Method:
1. Identify mainstream view (what most people believe)
2. Find counter-evidence or edge cases
3. Synthesize: Under what conditions is each view correct?

This creates TENSION = engagement

## Phase 4: Mental Model Matching
Automatically identify applicable frameworks:

Technical:
- Moravec's Paradox: Easy for humans, hard for AI (and vice versa)
- Bitter Lesson: Computation > hand-crafted features
- Scaling Laws: Performance = f(compute, data, parameters)

Business:
- Network Effects, Economies of Scale, Moats
- Disruption Theory, Jobs-to-be-Done

Psychology:
- Loss Aversion, Status Games, Social Proof
- System 1 vs System 2 Thinking

## Phase 5: Narrative Bridge
Connect insights to human emotions:
- Greed: "What opportunity am I missing?"
- Fear: "What threat is coming?"
- Laziness: "How can this make my life easier?"
- Status: "How does this affect my position?"

=== OUTPUT REQUIREMENTS ===

Generate a structured analysis report:

```json
{
  "topic_id": "...",

  "verified_facts": [
    "Sora uses 3D VAE with 16x compression [OpenAI Technical Report]",
    "Training cost: $5-10M estimated [Industry Analysis]"
  ],

  "root_cause_analysis": {
    "question": "Why does Sora generate better videos than predecessors?",
    "five_whys": ["...", "...", "...", "...", "..."],
    "root_cause": "Video is fundamentally 3D data (x,y,t), treating it as 3D from the start (3D VAE) matches problem structure",
    "theoretical_basis": "Moravec's Paradox + Scaling Laws"
  },

  "conflict_analysis": {
    "mainstream_view": "High-quality video generation requires 100GB+ VRAM and is only accessible to big tech",
    "counter_evidence": "DeepSeek-R1 paper shows distillation can reduce requirements to 24GB while maintaining 90% quality",
    "synthesis": "Both true: Frontier research needs massive compute, but productionization benefits from distillation. The real moat is training data, not inference cost.",
    "implication": "Small creators can now access near-frontier quality → democratization wave coming"
  },

  "mental_models": [
    {
      "model": "Scaling Laws",
      "application": "Video quality scales predictably with compute/data. Current Sora at ~10^23 FLOPs, next gen at 10^24 will see step-change improvement.",
      "prediction": "By 2026, expect 10x longer videos with consistent characters"
    }
  ],

  "emotional_hooks": {
    "greed": "Early adopters monetizing AI video are seeing $10k/month passive income",
    "fear": "Traditional video editors face 50% income drop if they don't adapt",
    "laziness": "Generate 100 video drafts in 1 hour vs 1 week of manual work",
    "status": "Being 'AI-native creator' is the new flex in 2025"
  },

  "narrative_strategy": {
    "angle": "The Great Video Generation Democratization",
    "structure": "Problem (traditional video is hard) → Breakthrough (Sora tech) → Implication (power shift to individuals) → How-to (practical guide)",
    "contrarian_take": "Most people think AI kills creativity. Reality: AI kills repetitive work, frees creators to focus on storytelling.",
    "call_to_action": "Show viewers exact workflow to create their first AI video"
  }
}
```

=== CHAIN OF THOUGHT (THINK STEP BY STEP) ===

Before generating output, reason through:
1. What are the 3 most important facts?
2. What would a PhD researcher ask about this topic?
3. What would bore the audience vs excite them?
4. What's the ONE insight that changes perspective?

Temperature: 0.7 (allow creative connections)
```

**输出数据结构**:
```python
class DeepAnalysisReport(BaseModel):
    topic_id: str

    # 事实层 (Fact Layer)
    verified_facts: List[str]  # 带引用的硬核数据

    # 逻辑层 (Logic Layer)
    root_cause_analysis: RootCauseAnalysis
    conflict_analysis: ConflictAnalysis
    mental_models: List[MentalModelApplication]

    # 叙事层 (Narrative Layer)
    emotional_hooks: EmotionalHooks
    narrative_strategy: NarrativeStrategy

    # 元数据
    confidence_level: float  # 0-1, 基于一手资料比例
    research_quality: str  # "excellent", "good", "limited"
    limitations: List[str]  # 坦诚说明数据局限性
```

---

## 🔧 模型选型矩阵

### 综合对比 (2025 Q4 数据)

| 模型 | 适用阶段 | 成本/次 | 上下文 | 推理能力 | 速度 | 推荐度 |
|------|---------|--------|--------|---------|------|--------|
| **DeepSeek V3** | 规划+萃取 | $0.0005 | 128k | 3/5 | ⚡⚡⚡ | ⭐⭐⭐⭐⭐ |
| **Gemini Flash** | 萃取 | $0.0003 | 1M | 3/5 | ⚡⚡⚡ | ⭐⭐⭐⭐ |
| **GPT-4o-mini** | 规划 | $0.0002 | 128k | 3/5 | ⚡⚡ | ⭐⭐⭐⭐ |
| **DeepSeek-R1** | 深度分析 | $0.01 | 64k | 5/5 | ⚡⚡ | ⭐⭐⭐⭐⭐ |
| **Kimi K2** | 深度分析 | $0.02 | 256k | 5/5 | ⚡ | ⭐⭐⭐⭐ |
| **Claude 3.5** | 深度分析 | $0.03 | 200k | 5/5 | ⚡⚡ | ⭐⭐⭐⭐ |
| **OpenAI o3** | 深度分析 | $0.10 | 128k | 5/5 | ⚡ | ⭐⭐⭐ |

### 推荐配置方案

#### **方案A: 性价比优先 (推荐)**
```yaml
planning_model: "deepseek/deepseek-chat"  # DeepSeek V3
extraction_model: "deepseek/deepseek-chat"  # 同上，复用
analysis_model: "deepseek/deepseek-reasoner"  # DeepSeek R1

estimated_cost_per_topic: $0.015  # 1.5分钱/选题
quality: 4.5/5
```

#### **方案B: 质量优先**
```yaml
planning_model: "anthropic/claude-3-5-haiku"  # 快速规划
extraction_model: "google/gemini-flash-1.5"  # 长文本处理
analysis_model: "anthropic/claude-3.5-sonnet"  # 顶级推理

estimated_cost_per_topic: $0.05  # 5分钱/选题
quality: 5/5
```

#### **方案C: 长上下文优先**
```yaml
planning_model: "deepseek/deepseek-chat"
extraction_model: "moonshot/kimi-chat"  # 256k上下文
analysis_model: "moonshot/kimi-k2"  # 同系列

estimated_cost_per_topic: $0.03
quality: 4.7/5
```

### 温度 (Temperature) 设置建议

基于Prompt Engineering最佳实践 (2025):

```python
temperature_config = {
    "planning": 0.1,      # 严谨分类，不需要创造性
    "extraction": 0.0,    # 绝对准确，不允许编造
    "analysis": 0.7,      # 允许创造性连接和洞察
}
```

---

## 🛠️ 工具集成清单

### 必需工具 (Must-Have)

#### 1. **Web Search (Tavily)**
```python
# tools/web_search.py 已存在，需确保配置
tavily_search(
    query="Sora technical analysis",
    include_raw_content=True,  # 🔑 关键：返回完整网页文本
    max_results=10,
    search_depth="advanced"  # "basic" or "advanced"
)
```

#### 2. **Arxiv Search**
```python
# tools/scholar_search.py (需新建或确认)
arxiv_search(
    query="video generation diffusion",
    max_results=5,
    sort_by="relevance"  # or "submittedDate"
)
```

### 可选工具 (Nice-to-Have)

#### 3. **YouTube Transcript Fetcher**
```python
# 用于提取视频字幕进行深度分析
get_youtube_transcript(video_id="xxx")
```

#### 4. **PDF Parser**
```python
# 如果搜到财报/研报PDF
parse_pdf_to_text(pdf_url="...")
```

---

## 📐 数据结构定义

### Core Models (核心数据模型)

```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from enum import Enum

# ===== 阶段1: 规划输出 =====

class TopicCategory(str, Enum):
    TECHNOLOGY = "technology"
    BUSINESS = "business"
    SOCIAL = "social"
    MIXED = "mixed"

class SearchTask(BaseModel):
    query: str = Field(..., description="搜索查询")
    source_type: str = Field(..., description="arxiv/web/scholar/youtube")
    priority: str = Field(..., description="high/medium/low")
    fallback_query: Optional[str] = Field(None, description="降级搜索词")

class ResearchPlan(BaseModel):
    topic_id: str
    category: TopicCategory
    reasoning: str = Field(..., description="为什么分类为这个类别")
    search_tasks: List[SearchTask] = Field(..., min_items=3, max_items=5)

# ===== 阶段2: 萃取输出 =====

class FactItem(BaseModel):
    fact: str = Field(..., description="事实陈述")
    quote: Optional[str] = Field(None, description="原文引用")
    page_section: Optional[str] = Field(None, description="来源章节")

class ConflictItem(BaseModel):
    mainstream_claim: str
    counter_evidence: str
    source: str

class DataPoint(BaseModel):
    metric: str  # "training cost", "accuracy", "market size"
    value: str
    source: str

class KeyInsight(BaseModel):
    source_name: str
    source_url: str
    credibility: str = Field(..., description="primary/secondary/tertiary")
    key_facts: List[FactItem]
    conflicts: List[ConflictItem] = Field(default_factory=list)
    data_points: List[DataPoint] = Field(default_factory=list)
    relevance_score: float = Field(..., ge=0, le=1)

# ===== 阶段3: 分析输出 =====

class RootCauseAnalysis(BaseModel):
    question: str
    five_whys: List[str] = Field(..., min_items=5, max_items=5)
    root_cause: str = Field(..., description="底层归因")
    theoretical_basis: str = Field(..., description="理论基础，如:熵增定律")

class ConflictAnalysis(BaseModel):
    mainstream_view: str
    counter_evidence: str
    synthesis: str = Field(..., description="辩证综合")
    implication: str = Field(..., description="对用户的启示")

class MentalModelApplication(BaseModel):
    model: str = Field(..., description="思维模型名称")
    application: str = Field(..., description="如何应用到本话题")
    prediction: Optional[str] = Field(None, description="基于模型的预测")

class EmotionalHooks(BaseModel):
    greed: str = Field(..., description="激发贪婪/机会感")
    fear: str = Field(..., description="激发恐惧/紧迫感")
    laziness: str = Field(..., description="激发懒惰/效率渴望")
    status: str = Field(..., description="激发地位/认同需求")

class NarrativeStrategy(BaseModel):
    angle: str = Field(..., description="叙事角度")
    structure: str = Field(..., description="内容结构建议")
    contrarian_take: str = Field(..., description="反直觉观点")
    call_to_action: str = Field(..., description="行动号召")

class DeepAnalysisReport(BaseModel):
    topic_id: str

    # 事实层
    verified_facts: List[str] = Field(..., description="带引用的硬核事实")

    # 逻辑层
    root_cause_analysis: RootCauseAnalysis
    conflict_analysis: ConflictAnalysis
    mental_models: List[MentalModelApplication]

    # 叙事层
    emotional_hooks: EmotionalHooks
    narrative_strategy: NarrativeStrategy

    # 元数据
    confidence_level: float = Field(..., ge=0, le=1, description="基于一手资料比例")
    research_quality: str = Field(..., description="excellent/good/limited")
    limitations: List[str] = Field(default_factory=list, description="研究局限性")
```

---

## 🔄 降级策略 (Fallback Strategy)

### 问题：搜索不到资料怎么办？

**多层降级机制** (确保系统不崩溃):

```python
async def search_with_fallback(search_task: SearchTask) -> List[KeyInsight]:
    """带降级的搜索策略"""

    # Level 1: 尝试一手资料
    try:
        if search_task.source_type == "arxiv":
            results = await arxiv_search(search_task.query)
            if len(results) >= 2:
                return await extract_insights(results, credibility="primary")
    except Exception as e:
        logger.warning(f"Arxiv search failed: {e}")

    # Level 2: 降级到权威二手
    try:
        fallback_query = search_task.fallback_query or f"{search_task.query} analysis"
        results = await tavily_search(
            fallback_query,
            include_domains=["nature.com", "ieee.org", "techcrunch.com"]
        )
        if len(results) >= 3:
            return await extract_insights(results, credibility="secondary")
    except Exception as e:
        logger.warning(f"Fallback search failed: {e}")

    # Level 3: 进一步降级到高质量三手
    try:
        generic_query = f"{search_task.query} expert opinion 2025"
        results = await tavily_search(generic_query)
        if len(results) >= 1:
            return await extract_insights(results, credibility="tertiary")
    except Exception as e:
        logger.error(f"All searches failed: {e}")

    # Level 4: 兜底 - 使用现有素材
    logger.warning("No external sources found, using existing materials only")
    return []  # 返回空，让分析阶段基于现有素材推理
```

### 质量标记

```python
def calculate_confidence_level(insights: List[KeyInsight]) -> float:
    """计算置信度"""
    if not insights:
        return 0.3  # 仅基于推理

    credibility_weights = {
        "primary": 1.0,
        "secondary": 0.7,
        "tertiary": 0.4
    }

    total_score = sum(
        credibility_weights.get(ins.credibility, 0.5) * ins.relevance_score
        for ins in insights
    )

    return min(total_score / len(insights), 1.0)
```

---

## 📊 实施路线图 (Roadmap)

### Phase 1: 基础设施 (Week 1)

#### Day 1-2: 工具准备
- [ ] 确认 `tools/web_search.py` 支持 `include_raw_content=True`
- [ ] 实现 `tools/arxiv_search.py` (或复用现有scholar工具)
- [ ] 测试工具返回数据格式

#### Day 3-4: 数据结构
- [ ] 创建 `core/analyst_models.py` 定义所有Pydantic模型
- [ ] 编写单元测试确保序列化/反序列化正常

#### Day 5-7: LLM配置
- [ ] 在 `config/models.yaml` 添加Analyst专用配置:
```yaml
analyst:
  planning_model:
    model_id: "deepseek/deepseek-chat"
    temperature: 0.1
    max_tokens: 2000

  extraction_model:
    model_id: "deepseek/deepseek-chat"
    temperature: 0.0
    max_tokens: 4000

  analysis_model:
    model_id: "deepseek/deepseek-reasoner"  # R1
    temperature: 0.7
    max_tokens: 16000
```

### Phase 2: 核心功能 (Week 2)

#### Day 8-10: 实现萃取器
- [ ] 创建 `nodes/content_processor.py`
- [ ] 实现 `extract_insights()` 函数
- [ ] 单元测试：输入20k原始HTML → 输出KeyInsight

#### Day 11-13: 实现分析器
- [ ] 创建 `nodes/analyst.py`
- [ ] 实现 `run_analyst()` 主函数
- [ ] 集成第一性原理Prompt

#### Day 14: 集成测试
- [ ] 端到端测试：TopicBrief → DeepAnalysisReport
- [ ] 验证降级策略是否生效

### Phase 3: 优化与联调 (Week 3)

#### Day 15-17: Prompt优化
- [ ] A/B测试不同Prompt版本
- [ ] 调整温度参数
- [ ] 优化思维模型库

#### Day 18-19: 性能优化
- [ ] 并行处理多个选题
- [ ] 缓存搜索结果（避免重复调用）
- [ ] 监控成本和延迟

#### Day 20-21: 与下游集成
- [ ] 确保输出格式与Writer Agent兼容
- [ ] 添加详细日志和可观测性
- [ ] 编写使用文档

---

## 🧪 测试用例设计

### Test Case 1: 技术类选题

**输入**:
```python
topic = TopicBrief(
    title="3分钟AI海报秒变Sora大片",
    keywords=["AI生图", "Sora", "图生视频"],
    source_videos=[...]
)
```

**期望输出**:
```python
assert report.root_cause_analysis.root_cause.contains("3D VAE")
assert report.conflict_analysis.mainstream_view != ""
assert len(report.verified_facts) >= 3
assert report.confidence_level >= 0.6  # 应该能找到Sora论文
```

### Test Case 2: 商业类选题

**输入**:
```python
topic = TopicBrief(
    title="小红书商业化困境",
    keywords=["小红书", "商业模式", "广告"],
    source_videos=[...]
)
```

**期望输出**:
```python
assert "财报" in str(report.verified_facts) or "市场数据" in str(report.verified_facts)
assert report.mental_models[0].model in ["Network Effects", "Platform Economics"]
assert report.emotional_hooks.fear != ""  # 商业话题应激发恐惧/机会
```

### Test Case 3: 降级场景

**输入**:
```python
topic = TopicBrief(
    title="小众话题：手工陶艺的禅意",
    keywords=["陶艺", "禅意", "手工"],
    source_videos=[...]
)
```

**期望输出**:
```python
assert report.confidence_level <= 0.5  # 一手资料少，置信度低
assert len(report.limitations) >= 1  # 应坦诚说明研究局限
assert report.research_quality == "limited"
# 但仍应有洞察，不能崩溃
assert report.narrative_strategy.angle != ""
```

---

## ⚠️ 风险控制与最佳实践

### 1. 成本控制

**问题**: DeepSeek-R1虽便宜，但大量调用仍有成本

**解决**:
```python
# 设置每选题成本上限
MAX_COST_PER_TOPIC = 0.05  # $0.05

# 搜索结果缓存（24小时）
@lru_cache(maxsize=100)
def cached_search(query: str, source_type: str):
    return execute_search(query, source_type)
```

### 2. 质量监控

**问题**: 如何知道分析质量好不好？

**解决**:
```python
# 内置质量自检
quality_metrics = {
    "has_primary_sources": len([i for i in insights if i.credibility == "primary"]) >= 1,
    "has_data_points": len(all_data_points) >= 3,
    "has_conflicts": report.conflict_analysis.counter_evidence != "",
    "root_cause_depth": len(report.root_cause_analysis.five_whys) == 5
}

quality_score = sum(quality_metrics.values()) / len(quality_metrics)
report.research_quality = "excellent" if quality_score >= 0.8 else \
                          "good" if quality_score >= 0.6 else "limited"
```

### 3. 来源标记

**问题**: Writer需要知道信息来源以增加权威性

**解决**:
```python
# 所有事实必须带引用
verified_facts = [
    "Sora uses 3D VAE with 16x compression [OpenAI Technical Report, p.12]",
    "Training cost estimated $5-10M [MIT Technology Review, Jan 2025]"
]

# 格式: [来源, 具体位置]
```

### 4. 错误处理

**问题**: 网络错误、API限流、模型超时

**解决**:
```python
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def robust_llm_call(prompt: str, model: str):
    try:
        result = await llm.call(prompt, model)
        return result
    except RateLimitError:
        logger.warning("Rate limit hit, waiting...")
        await asyncio.sleep(30)
        raise  # 触发retry
    except TimeoutError:
        logger.error("Model timeout")
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return fallback_response()
```

---

## 📈 成功指标 (Success Metrics)

### 核心KPI

| 指标 | 目标值 | 测量方式 |
|------|--------|----------|
| **一手资料占比** | >30% | `primary sources / total sources` |
| **置信度均值** | >0.6 | `avg(confidence_level)` |
| **成本/选题** | <$0.03 | 实际API消耗 |
| **端到端延迟** | <60秒 | `time(input → output)` |
| **Writer满意度** | >85% | Writer Agent能否顺利生成内容 |

### 质量指标

- **深度**: 每个报告至少3个verified_facts带引用
- **洞察力**: conflict_analysis不能为空
- **可操作性**: narrative_strategy给出具体建议

---

## 🎯 总结：三大核心优势

### 1. 通用性 (Generalizability)
- ✅ 动态路由：技术/商业/社会自动适配
- ✅ 多源兼容：Arxiv/Web/Scholar/YouTube
- ✅ 降级策略：确保永不崩溃

### 2. 智能性 (Intelligence)
- ✅ 三级火箭：规划→萃取→分析
- ✅ 第一性原理：5-Why找根因
- ✅ 思维模型库：自动匹配框架

### 3. 鲁棒性 (Robustness)
- ✅ 多层降级：一手→二手→三手→兜底
- ✅ 质量标记：坦诚说明置信度和局限
- ✅ 错误处理：Retry + 超时保护

---

**下一步行动**: 按照Phase 1路线图开始实施，预计3周完成MVP版本。

**关键风险**: 搜索质量依赖Tavily/Arxiv API稳定性，需准备降级方案。

**预期效果**: 将选题简报升维为深度研报，为Writer Agent提供高质量弹药库。
