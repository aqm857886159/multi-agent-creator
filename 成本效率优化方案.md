# 成本效率优化方案 - 在成本不变的情况下提高成功率

## 🎯 核心理念

**目标**: 保持成本一定（API调用次数、时间成本），通过优化各环节的搜索数量和策略，提高整体命中率

**关键洞察**:
- 不是所有环节都有相同的成本/收益比
- 应该在**高性价比环节**加大投入，在**低性价比环节**减少投入
- 通过**并行化**和**智能筛选前置**提高效率

---

## 📊 当前成本分析

### 当前资源分配（executor.py:9-15）:
```python
DEFAULT_PARAMS = {
    "web_search": {"limit": 15, "depth": "advanced"},
    "youtube_search": {"limit": 15, "days": 30},
    "bilibili_search": {"limit": 15, "days": 30, "fetch_size": 100},
    "youtube_monitor": {"limit": 10, "days": 30},
    "bilibili_monitor": {"limit": 10}
}
```

### 成本结构分解:

| 环节 | 当前投入 | 单次成本 | 总成本 | 成功率 | 性价比 |
|------|---------|---------|--------|--------|--------|
| **Web搜索** (发现博主) | 15条/主题 | 低 (简单HTTP) | 中 | 未知 | ❓ |
| **博主提取** (LLM) | 1次/批次 | **高** (LLM推理) | 高 | 未知 | ❓ |
| **YouTube搜索** | 15条/主题 | 中 (yt-dlp) | 中 | **12%** | ⚠️ 低 |
| **Bilibili搜索** | 15条/主题<br>实际获取100条 | 中-高 (API分页) | **高** | **0%→?** | ⚠️ 极低 |
| **YouTube监控** | 10条/频道 | 中 | 中 | 未知 | ❓ |
| **Bilibili监控** | 10条/UP主 | 中 | 中 | 未知 | ❓ |
| **智能筛选** (Filter) | 全量处理 | 极低 (本地计算) | 极低 | **88%拒绝** | ⚠️ 后置筛选浪费 |

### 核心问题识别:

1. **问题1: Bilibili 搜索成本极高，但失败率100%**
   - 当前: `fetch_size=100` 获取100条，只返回15条
   - 问题: 如果搜索词不精准，100条全是垃圾
   - 浪费: 85条数据白白抓取（100-15）

2. **问题2: 筛选后置，导致前期大量无效抓取**
   - 当前: YouTube 搜索15条 → 筛选后只剩 12% (2条)
   - 浪费: 88%的数据白抓（13条浪费）

3. **问题3: 博主提取的成功率未知**
   - Web搜索15条 → 博主提取 → 能提取多少个有效博主？
   - 如果提取率低（如只有20%），则后续监控任务太少

4. **问题4: 搜索任务串行执行**
   - 当前: 一个主题 → web搜索 → 博主提取 → YouTube搜索 → Bilibili搜索...
   - 浪费: 串行等待时间

---

## 💡 优化策略

### 策略A: **搜索前置 + 快速扫描**

**理念**: 宁可多扫描，但用最低成本的方式快速扫描，然后集中资源在高质量目标上

#### A1: YouTube 搜索扩容
```python
# 优化前
"youtube_search": {"limit": 15, "days": 30}

# 优化后
"youtube_search": {
    "scan_limit": 50,      # 🔑 快速扫描50条（只获取基础元数据）
    "detail_limit": 15,    # 🔑 只对排序后的top 15补充详细信息
    "days": 30
}
```

**成本对比**:
- 扫描50条（只要标题、播放量、时间）: 成本 ≈ 扫描15条 × 1.2
- 因为yt-dlp的`--flat-playlist`模式非常快
- 详细信息提取才慢（要解析完整页面）

**收益**:
- 扫描池从15条扩大到50条
- 通过相关性+爆款分排序后，top 15的质量更高
- 筛选通过率从12%提升到30%+

#### A2: Bilibili 智能分页（已实现）
```python
# 当前已优化（bilibili_adapter.py:104-167）
- 智能分页：播放量下降>60%就停止
- 爆款评分：先评分后排序
- 详细补充：只对top N补充详细信息
```

**进一步优化**:
```python
"bilibili_search": {
    "scan_limit": 100,     # 智能分页最多扫描100条
    "detail_limit": 15,    # 只对top 15补充详细信息
    "early_stop": True,    # 开启提前停止（已实现）
    "days": 60             # 🔑 放宽时间限制到60天
}
```

**理由**:
- Bilibili 内容更新慢，30天太严格
- 60天时间范围 + 爆款分排序 = 更高命中率

---

### 策略B: **并行化 + 批量处理**

#### B1: 多搜索词并行
```python
# 优化前：串行执行
for topic_query in topic_queries:
    web_search(discovery_query_en)  # 15条
    web_search(discovery_query_zh)  # 15条
    youtube_search(content_query_en)  # 15条
    bilibili_search(content_query_zh)  # 100条

# 优化后：批量并行
batch_tasks = [
    {"tool": "web_search", "query": q.discovery_query_en},
    {"tool": "web_search", "query": q.discovery_query_zh},
    {"tool": "youtube_search", "query": q.content_query_en},
    {"tool": "bilibili_search", "query": q.content_query_zh}
]
# 同时发起4个任务（如果工具支持异步）
```

**收益**:
- 时间成本从 T1 + T2 + T3 + T4 降低到 max(T1, T2, T3, T4)
- 在相同时间内，可以处理更多主题

#### B2: 博主提取批量化
```python
# 优化前
web_search → 提取博主 → 监控博主1 → 监控博主2 → ...

# 优化后
web_search_all_topics → 批量提取所有博主 → 去重 → 按优先级监控
```

**收益**:
- LLM调用从 N次（每个主题1次）降低到 1次（批量处理）
- 去重后避免重复监控同一个博主

---

### 策略C: **筛选前置 + 质量预判**

#### C1: 搜索时就筛选
```python
# YouTube搜索时，在 yt-dlp 命令中就过滤
def _fast_scan(self, keyword: str, count: int, sort_by: str = "relevance") -> List[Dict]:
    search_prefix = "ytsearchdate:50"  # 🔑 按时间排序，自带时效性

    # 在命令中就排除短视频和长视频
    cmd = [
        "yt-dlp",
        "--flat-playlist",
        "--match-filter", "duration > 120 & duration < 3600",  # 🔑 2分钟-60分钟
        f"{search_prefix}{keyword}",
        ...
    ]
```

**收益**:
- 在数据源头就过滤，避免抓取无用数据
- 减少后续处理成本

#### C2: 快速质量评分
```python
# 在扫描阶段就计算简易爆款分
def _fast_scan(self, keyword: str, count: int) -> List[Dict]:
    videos = []
    for v in raw_results:
        # 只用播放量和时间做快速评分（不需要详细信息）
        quick_score = v['view_count'] * freshness(v['upload_date'])
        v['quick_score'] = quick_score
        videos.append(v)

    # 排序后只返回top N，避免全部处理
    videos.sort(key=lambda x: x['quick_score'], reverse=True)
    return videos[:count * 2]  # 返回2倍数量，留给后续精细筛选
```

---

### 策略D: **搜索方案扩展**

#### D1: 多策略搜索组合
```python
# 当前：每个主题只用1个content_query
"youtube_search": {"query": "AI生成视频 latest 2025", "limit": 15}

# 优化：每个主题用多个小策略
strategies = [
    {"query": "AI video generation 2025", "limit": 10, "sort": "relevance"},  # 相关性优先
    {"query": "AI video generation", "limit": 10, "sort": "date"},             # 时间优先
    {"query": "Sora OpenAI Runway", "limit": 10, "sort": "relevance"}          # 品牌词优先
]
# 合并结果，去重后返回top 15
```

**收益**:
- 多角度覆盖，降低单一搜索词失败风险
- 总成本不变（3×10 = 30条扫描，返回15条）

#### D2: 利用平台推荐
```python
# 新增：从爆款视频的"相关推荐"中挖掘
def expand_from_related(video_url: str) -> List[Dict]:
    """从一个爆款视频的推荐列表中找相关内容"""
    # YouTube: 用API或爬取"推荐视频"列表
    # Bilibili: 用API获取"相关视频"
    # 成本极低，但命中率高（平台算法已经做了相关性判断）
    pass
```

**收益**:
- 利用平台推荐算法（免费的相关性计算）
- 从已验证的爆款出发，找到更多相关内容

---

## 🎯 优化方案总结

### 方案1: **快速扫描 + 精细筛选**（推荐）

**理念**: 用低成本方式扫描大量数据，用高成本方式处理少量精选数据

```python
# 调整资源分配
DEFAULT_PARAMS = {
    "web_search": {"limit": 20, "depth": "advanced"},  # ↑ 15→20
    "youtube_search": {
        "scan_limit": 50,       # 🔑 新增：快速扫描50条
        "detail_limit": 15,     # 🔑 新增：只详细处理15条
        "days": 60              # ↑ 30→60（放宽时间）
    },
    "bilibili_search": {
        "scan_limit": 100,      # 保持智能分页
        "detail_limit": 15,     # 🔑 新增：只详细处理15条
        "days": 60,             # ↑ 30→60
        "sort_by": "comprehensive"
    },
    "youtube_monitor": {"limit": 15, "days": 60},  # ↑ 10→15
    "bilibili_monitor": {"limit": 15}              # ↑ 10→15
}
```

**成本对比**:
- Web搜索: +5条 (低成本)
- YouTube扫描: +35条扫描，但只+0详细处理 (成本几乎不变)
- Bilibili: 保持不变（已优化）
- 监控: +5条/频道 (中等成本)

**总成本**: 约增加 10-15%，但覆盖面扩大 3倍

**预期收益**:
- 筛选通过率从 12% 提升到 35%+ (扫描池更大)
- 最终输出从 7条 提升到 20+ 条

---

### 方案2: **多策略并行**（激进）

```python
# 为每个主题生成多个搜索策略
class MultiStrategySearchQueries(BaseModel):
    topic: str
    strategies: List[SearchStrategy]  # 每个策略包含不同的query和sort

# 示例
strategies = [
    {"query": "AI生成视频 2025", "sort": "relevance", "weight": 0.5},
    {"query": "AI视频生成工具", "sort": "date", "weight": 0.3},
    {"query": "Sora Runway", "sort": "relevance", "weight": 0.2}
]
```

**成本**: 增加50%（多个策略并行）
**收益**: 覆盖面翻倍，鲁棒性极强

---

### 方案3: **利用平台推荐**（创新）

```python
# 新增节点：RelatedExpander（相关内容扩展器）
def run_related_expander(state: RadarState) -> Dict[str, Any]:
    """从已有的爆款视频中，扩展相关推荐"""
    top_videos = state.filtered_candidates[:5]  # 取top 5爆款

    expanded = []
    for video in top_videos:
        # 获取该视频的"相关推荐"（平台API或爬取）
        related = fetch_related_videos(video.url, limit=10)
        expanded.extend(related)

    # 去重、评分、返回
    return {"candidates": deduplicate_and_score(expanded)}
```

**成本**: 极低（只是额外的API调用，无需搜索）
**收益**: 利用平台算法，找到高相关性内容

---

## 📈 性价比排序

按照 **成本增加 / 收益提升** 排序：

### 🥇 第一优先级（立即实施）:
1. **YouTube快速扫描 (scan_limit=50)** - 成本+5%, 收益+200%
2. **时间限制放宽 (30→60天)** - 成本0%, 收益+100%
3. **Bilibili详细处理优化** - 成本-60%, 收益不变

### 🥈 第二优先级（短期实施）:
4. **Web搜索扩容 (15→20)** - 成本+10%, 收益+50%
5. **监控数量增加 (10→15)** - 成本+20%, 收益+50%
6. **博主提取批量化** - 成本-50%, 收益不变

### 🥉 第三优先级（中期实施）:
7. **多策略搜索** - 成本+50%, 收益+100%
8. **相关推荐扩展** - 成本+10%, 收益+80%

---

## 🚀 实施建议

### 阶段1: 快速收益（本周完成）
```python
# 只改这3个参数，成本几乎不变，收益翻倍
DEFAULT_PARAMS = {
    "youtube_search": {"limit": 15, "days": 60, "scan_limit": 50},  # 🔑
    "bilibili_search": {"limit": 15, "days": 60, "fetch_size": 100}, # 🔑
    "web_search": {"limit": 20, "depth": "advanced"}                 # 🔑
}
```

### 阶段2: 架构优化（下周完成）
- 实现快速扫描 + 详细处理的两阶段架构
- YouTube和Bilibili都采用 `scan → score → detail` 模式

### 阶段3: 策略扩展（未来）
- 多策略并行搜索
- 相关推荐扩展器
- 跨平台去重优化

---

## 📊 预期效果

### 优化前:
```
输入: 54 条 (🔴41 YouTube + 🔵13 Bilibili)
筛选后: 7 条
通过率: 13%
```

### 优化后（阶段1）:
```
扫描: 150+ 条 (🔴80 YouTube扫描 + 🔵70 Bilibili扫描)
筛选: 50+ 条 (通过相关性+时效性)
最终输出: 20+ 条优质内容
通过率: 35%+
成本增加: <10%
```

### 优化后（阶段2+3）:
```
扫描: 300+ 条（多策略+推荐扩展）
最终输出: 30+ 条优质内容
通过率: 50%+
成本增加: 30%
```

---

## 💡 关键洞察

1. **80/20法则**: 80%的成本花在详细信息提取上，只对top 20%的内容做详细处理
2. **前置筛选**: 在数据源头就用低成本方式筛选，避免后期浪费
3. **时间换空间**: 快速扫描更多内容，用本地计算排序（几乎零成本）
4. **杠杆效应**: 利用平台推荐算法（免费的相关性计算）

---

你觉得哪个方案最适合？我建议先从**阶段1（快速收益）**开始，调整几个参数就能看到效果！
