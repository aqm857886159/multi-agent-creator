"""
è§„åˆ’å¤§è„‘ v2.2 - ä»»åŠ¡è°ƒåº¦å™¨
æ ¸å¿ƒæ”¹è¿›:
1. ä»»åŠ¡é˜Ÿåˆ—åŒ–ç®¡ç†
2. æ™ºèƒ½ä»»åŠ¡é€‰æ‹©ï¼ˆå¹³å°å¹³è¡¡ + å¼•æ“å¹³è¡¡ï¼‰
3. ç»“æ„åŒ–æ—¥å¿—è¾“å‡º
4. ğŸ”‘ P1: é›†æˆ PlatformBalancer å¼ºåˆ¶å¹³è¡¡
5. ğŸ”‘ P1: å¤è¿°æœºåˆ¶ï¼ˆç›®æ ‡æé†’ï¼‰
6. ğŸ”‘ P2: åŠ¨æ€å·¥å…·å±è”½ (ToolMasker)
7. ğŸ”‘ P1: ä¸Šä¸‹æ–‡å‹ç¼© (ContextCompressor)
"""

from typing import Dict, Any, List, Optional, Tuple
from pydantic import BaseModel, Field
from core.state import RadarState, TaskItem, TopicSearchQueries
from core.llm import get_llm_with_schema
from core.platform_balancer import (
    get_platform_balancer, 
    get_balance_summary,
    BalanceMode
)
# ğŸ”‘ P2: åŠ¨æ€å·¥å…·å±è”½
from core.tool_masker import (
    get_masked_tools,
    get_tool_descriptions,
    get_tool_hints
)
# ğŸ”‘ P1: ä¸Šä¸‹æ–‡å‹ç¼©
from core.context_compressor import (
    compress_state,
    should_compress
)
# ğŸ”‘ P0: Prompt ç®¡ç†
from core.prompt_manager import (
    get_prompt,
    build_goal_recap,
    build_state_summary,
    build_error_summary
)
from datetime import datetime
import sys
import os

# æ·»åŠ  utils åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from utils.logger import (
    print_phase_header,
    print_progress_dashboard,
    print_task_selected,
    print_task_queue_status,
    print_separator
)

TARGET_TOTAL_ITEMS = 50
MAX_PLAN_STEPS = 50

# ğŸ”‘ P1: å…¨å±€å¹³å°å¹³è¡¡å™¨
_balancer = get_platform_balancer()


class ToolCall(BaseModel):
    tool_name: str = Field(..., description="è°ƒç”¨çš„å·¥å…·åç§°")
    arguments: Dict[str, Any] = Field(..., description="å·¥å…·è°ƒç”¨å‚æ•°")
    reasoning: str = Field(..., description="ä¸ºä»€ä¹ˆç°åœ¨éœ€è¦è°ƒç”¨è¿™ä¸ªå·¥å…·")


class PlannerOutput(BaseModel):
    thought: str = Field(..., description="å¯¹å½“å‰æƒ…å†µçš„åˆ†æ")
    action: Optional[ToolCall] = Field(None, description="è¦æ‰§è¡Œçš„å·¥å…·è°ƒç”¨")
    is_finished: bool = Field(False, description="æ˜¯å¦å·²æ”¶é›†åˆ°è¶³å¤Ÿçš„æ•°æ®")


def run_planner(state: RadarState) -> Dict[str, Any]:
    """
    è§„åˆ’å™¨ v2.1 - ä»»åŠ¡è°ƒåº¦å™¨

    æ ¸å¿ƒé€»è¾‘:
    1. é˜¶æ®µç®¡ç† (init â†’ discovery â†’ collection â†’ filtering)
    2. ä»»åŠ¡é˜Ÿåˆ—åˆå§‹åŒ–
    3. æ™ºèƒ½ä»»åŠ¡é€‰æ‹©
    4. ç»“æ„åŒ–æ—¥å¿—è¾“å‡º
    5. ğŸ”‘ P1: å¤è¿°æœºåˆ¶ï¼ˆç›®æ ‡æé†’ï¼‰
    6. ğŸ”‘ P1: é”™è¯¯å†å²å‚è€ƒ
    """
    collected = len(state.candidates)

    # ğŸ”‘ P1: å¤è¿°æœºåˆ¶ - æ¯æ¬¡è¿­ä»£æ‰“å°ç›®æ ‡æé†’ï¼ˆManusæœ€ä½³å®è·µï¼‰
    _print_goal_recap(state, collected)

    # åªåœ¨åˆå§‹åŒ–æˆ–è¾¾åˆ°ç›®æ ‡æ—¶æ‰“å°è¯¦ç»†ä»ªè¡¨ç›˜
    if state.current_phase == "init" or collected >= TARGET_TOTAL_ITEMS:
        print(f"\n{'='*60}")
        print(f"ğŸ“ {state.current_phase.upper()}")
        print_progress_dashboard(state)

    # ============ é˜¶æ®µ1: åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ— ============
    if state.current_phase == "init":
        if state.topic_queries:
            _initialize_task_queue(state)
            state.current_phase = "discovery"
            print("âœ… åˆå§‹åŒ–å®Œæˆ\n")
            return {
                "plan_status": "planning",
                "current_phase": "discovery",
                "task_queue": state.task_queue
            }
        else:
            return {"plan_status": "planning"}

    # ============ é˜¶æ®µ2: æ£€æŸ¥å®Œæˆæ¡ä»¶ ============
    if collected >= TARGET_TOTAL_ITEMS:
        print(f"ğŸ¯ ç›®æ ‡è¾¾æˆ: å·²æ”¶é›† {collected}/{TARGET_TOTAL_ITEMS} æ¡")
        state.current_phase = "filtering"
        return {
            "plan_status": "finished",
            "current_phase": "filtering"
        }

    if len(state.plan_scratchpad) >= MAX_PLAN_STEPS:
        print(f"âš ï¸ è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ ({MAX_PLAN_STEPS}), è¿›å…¥ç­›é€‰")
        state.current_phase = "filtering"
        return {
            "plan_status": "finished",
            "current_phase": "filtering"
        }

    # ============ é˜¶æ®µ2.5: è‡ªé€‚åº”åé¦ˆæ£€æŸ¥ ============
    # ğŸ”‘ æ£€æŸ¥ä¸Šä¸€ä¸ªä»»åŠ¡çš„è´¨é‡åé¦ˆï¼Œå†³å®šæ˜¯å¦éœ€è¦é‡è¯•
    if state.feedback_enabled and state.quality_checks:
        retry_task = _check_quality_feedback_and_retry(state)
        if retry_task:
            # æœ‰é‡è¯•ä»»åŠ¡ï¼Œä¼˜å…ˆæ‰§è¡Œ
            next_task = retry_task
        else:
            # æ²¡æœ‰é‡è¯•ä»»åŠ¡ï¼Œæ­£å¸¸é€‰æ‹©
            next_task = _select_next_task(state)
    else:
        # åé¦ˆç³»ç»Ÿå…³é—­æˆ–æ— æ£€æŸ¥å†å²ï¼Œæ­£å¸¸é€‰æ‹©
        next_task = _select_next_task(state)

    # ============ é˜¶æ®µ3: ä»»åŠ¡æ‰§è¡Œ ============
    # next_task = _select_next_task(state)  # ç§»é™¤é‡å¤

    if next_task:
        # ç®€åŒ–ä»»åŠ¡æ—¥å¿—
        engine_icon = "ğŸ”´" if next_task.engine == "engine1" else "ğŸ”µ"
        print(f"{engine_icon} {next_task.tool_name} | {next_task.arguments.get('query') or next_task.arguments.get('keyword', '')[:40]}")

        # æ ‡è®°ä¸ºè¿›è¡Œä¸­
        next_task.status = "in_progress"

        # åˆ›å»ºtool_call
        action = ToolCall(
            tool_name=next_task.tool_name,
            arguments=next_task.arguments,
            reasoning=next_task.reasoning
        )

        # æ·»åŠ ä»»åŠ¡IDåˆ°reasoningä¸­ï¼ˆç”¨äºåç»­æ ‡è®°å®Œæˆï¼‰
        action.reasoning = f"[{next_task.task_id}] {action.reasoning}"

        state.plan_scratchpad.append({"tool_call": action.model_dump()})

        return {
            "plan_status": "executing",
            "task_queue": state.task_queue,
            "plan_scratchpad": state.plan_scratchpad  # ğŸ”‘ å¿…é¡»è¿”å›ä»¥ä¿å­˜ä¿®æ”¹ï¼
        }

    # ============ é˜¶æ®µ4: åŠ¨æ€ä»»åŠ¡ç”Ÿæˆ ============
    # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºä½†è¿˜æ²¡è¾¾åˆ°ç›®æ ‡ï¼Œè®©LLMè¡¥å……ä»»åŠ¡
    if collected < TARGET_TOTAL_ITEMS:
        print("ğŸ“‹ ä»»åŠ¡é˜Ÿåˆ—ä¸ºç©ºï¼Œå°è¯•ç”Ÿæˆæ–°ä»»åŠ¡...")
        new_tasks = _llm_generate_tasks(state)

        if new_tasks:
            added_count = _add_tasks_with_deduplication(state, new_tasks)
            print(f"âœ… ç”Ÿæˆ {added_count} ä¸ªæ–°ä»»åŠ¡")
            return {
                "plan_status": "planning",
                "task_queue": state.task_queue
            }

    # ============ é˜¶æ®µ5: æ— ä»»åŠ¡å¯æ‰§è¡Œï¼Œç»“æŸ ============
    print("âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œè¿›å…¥ç­›é€‰é˜¶æ®µ")
    state.current_phase = "filtering"
    return {
        "plan_status": "finished",
        "current_phase": "filtering"
    }


def _is_duplicate_task(task: TaskItem, state: RadarState) -> bool:
    """
    æ£€æŸ¥ä»»åŠ¡æ˜¯å¦é‡å¤

    ç­–ç•¥:
    1. æ£€æŸ¥ task_id æ˜¯å¦å·²åœ¨é˜Ÿåˆ—æˆ–å®Œæˆåˆ—è¡¨ä¸­
    2. æ£€æŸ¥ç›¸åŒ tool_name + arguments ç»„åˆæ˜¯å¦å·²å­˜åœ¨

    Returns:
        True if duplicate, False if unique
    """
    # æ£€æŸ¥1: task_idå·²å®Œæˆ
    if task.task_id in state.completed_tasks:
        return True

    # æ£€æŸ¥2: task_idå·²åœ¨é˜Ÿåˆ—ä¸­
    existing_ids = {t.task_id for t in state.task_queue}
    if task.task_id in existing_ids:
        return True

    # æ£€æŸ¥3: ç›¸åŒå·¥å…·+å‚æ•°ç»„åˆï¼ˆé˜²æ­¢å‚æ•°å®Œå…¨ç›¸åŒçš„é‡å¤ä»»åŠ¡ï¼‰
    for existing_task in state.task_queue:
        if (existing_task.tool_name == task.tool_name and
            existing_task.arguments == task.arguments and
            existing_task.status in ["pending", "in_progress"]):
            return True

    return False


def _add_tasks_with_deduplication(state: RadarState, new_tasks: List[TaskItem]) -> int:
    """
    æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—ï¼Œè‡ªåŠ¨å»é‡

    Returns:
        å®é™…æ·»åŠ çš„ä»»åŠ¡æ•°é‡
    """
    added_count = 0
    duplicate_count = 0

    for task in new_tasks:
        if _is_duplicate_task(task, state):
            duplicate_count += 1
            print(f"   âš ï¸ è·³è¿‡é‡å¤ä»»åŠ¡: {task.task_id}")
        else:
            state.task_queue.append(task)
            added_count += 1

    if duplicate_count > 0:
        print(f"   ğŸ” å»é‡: è·³è¿‡ {duplicate_count} ä¸ªé‡å¤ä»»åŠ¡ï¼Œæ–°å¢ {added_count} ä¸ª")

    return added_count


def _initialize_task_queue(state: RadarState):
    """
    åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—

    ç­–ç•¥:
    1. ä¼˜å…ˆçº§: discovery (å‘ç°åšä¸») > content_search (å…³é”®è¯æœç´¢)
    2. ä¸­è‹±æ–‡é…å¯¹: æ¯ä¸ªä¸»é¢˜ç”Ÿæˆ4ä¸ªä»»åŠ¡
    3. å¹³å°è½®æ¢: YouTubeå’ŒBilibiliäº¤æ›¿
    """
    tasks = []
    priority = 100  # ä»é«˜åˆ°ä½

    print("\nğŸ”§ åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—...")

    for topic_query in state.topic_queries:
        topic = topic_query.get("topic", "unknown")

        # ========== å¼•æ“1: å‘ç°åšä¸»ä»»åŠ¡ ==========
        # ä¼˜å…ˆçº§æœ€é«˜

        # ä»»åŠ¡1: Webæœç´¢ - è‹±æ–‡ï¼ˆYouTubeåšä¸»ï¼‰
        tasks.append(TaskItem(
            task_id=f"web_search_{topic}_en",
            task_type="discovery",
            priority=priority,
            engine="engine1",
            platform="youtube",
            tool_name="web_search",
            arguments={
                "query": topic_query.get("discovery_query_en"),
                "limit": 10
            },
            status="pending",
            reasoning=f"ğŸ”´ [å¼•æ“1-å‘ç°åšä¸»] Webæœç´¢: {topic} (è‹±æ–‡) â†’ å‘ç°YouTubeåšä¸»"
        ))
        priority -= 1

        # ä»»åŠ¡2: Webæœç´¢ - ä¸­æ–‡ï¼ˆBilibiliåšä¸»ï¼‰
        tasks.append(TaskItem(
            task_id=f"web_search_{topic}_zh",
            task_type="discovery",
            priority=priority,
            engine="engine1",
            platform="bilibili",
            tool_name="web_search",
            arguments={
                "query": topic_query.get("discovery_query_zh"),
                "limit": 10
            },
            status="pending",
            reasoning=f"ğŸ”´ [å¼•æ“1-å‘ç°åšä¸»] Webæœç´¢: {topic} (ä¸­æ–‡) â†’ å‘ç°Bilibili UPä¸»"
        ))
        priority -= 1

        # ========== å¼•æ“2: å…³é”®è¯æœç´¢ä»»åŠ¡ ==========
        # ä¼˜å…ˆçº§ç•¥ä½

        # ä»»åŠ¡3: YouTubeå†…å®¹æœç´¢
        tasks.append(TaskItem(
            task_id=f"youtube_search_{topic}",
            task_type="content_search",
            priority=priority - 20,  # ä½äºå‘ç°åšä¸»
            engine="engine2",
            platform="youtube",
            tool_name="youtube_search",
            arguments={
                "keyword": topic_query.get("content_query_en"),
                "limit": 10
            },
            status="pending",
            reasoning=f"ğŸ”µ [å¼•æ“2-å…³é”®è¯æœç´¢] YouTubeæœç´¢: {topic}"
        ))

        # ä»»åŠ¡4: Bilibiliå†…å®¹æœç´¢
        tasks.append(TaskItem(
            task_id=f"bilibili_search_{topic}",
            task_type="content_search",
            priority=priority - 20,
            engine="engine2",
            platform="bilibili",
            tool_name="bilibili_search",
            arguments={
                "keyword": topic_query.get("content_query_zh"),
                "limit": 10
            },
            status="pending",
            reasoning=f"ğŸ”µ [å¼•æ“2-å…³é”®è¯æœç´¢] Bilibiliæœç´¢: {topic}"
        ))

        priority -= 30  # ä¸‹ä¸€ä¸ªä¸»é¢˜çš„ä¼˜å…ˆçº§æ›´ä½

    state.task_queue = tasks

    # ç»Ÿè®¡
    engine1_tasks = len([t for t in tasks if t.engine == "engine1"])
    engine2_tasks = len([t for t in tasks if t.engine == "engine2"])
    youtube_tasks = len([t for t in tasks if t.platform == "youtube"])
    bilibili_tasks = len([t for t in tasks if t.platform == "bilibili"])

    print(f"   ğŸ“‹ ä»»åŠ¡: {len(tasks)} ä¸ª (ğŸ”´{engine1_tasks} ğŸ”µ{engine2_tasks} | YT:{youtube_tasks} BL:{bilibili_tasks})")


def _select_next_task(state: RadarState) -> Optional[TaskItem]:
    """
    æ™ºèƒ½ä»»åŠ¡é€‰æ‹© v2.1
    
    ğŸ”‘ P1 æ”¹è¿›: ä½¿ç”¨ PlatformBalancer è¿›è¡Œæ›´æ™ºèƒ½çš„å¹³è¡¡

    ç­–ç•¥:
    1. å¹³å°å¹³è¡¡: ä½¿ç”¨ PlatformBalancerï¼ˆè‡ªé€‚åº”æ¨¡å¼ï¼‰
    2. å¼•æ“å¹³è¡¡: å¦‚æœå¼•æ“1æ¯”å¼•æ“2å¤š10æ¡ï¼Œä¼˜å…ˆé€‰å¼•æ“2ä»»åŠ¡
    3. ä¼˜å…ˆçº§æ’åº: åœ¨æ»¡è¶³å¹³è¡¡çš„å‰æä¸‹ï¼Œé€‰æ‹©æœ€é«˜ä¼˜å…ˆçº§ä»»åŠ¡
    4. åŠ¨æ€ç”Ÿæˆé¡ºè—¤æ‘¸ç“œä»»åŠ¡
    """
    # è·å–å¾…æ‰§è¡Œä»»åŠ¡
    pending_tasks = [t for t in state.task_queue if t.status == "pending"]

    if not pending_tasks:
        # ğŸ”‘ æ£€æŸ¥æ˜¯å¦æœ‰åšä¸»éœ€è¦"é¡ºè—¤æ‘¸ç“œ"
        influencer_tasks = _generate_influencer_search_tasks(state)
        if influencer_tasks:
            added_count = _add_tasks_with_deduplication(state, influencer_tasks)
            if added_count > 0:
                # è¿”å›ç¬¬ä¸€ä¸ªæ–°æ·»åŠ çš„ä»»åŠ¡
                return next((t for t in state.task_queue if t.status == "pending"), None)
        return None

    # ========== ç­–ç•¥1: å¹³å°å¹³è¡¡ï¼ˆä½¿ç”¨ PlatformBalancerï¼‰==========
    # è·å–å¹³å°ç»Ÿè®¡
    stats = _balancer.get_stats(state.candidates, state.task_queue)
    
    # è·å–å¯ç”¨å¹³å°
    available_platforms = list(set(t.platform for t in pending_tasks if t.platform in ["youtube", "bilibili"]))
    
    # è®©å¹³è¡¡å™¨å†³å®šä¼˜å…ˆå¹³å°
    preferred_platform = _balancer.select_platform(stats, available_platforms)
    
    if preferred_platform:
        platform_tasks = [t for t in pending_tasks if t.platform == preferred_platform]
        if platform_tasks:
            selected = max(platform_tasks, key=lambda t: t.priority)
            print(f"   âš–ï¸ å¹³å°å¹³è¡¡ â†’ {preferred_platform.upper()} (YT:{stats.youtube_count} BL:{stats.bilibili_count})")
            _balancer.record_execution(preferred_platform)
            return selected

    # ========== ç­–ç•¥2: å¼•æ“å¹³è¡¡ ==========
    engine1_count = state.engine_progress.get("engine1", 0)
    engine2_count = state.engine_progress.get("engine2", 0)

    if engine1_count > engine2_count + 10:
        engine2_tasks = [t for t in pending_tasks if t.engine == "engine2"]
        if engine2_tasks:
            selected = max(engine2_tasks, key=lambda t: t.priority)
            print(f"   âš–ï¸ å¼•æ“å¹³è¡¡ â†’ å¼•æ“2 (E1:{engine1_count} > E2:{engine2_count})")
            _balancer.record_execution(selected.platform)
            return selected

    if engine2_count > engine1_count + 10:
        engine1_tasks = [t for t in pending_tasks if t.engine == "engine1"]
        if engine1_tasks:
            selected = max(engine1_tasks, key=lambda t: t.priority)
            print(f"   âš–ï¸ å¼•æ“å¹³è¡¡ â†’ å¼•æ“1 (E2:{engine2_count} > E1:{engine1_count})")
            _balancer.record_execution(selected.platform)
            return selected

    # ========== ç­–ç•¥3: é»˜è®¤ä¼˜å…ˆçº§ ==========
    selected = max(pending_tasks, key=lambda t: t.priority)
    _balancer.record_execution(selected.platform)
    return selected


def _check_quality_feedback_and_retry(state: RadarState) -> Optional[TaskItem]:
    """
    ğŸ”‘ è‡ªé€‚åº”åé¦ˆå¾ªç¯ï¼šæ£€æŸ¥è´¨é‡åé¦ˆå¹¶ç”Ÿæˆé‡è¯•ä»»åŠ¡

    é€»è¾‘ï¼š
    1. è¯»å–æœ€åä¸€ä¸ªè´¨é‡æ£€æŸ¥ç»“æœ
    2. å¦‚æœä¸é€šè¿‡ä¸”å»ºè®®è°ƒæ•´ â†’ åˆ›å»ºé‡è¯•ä»»åŠ¡
    3. åº”ç”¨adjustment_planä¸­çš„å‚æ•°è°ƒæ•´
    4. æŠ¤æ ï¼šæœ€å¤šé‡è¯•3æ¬¡ï¼Œæ¯ä¸ªå¤±è´¥ä»»åŠ¡åªé‡è¯•1æ¬¡

    Returns:
        TaskItem iféœ€è¦é‡è¯•, None otherwise
    """
    if not state.quality_checks:
        return None

    # è·å–æœ€åä¸€ä¸ªè´¨é‡æ£€æŸ¥
    last_check = state.quality_checks[-1]

    # åªå¤„ç†å¤±è´¥çš„æ£€æŸ¥
    if last_check["passed"]:
        return None

    # åªå¤„ç†å»ºè®®è°ƒæ•´çš„æƒ…å†µ
    if last_check["suggested_action"] not in ["adjust_params", "retry"]:
        return None

    # ğŸ”‘ æ£€æŸ¥è¿™ä¸ªè´¨é‡æ£€æŸ¥æ˜¯å¦å·²ç»è¢«å¤„ç†è¿‡
    check_id = last_check.get("timestamp", "")
    if not check_id:
        return None

    # ä½¿ç”¨ä¸€ä¸ªæ–°çš„çŠ¶æ€å­—æ®µè®°å½•å·²å¤„ç†çš„æ£€æŸ¥
    if not hasattr(state, 'processed_quality_checks'):
        state.processed_quality_checks = []

    if check_id in state.processed_quality_checks:
        return None  # è¿™ä¸ªæ£€æŸ¥å·²ç»å¤„ç†è¿‡äº†

    # æ ‡è®°è¿™ä¸ªæ£€æŸ¥ä¸ºå·²å¤„ç†
    state.processed_quality_checks.append(check_id)

    # ç”Ÿæˆé‡è¯•ä»»åŠ¡ID
    tool_name = last_check["tool_name"]
    tool_args = last_check["tool_args"]
    retry_key = f"retry_{tool_name}_{hash(str(tool_args))}"

    # ğŸ”‘ æŠ¤æ 2: å…¨å±€é‡è¯•æ¬¡æ•°é™åˆ¶
    if state.retry_count >= 3:
        print(f"   âš ï¸ å·²è¾¾å…¨å±€é‡è¯•ä¸Šé™({state.retry_count}æ¬¡)ï¼Œåœæ­¢è‡ªé€‚åº”åé¦ˆ")
        state.feedback_enabled = False
        return None

    # æ˜¾ç¤ºåé¦ˆä¿¡æ¯
    print(f"\nğŸ”„ è‡ªé€‚åº”åé¦ˆ: æ£€æµ‹åˆ° {tool_name} è´¨é‡é—®é¢˜")
    print(f"   åˆ†æ•°: {last_check['quality_score']:.2f}")
    print(f"   é—®é¢˜: {last_check['issues'][0] if last_check['issues'] else 'æœªçŸ¥'}")
    print(f"   å»ºè®®: {last_check['suggested_action']}")

    # ğŸ”‘ åº”ç”¨è°ƒæ•´æ–¹æ¡ˆ
    adjusted_params = tool_args.copy()
    if last_check.get("adjustment_plan"):
        adjusted_params.update(last_check["adjustment_plan"])
        print(f"   ğŸ”§ å‚æ•°è°ƒæ•´: {last_check['adjustment_plan']}")

    # æ¨æ–­å¹³å°å’Œå¼•æ“
    platform = _infer_platform(tool_name)
    engine = _infer_engine_from_check(last_check)

    # åˆ›å»ºé‡è¯•ä»»åŠ¡
    retry_task = TaskItem(
        task_id=retry_key,
        task_type="quality_retry",
        priority=999,  # æœ€é«˜ä¼˜å…ˆçº§
        engine=engine,
        platform=platform,
        tool_name=tool_name,
        arguments=adjusted_params,
        status="pending",
        reasoning=f"ğŸ”„ [è´¨é‡åé¦ˆé‡è¯•] {last_check['reasoning'][:80]}..."
    )

    # ğŸ”‘ åªå¢åŠ é‡è¯•è®¡æ•°ï¼Œä¸ç«‹å³æ ‡è®°ä¸ºcompleted
    # completedæ ‡è®°ç”±executoråœ¨ä»»åŠ¡çœŸæ­£æ‰§è¡Œå®Œæˆåå¤„ç†
    state.retry_count += 1

    print(f"   âœ… åˆ›å»ºé‡è¯•ä»»åŠ¡ (ç¬¬{state.retry_count}æ¬¡å…¨å±€é‡è¯•)")

    return retry_task


def _infer_platform(tool_name: str) -> str:
    """ä»å·¥å…·åæ¨æ–­å¹³å°"""
    if "youtube" in tool_name:
        return "youtube"
    elif "bilibili" in tool_name:
        return "bilibili"
    else:
        return "both"


def _infer_engine_from_check(check: Dict[str, Any]) -> str:
    """ä»è´¨é‡æ£€æŸ¥ä¸­æ¨æ–­å¼•æ“"""
    reasoning = check.get("reasoning", "")
    if "engine1" in reasoning.lower() or "åšä¸»" in reasoning or "influencer" in reasoning.lower():
        return "engine1"
    else:
        return "engine2"


def _is_english(text: str) -> bool:
    """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯è‹±æ–‡"""
    if not text:
        return False
    # ç®€å•æ£€æµ‹ï¼šå¦‚æœè¶…è¿‡ 70% æ˜¯ ASCII å­—ç¬¦ï¼Œè®¤ä¸ºæ˜¯è‹±æ–‡
    ascii_count = sum(1 for c in text if ord(c) < 128)
    return ascii_count / len(text) > 0.7

def _is_chinese(text: str) -> bool:
    """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡"""
    if not text:
        return False
    # æ£€æµ‹ä¸­æ–‡å­—ç¬¦
    chinese_count = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    return chinese_count / len(text) > 0.3


def _generate_influencer_search_tasks(state: RadarState) -> List[TaskItem]:
    """
    ç”Ÿæˆ"é¡ºè—¤æ‘¸ç“œ"ä»»åŠ¡
    ä¸ºå·²å‘ç°ä½†æœªæœç´¢çš„åšä¸»åˆ›å»ºæœç´¢ä»»åŠ¡
    """
    if not state.discovered_influencers:
        return []

    print(f"\nğŸ” åŠ¨æ€ç”Ÿæˆåšä¸»ä»»åŠ¡: å‘ç°{len(state.discovered_influencers)}ä¸ªåšä¸»")

    tasks = []
    searched_ids = set(state.searched_influencers)

    # æŒ‰ç½®ä¿¡åº¦å’ŒæåŠæ¬¡æ•°æ’åº
    confidence_score = {"high": 3, "medium": 2, "low": 1}
    sorted_influencers = sorted(
        state.discovered_influencers,
        key=lambda x: (confidence_score.get(x.get("confidence", "medium"), 1), x.get("mention_count", 1)),
        reverse=True
    )

    # åªä¸ºå‰5ä¸ªæœªæœç´¢çš„åšä¸»ç”Ÿæˆä»»åŠ¡
    count = 0
    for idx, influencer in enumerate(sorted_influencers, 1):
        if count >= 5:
            break

        identifier = influencer.get("identifier", "")
        name = influencer.get("name", "")
        platform = influencer.get("platform", "")
        confidence = influencer.get("confidence", "medium")

        print(f"   [{idx}] {name} ({platform}, {confidence}) - identifier: {identifier[:30] if identifier else 'N/A'}")

        if identifier in searched_ids:
            print(f"       â­ï¸ å·²æœç´¢è¿‡ï¼Œè·³è¿‡")
            continue

        if not name or not platform:
            print(f"       âš ï¸ ç¼ºå°‘nameæˆ–platformï¼Œè·³è¿‡")
            continue

        # ğŸ”‘ ç”Ÿæˆæœç´¢å…³é”®è¯ - æ ¹æ®å¹³å°ä½¿ç”¨å¯¹åº”è¯­è¨€
        # é¿å…æ··åˆè¯­è¨€å¯¼è‡´æœç´¢ç»“æœä¸ä½³
        target_domain = state.target_domains[0] if state.target_domains else ""
        
        if platform == "youtube":
            # YouTube: çº¯è‹±æ–‡æœç´¢è¯
            # å¦‚æœåšä¸»åæ˜¯è‹±æ–‡ï¼Œç›´æ¥ç”¨ï¼›å¦‚æœæ˜¯ä¸­æ–‡ï¼Œéœ€è¦ç¿»è¯‘æˆ–ä½¿ç”¨è‹±æ–‡å…³é”®è¯
            keyword = f"{name} {target_domain}".strip() if _is_english(name) else f"{name}"
        else:
            # Bilibili: çº¯ä¸­æ–‡æœç´¢è¯
            # ä½¿ç”¨ä¸­æ–‡å…³é”®è¯
            keyword = f"{name} {target_domain}".strip() if _is_chinese(target_domain) else f"{name} æœ€æ–°è§†é¢‘"

        tool_name = "youtube_search" if platform == "youtube" else "bilibili_search"

        # ğŸ”‘ æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´ä¼˜å…ˆçº§
        confidence = influencer.get("confidence", "medium")
        base_priority = 60
        if confidence == "high":
            priority_offset = 10  # ä¼˜å…ˆçº§70
        elif confidence == "medium":
            priority_offset = 0   # ä¼˜å…ˆçº§60
        else:  # low
            priority_offset = -15  # ä¼˜å…ˆçº§45ï¼ˆä½äºEngine2ï¼‰

        task = TaskItem(
            task_id=f"influencer_search_{platform}_{name}",
            task_type="influencer_search",
            priority=base_priority + priority_offset,
            engine="engine1",
            platform=platform,
            tool_name=tool_name,
            arguments={
                "keyword": keyword,
                "limit": 8,
                "from_influencer": identifier,
                "influencer_name": name
            },
            status="pending",
            reasoning=f"ğŸ”´ [å¼•æ“1-é¡ºè—¤æ‘¸ç“œ-{confidence}] æœç´¢åšä¸»: {name} ({platform})"
        )

        tasks.append(task)
        count += 1

        # æ ‡è®°ä¸ºå·²æœç´¢
        state.searched_influencers.append(identifier)

    if tasks:
        print(f"   ğŸŒ¿ é¡ºè—¤æ‘¸ç“œ: +{len(tasks)} åšä¸»ä»»åŠ¡")

    return tasks


def _llm_generate_tasks(state: RadarState) -> List[TaskItem]:
    """
    LLMåŠ¨æ€ç”Ÿæˆä»»åŠ¡ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
    å½“ä»»åŠ¡é˜Ÿåˆ—ä¸ºç©ºä½†ç›®æ ‡æœªè¾¾æˆæ—¶è°ƒç”¨
    
    ğŸ”‘ P3: é›†æˆ Skills æ¡†æ¶ + PromptManagerï¼Œæ³¨å…¥ä¸“ä¸šçŸ¥è¯†åˆ° prompt
    """
    from skills import get_skill_context
    from core.prompt_manager import get_prompt, build_state_summary, build_error_summary
    
    # è·å–å½“å‰æœç´¢ä¸»é¢˜
    topic = state.session_focus or "AI"
    
    # ğŸ”‘ æ ¹æ®å½“å‰çŠ¶æ€åŒ¹é…ç›¸å…³ Skills
    context_hint = f"{topic} youtube bilibili æœç´¢ ç­›é€‰"
    skill_context = get_skill_context(context_hint, max_skills=2)
    
    # ğŸ”‘ ä½¿ç”¨ PromptManager æ„å»ºçŠ¶æ€æ‘˜è¦å’Œé”™è¯¯æ‘˜è¦
    state_summary = build_state_summary(state)
    error_summary = build_error_summary(state, max_errors=3)
    
    # æ„å»º prompt
    collected = len(state.candidates)
    youtube_count = len([c for c in state.candidates if c.platform == "youtube"])
    bilibili_count = len([c for c in state.candidates if c.platform == "bilibili"])
    
    # ğŸ”‘ ä½¿ç”¨ PromptManager è·å–åŸºç¡€æç¤ºè¯
    base_prompt = get_prompt("task_generator", "system", topic=topic)
    
    system_prompt = f"""{base_prompt}

## ä¸“ä¸šçŸ¥è¯†å‚è€ƒ
{skill_context}

## å½“å‰çŠ¶æ€
{state_summary}

## å†å²é”™è¯¯ï¼ˆé¿å…é‡å¤ï¼‰
{error_summary if error_summary else "æ— "}

## ä»»åŠ¡è¦æ±‚
1. å¹³å°å¹³è¡¡ï¼šä¼˜å…ˆè¡¥å……æ•°é‡è¾ƒå°‘çš„å¹³å° (YouTube: {youtube_count}, Bilibili: {bilibili_count})
2. å…³é”®è¯å¤šæ ·ï¼šé¿å…é‡å¤å·²æœç´¢çš„è¯
3. è¯­è¨€çº¯å‡€ï¼šYouTubeçº¯è‹±æ–‡ï¼ŒBilibiliçº¯ä¸­æ–‡ï¼Œç¦æ­¢æ··åˆ
"""

    user_prompt = f"""åŸºäºä¸»é¢˜ã€Œ{topic}ã€ï¼Œç”Ÿæˆ 2-4 ä¸ªæœç´¢ä»»åŠ¡ã€‚

å·²æœç´¢çš„å…³é”®è¯ï¼ˆé¿å…é‡å¤ï¼‰ï¼š
{[t.arguments.get('query', t.arguments.get('keyword', '')) for t in state.task_queue[:10]]}

è¯·è¿”å› JSON æ ¼å¼çš„ä»»åŠ¡åˆ—è¡¨ï¼š
[
  {{"platform": "youtube", "query": "çº¯è‹±æ–‡æœç´¢è¯", "reason": "åŸå› "}},
  {{"platform": "bilibili", "query": "çº¯ä¸­æ–‡æœç´¢è¯", "reason": "åŸå› "}}
]
"""

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆä»»åŠ¡
        from core.llm import get_llm_with_schema
        from pydantic import BaseModel, Field
        from typing import List as ListType
        
        class TaskSuggestion(BaseModel):
            platform: str = Field(..., description="å¹³å°: youtube æˆ– bilibili")
            query: str = Field(..., description="æœç´¢å…³é”®è¯")
            reason: str = Field(..., description="é€‰æ‹©åŸå› ")
        
        class TaskSuggestions(BaseModel):
            tasks: ListType[TaskSuggestion] = Field(..., description="å»ºè®®çš„ä»»åŠ¡åˆ—è¡¨")
        
        # ğŸ”‘ ä¿®å¤: get_llm_with_schema ç›´æ¥è¿”å›ç»“æœï¼Œä¸æ˜¯è¿”å› LLM å¯¹è±¡
        result = get_llm_with_schema(
            user_prompt=user_prompt,
            response_model=TaskSuggestions,
            system_prompt=system_prompt,
            capability="fast"
        )
        
        # è½¬æ¢ä¸º TaskItem
        new_tasks = []
        for i, suggestion in enumerate(result.tasks):
            tool_name = f"{suggestion.platform}_search"
            task = TaskItem(
                task_id=f"llm_gen_{len(state.task_queue)}_{i}",
                tool_name=tool_name,
                engine="engine2",  # LLM ç”Ÿæˆçš„ä»»åŠ¡å½’å…¥å¼•æ“2
                platform=suggestion.platform,
                priority=60,  # ä¸­ç­‰ä¼˜å…ˆçº§
                arguments={
                    "query" if suggestion.platform == "youtube" else "keyword": suggestion.query,
                    "limit": 10,
                    "days": 30
                },
                status="pending",
                reasoning=f"ğŸ¤– [LLMç”Ÿæˆ] {suggestion.reason}"
            )
            new_tasks.append(task)
        
        print(f"   ğŸ¤– LLM ç”Ÿæˆ {len(new_tasks)} ä¸ªä»»åŠ¡")
        return new_tasks
        
    except Exception as e:
        print(f"   âš ï¸ LLM ä»»åŠ¡ç”Ÿæˆå¤±è´¥: {e}")
        return []


# ============ P1: å¤è¿°æœºåˆ¶ ============

def _print_goal_recap(state: RadarState, collected: int):
    """
    ğŸ”‘ P1: å¤è¿°æœºåˆ¶ - æ¯æ¬¡è¿­ä»£æ‰“å°ç›®æ ‡æé†’
    
    Manus æœ€ä½³å®è·µï¼šé€šè¿‡ä¸æ–­å¤è¿°ç›®æ ‡ï¼Œå°†æ³¨æ„åŠ›å¼•å¯¼åˆ°ä»»åŠ¡ç„¦ç‚¹
    é¿å… LLM åœ¨é•¿ä»»åŠ¡é“¾ä¸­"è¿·å¤±æ–¹å‘"
    
    ğŸ”‘ P0: ä½¿ç”¨ PromptManager çš„ build_goal_recap
    """
    # åªåœ¨éåˆå§‹åŒ–é˜¶æ®µä¸”æœ‰ä¸€å®šè¿›åº¦æ—¶æ‰“å°
    if state.current_phase == "init":
        return
    
    # æ¯ 5 æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡å®Œæ•´æé†’
    step_count = len(state.plan_scratchpad)
    
    if step_count > 0 and step_count % 5 == 0:
        # ğŸ”‘ ä½¿ç”¨ PromptManager æ„å»ºç›®æ ‡æé†’
        recap = build_goal_recap(state, TARGET_TOTAL_ITEMS)
        print(f"\n{recap}")
        
        # ğŸ”‘ P2: æ˜¾ç¤ºå½“å‰å¯ç”¨å·¥å…·æç¤º
        tool_hints = get_tool_hints(state)
        if tool_hints:
            print(f"\nğŸ’¡ å·¥å…·æç¤º:")
            print(f"   {tool_hints}")
        
        print()


def _build_error_context(state: RadarState, limit: int = 3) -> str:
    """
    ğŸ”‘ P1: æ„å»ºé”™è¯¯ä¸Šä¸‹æ–‡ä¾› LLM å‚è€ƒ
    
    è¿”å›æœ€è¿‘çš„é”™è¯¯è®°å½•æ‘˜è¦ï¼Œå¸®åŠ© LLM é¿å…é‡å¤çŠ¯é”™
    """
    if not state.error_history:
        return ""
    
    recent_errors = state.error_history[-limit:]
    lines = ["ã€æœ€è¿‘é”™è¯¯è®°å½• - è¯·é¿å…é‡å¤ã€‘"]
    
    for err in recent_errors:
        tool = err.get("tool_name", "unknown")
        error_type = err.get("error_type", "Error")
        error_msg = err.get("error", "")[:80]
        lines.append(f"- {tool}: [{error_type}] {error_msg}")
    
    return "\n".join(lines)


def get_planner_context_summary(state: RadarState) -> str:
    """
    ğŸ”‘ P1: ç”Ÿæˆè§„åˆ’å™¨ä¸Šä¸‹æ–‡æ‘˜è¦
    
    ç”¨äºåœ¨ LLM è°ƒç”¨æ—¶é™„åŠ åˆ° prompt æœ«å°¾ï¼Œå®ç°å¤è¿°æœºåˆ¶
    
    ğŸ”‘ P1: ä½¿ç”¨ ContextCompressor è¿›è¡Œæ™ºèƒ½å‹ç¼©
    """
    # ğŸ”‘ P1: åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©
    if should_compress(state):
        # ä½¿ç”¨å‹ç¼©å™¨ç”Ÿæˆæ‘˜è¦
        summary = compress_state(state)
    else:
        # ç®€å•æ‘˜è¦
        collected = len(state.candidates)
        youtube_count = len([c for c in state.candidates if c.platform == "youtube"])
        bilibili_count = len([c for c in state.candidates if c.platform == "bilibili"])
        
        # å¹³è¡¡çŠ¶æ€
        balance_summary = get_balance_summary(state.candidates, state.task_queue)
        
        # ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€
        pending_count = len([t for t in state.task_queue if t.status == "pending"])
        
        summary = f"""
ã€å½“å‰çŠ¶æ€æ‘˜è¦ã€‘
- ç›®æ ‡: æ”¶é›† {TARGET_TOTAL_ITEMS} æ¡å†…å®¹
- è¿›åº¦: {collected}/{TARGET_TOTAL_ITEMS} ({collected*100//TARGET_TOTAL_ITEMS if TARGET_TOTAL_ITEMS > 0 else 0}%)
- {balance_summary}
- å¾…æ‰§è¡Œä»»åŠ¡: {pending_count} ä¸ª
- å½“å‰é˜¶æ®µ: {state.current_phase}
"""
    
    # æ·»åŠ é”™è¯¯ä¸Šä¸‹æ–‡
    error_context = _build_error_context(state)
    if error_context:
        summary += f"\n{error_context}"
    
    # ğŸ”‘ P2: æ·»åŠ å¯ç”¨å·¥å…·ä¿¡æ¯
    available_tools = get_masked_tools(state)
    if available_tools:
        summary += f"\nã€å¯ç”¨å·¥å…·ã€‘{', '.join(available_tools)}"
    
    return summary
