from typing import List, Dict, Any
from datetime import datetime, timedelta
import dateutil.parser
import statistics
from collections import defaultdict
from core.state import RadarState, ContentItem
import sys
import os

# æ·»åŠ  utils åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from utils.logger import print_filter_result

def run_hybrid_filter(state: RadarState) -> Dict[str, Any]:
    """
    èŠ‚ç‚¹ 3: æ™ºèƒ½ç­›é€‰ (Filter V3)
    æ ¸å¿ƒé€»è¾‘: ç»Ÿè®¡å­¦å¼‚å¸¸æ£€æµ‹ (Z-Score & åŒè¡Œå¯¹æ¯”)
    """
    print("\n--- èŠ‚ç‚¹: æ™ºèƒ½ç­›é€‰ (Node 3: Filter V3) ---")
    
    valid_items = []
    logs = []
    now = datetime.now()

    before_count = len(state.candidates)
    state.candidates = _deduplicate_candidates(state.candidates)
    after_count = len(state.candidates)
    if after_count < before_count:
        logs.append(f"å»é‡ {before_count}->{after_count}")

    _enrich_cross_platform_metrics(state.candidates)
    
    # ğŸ”‘ ä¿®å¤: æŒ‰å¼•æ“åˆ†ç»„ï¼Œä¸æ˜¯æŒ‰source_type
    monitor_items = [i for i in state.candidates if
                     i.raw_data.get("engine") == "engine1" or
                     i.raw_data.get("from_influencer_search") or
                     i.source_type in ["youtube_monitor", "bilibili_monitor"]]

    hunter_items = [i for i in state.candidates if
                    i.raw_data.get("engine") == "engine2" and
                    not i.raw_data.get("from_influencer_search")]

    print(f"\n{'='*60}")
    print(f"ğŸ§¹ æ™ºèƒ½ç­›é€‰")
    print(f"   è¾“å…¥: {len(state.candidates)} æ¡ (ğŸ”´{len(monitor_items)} ğŸ”µ{len(hunter_items)})")
    
    # --- ç­–ç•¥ A: ç›‘æ§æ¨¡å¼ (çºµå‘å¯¹æ¯”) ---
    # ğŸ”µ å¼•æ“ 1: å¤´éƒ¨åšä¸»ç›‘æ§ - çºµå‘å¼‚å¸¸æ£€æµ‹
    # ç›®æ ‡: å‘ç°åšä¸»è‡ªå·±"å‘æŒ¥è¶…å¸¸"çš„è§†é¢‘

    passed_time_check = 0
    for item in monitor_items:
        # 1. æ—¶æ•ˆæ£€æŸ¥ï¼ˆæ”¾å®½åˆ°30å¤©ï¼‰
        if not _check_time(item.publish_time, days=30):
            continue
        passed_time_check += 1

        # 2. å¼‚å¸¸å€¼æ£€æµ‹
        if item.author_avg_views > 0:
            ratio = item.view_count / item.author_avg_views

            # é˜ˆå€¼: 1.2å€ (æ¯”å¹³æ—¶å¥½20%)
            if ratio > 1.2:
                item.score = 80.0 + (ratio * 10)
                item.raw_data["analysis_note"] = f"æ¯”å¹³æ—¶æ•°æ®å¥½ {ratio:.1f} å€"
                item.raw_data["engine"] = "å¼•æ“1-å¤´éƒ¨åšä¸»ç›‘æ§"
                item.raw_data["detection_type"] = "çºµå‘å¼‚å¸¸"
                valid_items.append(item)
            else:
                # ç‰¹ä¾‹: ç»å¯¹çˆ†æ¬¾ (è¶…è¿‡50ä¸‡æ’­æ”¾)ï¼Œå³ä½¿è¯¥åšä¸»å¹³æ—¶å°±å¾ˆå¼ºï¼Œä¹Ÿä¿ç•™
                if item.view_count > 500000:
                    item.score = 75.0
                    item.raw_data["engine"] = "å¼•æ“1-å¤´éƒ¨åšä¸»ç›‘æ§"
                    item.raw_data["detection_type"] = "ç»å¯¹çˆ†æ¬¾"
                    item.raw_data["analysis_note"] = "ç»å¯¹æµé‡çˆ†æ¬¾"
                    valid_items.append(item)
        else:
            # ğŸ”‘ ä¿®å¤å…³é”®é—®é¢˜ 2: é™ä½å…œåº•ç­–ç•¥é˜ˆå€¼ï¼ˆä» 5000 é™åˆ° 1000ï¼‰
            # å…œåº•: æ— å†å²åŸºå‡†æ•°æ®æ—¶ï¼Œåªçœ‹ç»å¯¹æŒ‡æ ‡
            if item.view_count > 1000 or item.interaction > 50:
                 item.score = 70.0
                 item.raw_data["analysis_note"] = "ç›‘æ§æ•°æ®ï¼ˆæ— å†å²åŸºå‡†ï¼‰"
                 item.raw_data["engine"] = "å¼•æ“1-å¤´éƒ¨åšä¸»ç›‘æ§"
                 item.raw_data["detection_type"] = "ç»å¯¹æŒ‡æ ‡"
                 valid_items.append(item)

    print(f"   å¼•æ“1é€šè¿‡æ—¶æ•ˆæ£€æŸ¥: {passed_time_check}/{len(monitor_items)}")

    # --- ç­–ç•¥ B: çŒæ€æ¨¡å¼ (æ¨ªå‘å¯¹æ¯”) ---
    # ğŸ”´ å¼•æ“ 2: å…³é”®è¯æœç´¢ - æ¨ªå‘å¯¹æ¯”æ‰¾çˆ†æ¬¾
    # ç›®æ ‡: å‘ç°æ¯”åŒè¡Œå¼ºå¾—å¤šçš„è§†é¢‘

    if hunter_items:
        # 1. è®¡ç®—å½“å‰æ± å­çš„ä¸­ä½æ•° (åŸºå‡†çº¿)
        views = [i.view_count for i in hunter_items if i.view_count > 0]
        median_views = statistics.median(views) if views else 1000

        print(f"ğŸ“Š çŒæ€æ± æ’­æ”¾ä¸­ä½æ•°: {median_views}")

        passed_time_check2 = 0
        passed_criteria = 0
        for item in hunter_items:
             # æ—¶æ•ˆæ£€æŸ¥ï¼ˆæ”¾å®½åˆ°60å¤©ï¼‰
            if not _check_time(item.publish_time, days=60):
                continue
            passed_time_check2 += 1

            # 2. åŒè¡Œå¯¹æ¯”
            # ğŸ”‘ ä¿®å¤å…³é”®é—®é¢˜ 4: é™ä½ç­›é€‰æ ‡å‡†ï¼ˆä» 2å€é™åˆ° 1.5å€ï¼‰

            fans = item.author_fans if item.author_fans > 0 else 5000
            interaction = item.interaction
            if interaction == 0:
                interaction = item.view_count * 0.02 # ä¼°ç®—äº’åŠ¨

            rate = interaction / fans

            is_view_outlier = (item.view_count > median_views * 1.5)  # ä» 2 é™åˆ° 1.5
            is_eng_outlier = (rate > 0.01)  # ä» 0.02 é™åˆ° 0.01 
            
            normalized_view = item.raw_data.get("normalized_view", 0)
            if normalized_view > 0:
                score_boost = normalized_view * 5
            else:
                score_boost = (item.view_count / median_views * 5)

            if is_view_outlier or is_eng_outlier or normalized_view > 2:
                passed_criteria += 1
                item.score = 60.0 + score_boost
                note = "èµ›é“é»‘é©¬ (Peer Outlier)"
                if normalized_view:
                    note += f" | å½’ä¸€æ’­æ”¾ {normalized_view:.1f}x"
                item.raw_data["analysis_note"] = note
                item.raw_data["engine"] = "å¼•æ“2-å…³é”®è¯æœç´¢"
                item.raw_data["detection_type"] = "æ¨ªå‘å¼‚å¸¸"
                valid_items.append(item)
            
            # Reddit ç‰¹èµ¦ (é«˜ä»·å€¼æ–‡æœ¬)
            elif item.platform == "reddit" and item.interaction > 50:
                item.score = 65.0
                item.raw_data["engine"] = "å¼•æ“2-å…³é”®è¯æœç´¢"
                item.raw_data["detection_type"] = "é«˜ä»·å€¼æ–‡æœ¬"
                valid_items.append(item)

        print(f"   å¼•æ“2é€šè¿‡æ—¶æ•ˆæ£€æŸ¥: {passed_time_check2}/{len(hunter_items)}")
        print(f"   å¼•æ“2ç¬¦åˆç­›é€‰æ ‡å‡†: {passed_criteria}/{passed_time_check2}")

    # æ’åºå¹¶æˆªå– Top 10
    valid_items.sort(key=lambda x: x.score, reverse=True)
    top_items = valid_items[:10]

    summary_msg = f"ç­›é€‰åå‰©ä½™ {len(top_items)} æ¡ä¼˜è´¨ç´ æã€‚"
    logs.append(summary_msg)
    state.logs.append(f"ã€ç­›é€‰ã€‘è¾“å…¥ {len(state.candidates)} æ¡ï¼Œè¾“å‡º {len(top_items)} æ¡ã€‚")

    print(f"   è¾“å‡º: {len(top_items)} æ¡\n")
    
    return {
        "filtered_candidates": top_items,
        "logs": logs
    }

def _check_time(publish_time, days=7):
    try:
        now = datetime.now()
        p_time = None
        if isinstance(publish_time, (int, float)):
            p_time = datetime.fromtimestamp(publish_time)
        elif isinstance(publish_time, str):
            clean_time = publish_time.replace("/", "").replace("-", "").replace(".", "")
            if len(clean_time) == 8 and clean_time.isdigit():
                p_time = datetime.strptime(clean_time, "%Y%m%d")
            else:
                p_time = dateutil.parser.parse(publish_time)
        
        if p_time:
            if p_time.tzinfo: p_time = p_time.replace(tzinfo=None)
            if (now - p_time).days > days:
                return False
        return True
    except:
        return True 


def _deduplicate_candidates(items: List[ContentItem]) -> List[ContentItem]:
    deduped = {}
    for item in items:
        key_source = item.url or item.title or f"{item.platform}-{id(item)}"
        key = key_source.strip().lower()
        if key in deduped:
            existing = deduped[key]
            _merge_sources(existing, item)
            if item.view_count > existing.view_count:
                _merge_sources(item, existing)
                deduped[key] = item
        else:
            deduped[key] = item
    return list(deduped.values())


def _merge_sources(primary: ContentItem, secondary: ContentItem):
    chain = set()
    for entry in (primary, secondary):
        if entry.source_type:
            chain.add(entry.source_type)
        extra = entry.raw_data.get("source_chain")
        if isinstance(extra, list):
            chain.update(extra)
        elif isinstance(extra, str):
            chain.add(extra)
    primary.raw_data["source_chain"] = sorted(chain)


def _enrich_cross_platform_metrics(items: List[ContentItem]):
    if not items:
        return
    per_platform_views = defaultdict(list)
    all_views = []
    for item in items:
        if item.view_count > 0:
            per_platform_views[item.platform].append(item.view_count)
            all_views.append(item.view_count)

    platform_median = {
        platform: (statistics.median(values) if values else 1)
        for platform, values in per_platform_views.items()
    }
    global_median = statistics.median(all_views) if all_views else 1

    for item in items:
        base = platform_median.get(item.platform) or global_median or 1
        normalized_view = item.view_count / base if base else 0

        fans = item.author_fans if item.author_fans > 0 else 5000
        interaction = item.interaction if item.interaction > 0 else item.view_count * 0.02
        engagement_rate = interaction / fans if fans else 0

        item.raw_data["normalized_view"] = round(normalized_view, 2)
        item.raw_data["engagement_rate"] = round(engagement_rate, 4)
