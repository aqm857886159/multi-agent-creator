"""
Analyst Agent - æ·±åº¦åˆ†ææ™ºèƒ½ä½“
==================================

åŠŸèƒ½: å°† TopicBrief è½¬åŒ–ä¸º DeepAnalysisReport
æ¶æ„: ä¸‰çº§ç«ç®­æ¨¡å‹ (Scout â†’ Excavator â†’ Philosopher)

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-11-27
"""

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from core.state import TopicBrief, ResearchPlan, KeyInsight, DeepAnalysisReport
from core.llm import get_llm_with_schema
from tools.web_search import SearchGateway
from tools.arxiv_search import ArxivSearcher


# ============================================
# Level 1: Adaptive Scout (åŠ¨æ€ä¾¦å¯Ÿè§„åˆ’)
# ============================================

def _summarize_reference_data(reference_data: List[Dict[str, Any]]) -> str:
    """
    å°† reference_data è½¬æ¢ä¸ºç®€æ´çš„æ‘˜è¦æ–‡æœ¬ï¼Œä¾› LLM å‚è€ƒ
    """
    if not reference_data:
        return "æ— å·²æœ‰ç´ æ"

    summary_lines = []
    for idx, ref in enumerate(reference_data[:10], 1):  # æœ€å¤šå±•ç¤º 10 æ¡
        # å…¼å®¹ ContentItem dict å’ŒåŸå§‹ dict
        platform = ref.get('platform', 'æœªçŸ¥å¹³å°')
        title = ref.get('title', 'æ— æ ‡é¢˜')
        view_count = ref.get('view_count', 0)
        author = ref.get('author_name', 'æœªçŸ¥ä½œè€…')

        # æå–æè¿°ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        desc = ""
        if 'raw_data' in ref and isinstance(ref['raw_data'], dict):
            desc = ref['raw_data'].get('description', '') or ref['raw_data'].get('summary', '')
            if desc:
                desc = desc[:150] + "..." if len(desc) > 150 else desc

        summary_lines.append(
            f"{idx}. [{platform}] {title}\n"
            f"   æ’­æ”¾: {view_count:,} | ä½œè€…: {author}"
            + (f"\n   ç®€ä»‹: {desc}" if desc else "")
        )

    if len(reference_data) > 10:
        summary_lines.append(f"\n... ä»¥åŠå…¶ä»– {len(reference_data) - 10} æ¡ç´ æ")

    return "\n\n".join(summary_lines)


def plan_research_strategy(topic_brief: TopicBrief) -> ResearchPlan:
    """
    Level 1: Scout - ç†è§£é€‰é¢˜æ„å›¾ï¼Œè®¾è®¡æœ€ä¼˜æœç´¢ç­–ç•¥

    é€»è¾‘: ä»"åˆ†ç±»é©±åŠ¨"è½¬å‘"æ„å›¾é©±åŠ¨"
    - ä¸å†å¼ºåˆ¶åˆ†ç±»ï¼ˆtech/business/socialï¼‰
    - è€Œæ˜¯æ¨ç†ï¼šè¿™ä¸ªé€‰é¢˜éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Ÿåº”è¯¥å»å“ªæ‰¾ï¼Ÿ
    æ¨¡å‹: Fast Model (creative)
    """

    # ğŸ”‘ æå– reference_data ä¿¡æ¯
    ref_count = len(topic_brief.reference_data)
    ref_summary = _summarize_reference_data(topic_brief.reference_data)

    # ğŸ”‘ æå–å¹³å°ä¿¡æ¯
    platforms = set()
    for ref in topic_brief.reference_data:
        if isinstance(ref, dict) and 'platform' in ref:
            platforms.add(ref['platform'])
    platform_str = ', '.join(platforms) if platforms else 'æœªçŸ¥'

    user_prompt = f"""
# ä½ çš„ä»»åŠ¡ï¼šç†è§£é€‰é¢˜æ„å›¾ï¼Œè®¾è®¡æœ€ä¼˜æœç´¢ç­–ç•¥

## èƒŒæ™¯ä¿¡æ¯

**é€‰é¢˜**:
- æ ‡é¢˜: {topic_brief.title}
- åˆ‡å…¥ç‚¹: {topic_brief.core_angle}
- æ¨èç†ç”±: {topic_brief.rationale}
- æ¥æºç±»å‹: {topic_brief.source_type}
  ï¼ˆè¯´æ˜: "search" = ä»è§†é¢‘å¹³å°æœç´¢å¾—åˆ°, "viral_hit" = çˆ†æ¬¾è§†é¢‘, "competitor" = ç«å“åˆ†æï¼‰

**å·²æœ‰ç´ æ** ({ref_count} æ¡ï¼Œæ¥è‡ª {platform_str}):
{ref_summary}

---

## ä½ çš„æ¨ç†è¿‡ç¨‹ï¼ˆè¯·é€æ­¥æ€è€ƒï¼‰

### Step 1: ç†è§£é€‰é¢˜çš„çœŸå®æ„å›¾

**æ€è€ƒ**:
1. è¿™ä¸ªé€‰é¢˜çš„æ ¸å¿ƒä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ
   - ç†è®ºéªŒè¯ï¼ˆéœ€è¦å­¦æœ¯è®ºæ–‡æ”¯æ’‘ï¼‰
   - å®æˆ˜æ•™ç¨‹ï¼ˆéœ€è¦æ“ä½œæ‰‹å†Œå’Œæ¡ˆä¾‹ï¼‰
   - è§‚ç‚¹è¯„è®ºï¼ˆéœ€è¦ä¸“å®¶è®¿è°ˆå’Œæ•°æ®ï¼‰
   - æ¡ˆä¾‹åˆ†äº«ï¼ˆéœ€è¦çœŸå®ç»éªŒå’Œæ•…äº‹ï¼‰

2. ç›®æ ‡å—ä¼—æ˜¯è°ï¼Ÿ
   - å­¦æœ¯ç ”ç©¶è€… vs æ™®é€šç”¨æˆ· vs ä¸“ä¸šäººå£«

3. é€‰é¢˜æ¥è‡ªå“ªé‡Œï¼Ÿ
   - å¦‚æœæ¥è‡ª "Bilibili" â†’ å¯èƒ½æ˜¯å®æˆ˜æ–¹æ³•ã€æ•™ç¨‹
   - å¦‚æœæ¥è‡ª "YouTube" â†’ å¯èƒ½æ˜¯ç§‘æ™®ã€è¯„æµ‹
   - å¦‚æœæ¥è‡ª "Webæ–‡ç« " â†’ å¯èƒ½æ˜¯æ–°é—»ã€è§‚ç‚¹

**ä½ çš„åˆ¤æ–­**: ï¼ˆç”¨1-2å¥è¯è¯´æ˜é€‰é¢˜æ„å›¾ï¼‰

---

### Step 2: åˆ†æå·²æœ‰ç´ æ

**æ€è€ƒ**:
1. å·²æœ‰ {ref_count} æ¡ç´ æè¯´æ˜ä»€ä¹ˆï¼Ÿ
   - å¦‚æœæ•°é‡ >= 2 ä¸”æ¥è‡ªåŒä¸€å¹³å° â†’ è¯´æ˜è¿™ä¸ªè¯é¢˜åœ¨è¯¥å¹³å°æœ‰çƒ­åº¦
   - æ ‡é¢˜ä¸­çš„å…³é”®è¯ï¼ˆå¦‚"å®æµ‹"ã€"æ•™ç¨‹"ã€"æ·±åº¦"ï¼‰æš—ç¤ºå†…å®¹ç±»å‹

2. è¿™äº›ç´ æå·²ç»è¦†ç›–äº†å“ªäº›å†…å®¹ï¼Ÿè¿˜ç¼ºä»€ä¹ˆï¼Ÿ
   - å·²æœ‰: è§†é¢‘æœ¬èº«å¯èƒ½åŒ…å«æ“ä½œæ¼”ç¤ºã€æ–¹æ³•è®²è§£
   - ç¼ºå°‘: å¯èƒ½ç¼ºç†è®ºæ”¯æ’‘ã€æ•°æ®éªŒè¯ã€å¤šæ ·åŒ–æ¡ˆä¾‹

**ä½ çš„åˆ¤æ–­**: ï¼ˆè¯´æ˜å·²æœ‰ç´ æçš„ä»·å€¼å’Œç¼ºå£ï¼‰

---

### Step 3: è®¾è®¡æœç´¢ç­–ç•¥

**ä¼˜å…ˆçº§ 1**: æ˜¯å¦éœ€è¦æœç´¢ï¼Ÿ

- å¦‚æœå·²æœ‰ç´ æ >= 2æ¡ä¸”è´¨é‡é«˜ï¼ˆæ¥è‡ªçˆ†æ¬¾è§†é¢‘ï¼‰ â†’ å¯èƒ½æ— éœ€é¢å¤–æœç´¢ï¼Œç›´æ¥åˆ†æå·²æœ‰ç´ æå³å¯
- å¦‚æœé€‰é¢˜éœ€è¦ç†è®ºæ”¯æ’‘ â†’ éœ€è¦æœç´¢å­¦æœ¯èµ„æ–™
- å¦‚æœéœ€è¦å¤šæ ·åŒ–æ¡ˆä¾‹ â†’ éœ€è¦æœç´¢ç¤¾åŒºç»éªŒ

**ä¼˜å…ˆçº§ 2**: å¦‚æœéœ€è¦æœç´¢ï¼Œæœä»€ä¹ˆï¼Ÿ

**å®æˆ˜ç±»é€‰é¢˜**ï¼ˆå­¦ä¹ æ–¹æ³•ã€æ•ˆç‡æŠ€å·§ã€å·¥å…·ä½¿ç”¨ï¼‰:
- ä¼˜å…ˆ: ä¸­æ–‡å®æˆ˜ç¤¾åŒºï¼ˆçŸ¥ä¹ã€å°çº¢ä¹¦ã€Bç«™ä¸“æ ï¼‰
- æ¬¡è¦: è‹±æ–‡å®æˆ˜èµ„æºï¼ˆReddit, Medium, GitHubå®ç”¨å·¥å…·ï¼‰
- é¿å…: å­¦æœ¯è®ºæ–‡ï¼ˆå¤ªç†è®ºï¼Œä¸é€‚åˆå®æˆ˜ï¼‰

**ç†è®ºç±»é€‰é¢˜**ï¼ˆæŠ€æœ¯åŸç†ã€ç®—æ³•ã€å­¦æœ¯ç»¼è¿°ï¼‰:
- ä¼˜å…ˆ: å­¦æœ¯èµ„æºï¼ˆArxiv, GitHubå¼€æºå®ç°, å®˜æ–¹æ–‡æ¡£ï¼‰
- æ¬¡è¦: æŠ€æœ¯åšå®¢
- é¿å…: è¥é”€å·æ–‡ç« 

**è§‚ç‚¹ç±»é€‰é¢˜**ï¼ˆè¡Œä¸šåˆ†æã€è¶‹åŠ¿é¢„æµ‹ï¼‰:
- ä¼˜å…ˆ: æƒå¨è§‚ç‚¹ï¼ˆä¸“å®¶è®¿è°ˆã€è¡Œä¸šæŠ¥å‘Šï¼‰
- æ¬¡è¦: ç¤¾åŒºè®¨è®ºï¼ˆçŸ¥ä¹é«˜èµã€HackerNewsï¼‰

**ä¼˜å…ˆçº§ 3**: å…³é”®è¯è®¾è®¡

- å¦‚æœé€‰é¢˜åŒ…å«ä¸­æ–‡ä¿—è¯­/é»‘è¯ï¼ˆå¦‚"2+3+5"ï¼‰ â†’ ä¿ç•™ä¸­æ–‡æœç´¢ï¼Œä¸è¦ç¿»è¯‘
- å¦‚æœæ˜¯é€šç”¨æ¦‚å¿µ â†’ ä¸­è‹±æ–‡ç»“åˆ
- é¿å…: ç”Ÿç¡¬ç›´è¯‘ä¸“æœ‰åè¯

---

## è¾“å‡ºè¦æ±‚

è¯·è¾“å‡º ResearchPlanï¼ŒåŒ…å«:
- topic_category: é€‰é¢˜ç±»å‹çš„ç®€çŸ­æ ‡ç­¾ï¼ˆå¦‚"å®æˆ˜æ•™ç¨‹"ã€"ç†è®ºåˆ†æ"ã€"è§‚ç‚¹è¯„è®º"ï¼‰
- search_strategy: ä½ çš„æœç´¢ç­–ç•¥æè¿°ï¼ˆ1-2å¥è¯ï¼‰
- search_instructions: 3-5ä¸ªæœç´¢æŒ‡ä»¤ï¼Œæ¯ä¸ªåŒ…å«ï¼š
  * tool: "web_search" æˆ– "arxiv_search"
  * query: å…·ä½“æœç´¢è¯
  * target: æœŸæœ›æ‰¾åˆ°ä»€ä¹ˆ
  * priority: 1-3ï¼ˆ1æœ€é«˜ï¼‰
- reasoning: ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªç­–ç•¥

**å…³é”®è¯è®¾è®¡ç¤ºä¾‹**:
- âœ… å¥½: "çŸ¥ä¹ 2+3+5 å­¦ä¹ æ³•"ï¼ˆä¿ç•™ä¸­æ–‡é»‘è¯ï¼‰
- âŒ å·®: "2-3-5 time blocking study"ï¼ˆç”Ÿç¡¬ç¿»è¯‘ï¼‰
- âœ… å¥½: "Feynman technique practical guide"ï¼ˆé€šç”¨æ¦‚å¿µç”¨è‹±æ–‡ï¼‰
- âŒ å·®: "è´¹æ›¼æŠ€å·§ å®ç”¨æŒ‡å—"ï¼ˆé€šç”¨æ¦‚å¿µå¼ºè¡Œä¸­æ–‡ï¼‰

**é‡è¦**: æ‰€æœ‰è¾“å‡ºä½¿ç”¨ä¸­æ–‡ï¼Œä½†æœç´¢è¯æ ¹æ®å®é™…éœ€è¦é€‰æ‹©ä¸­è‹±æ–‡
"""

    try:
        plan: ResearchPlan = get_llm_with_schema(
            user_prompt=user_prompt,
            response_model=ResearchPlan,
            capability="creative",  # Use creative for planning
            system_prompt="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶ç­–ç•¥ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºæ·±åº¦åˆ†ææ‰¾åˆ°æœ€ä½³çš„ä¸€æ‰‹èµ„æ–™æ¥æºã€‚

æ ¸å¿ƒåŸåˆ™ï¼š
- æ‰€æœ‰è¾“å‡ºå¿…é¡»ä½¿ç”¨ä¸­æ–‡
- ä¼˜å…ˆå¯»æ‰¾ä¸€æ‰‹èµ„æ–™è€ŒéäºŒæ‰‹ä¿¡æ¯
- æœç´¢ç­–ç•¥è¦å…·ä½“ã€å¯æ‰§è¡Œ"""
        )

        print(f"\nğŸ“‹ Research Plan Generated:")
        print(f"   Category: {plan.topic_category}")
        print(f"   Strategy: {plan.search_strategy}")
        print(f"   Search Instructions: {len(plan.search_instructions)} actions")

        return plan

    except Exception as e:
        print(f"âš ï¸ Research planning failed: {e}")
        # Fallback: é€šç”¨æœç´¢ç­–ç•¥
        return ResearchPlan(
            topic_category="general",
            search_strategy="Fallback to general web search due to planning error",
            search_instructions=[
                {
                    "tool": "web_search",
                    "query": f"{topic_brief.title} authoritative analysis",
                    "target": "General information",
                    "priority": 1
                }
            ],
            reasoning="Fallback strategy due to error"
        )


# ============================================
# Level 2: Excavator (æ™ºèƒ½èƒå–)
# ============================================

class ContentProcessor:
    """
    Level 2: Excavator - æŒ–æ˜ä¸æ™ºèƒ½èƒå–

    åŠŸèƒ½: ä»é•¿æ–‡æœ¬ä¸­æå–é«˜ä¿¡å™ªæ¯”çš„æƒ…æŠ¥å¡ç‰‡
    æ¨¡å‹: Fast Model (DeepSeek V3 / Gemini Flash) - é•¿æ–‡æœ¬ + ä½æˆæœ¬
    """

    def __init__(self, reference_data: List[Dict[str, Any]] = None):
        self.search_gateway = SearchGateway()
        self.arxiv_searcher = ArxivSearcher()
        self.reference_data = reference_data or []  # ğŸ”‘ å­˜å‚¨å·²æœ‰ç´ æ

    def execute_search_plan(self, plan: ResearchPlan, topic_title: str) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæœç´¢è®¡åˆ’ï¼Œè·å–åŸå§‹æ•°æ®

        Returns:
            List of raw search results with long text content
        """
        all_results = []

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_instructions = sorted(
            plan.search_instructions,
            key=lambda x: x.get('priority', 99)
        )

        for instruction in sorted_instructions[:5]:  # é™åˆ¶æœ€å¤š5ä¸ªæœç´¢
            tool = instruction.get('tool', 'web_search')
            query = instruction.get('query', '')
            target = instruction.get('target', '')

            print(f"\nğŸ” Executing: {tool} | {query}")
            print(f"   Target: {target}")

            try:
                if tool == "analyze_existing":
                    # ğŸ”‘ æ–°å·¥å…·: ç›´æ¥åˆ†æå·²æœ‰ç´ æ
                    results = self._analyze_existing_materials(query, target)
                    all_results.extend(results)

                elif tool == "arxiv_search":
                    results = self.arxiv_searcher.search(query, max_results=3)
                    for r in results:
                        all_results.append({
                            "source": r.get('title', 'Unknown'),
                            "url": r.get('url', ''),
                            "content": r.get('summary', ''),
                            "is_primary": True,  # Arxiv papers are primary sources
                            "search_target": target
                        })

                elif tool == "web_search":
                    # ğŸ”‘ ä½¿ç”¨ include_raw_content=True è·å–å®Œæ•´æ–‡æœ¬
                    results = self.search_gateway.search(
                        query=query,
                        limit=3,
                        depth="advanced",
                        include_raw_content=True
                    )
                    for r in results:
                        # ä¼˜å…ˆä½¿ç”¨ raw_contentï¼Œå¦åˆ™ä½¿ç”¨æ‘˜è¦
                        content = r.get('raw_content', r.get('content', ''))
                        all_results.append({
                            "source": r.get('title', 'Unknown'),
                            "url": r.get('url', ''),
                            "content": content,
                            "is_primary": False,  # Web content needs verification
                            "search_target": target
                        })

                else:
                    print(f"   âš ï¸ Unknown tool: {tool}, skipping")

            except Exception as e:
                print(f"   âŒ Search failed: {e}")
                continue

        print(f"\nâœ… Collected {len(all_results)} raw sources")
        return all_results

    def _analyze_existing_materials(self, query: str, target: str) -> List[Dict[str, Any]]:
        """
        ğŸ”‘ æ–°åŠŸèƒ½: åˆ†æå·²æœ‰ç´ æï¼ˆreference_dataï¼‰

        å‚æ•°:
            query: åˆ†æå…³é”®è¯ï¼ˆç”¨äºç­›é€‰ç›¸å…³ç´ æï¼Œ"*" è¡¨ç¤ºåˆ†ææ‰€æœ‰ï¼‰
            target: æœŸæœ›æå–çš„ä¿¡æ¯ç±»å‹

        è¿”å›:
            æ ‡å‡†åŒ–çš„æœç´¢ç»“æœæ ¼å¼ï¼Œä¾› extract_insights ä½¿ç”¨
        """
        if not self.reference_data:
            print(f"   âš ï¸ æ— å·²æœ‰ç´ æå¯åˆ†æ")
            return []

        results = []
        query_lower = query.lower()

        for ref in self.reference_data[:10]:  # æœ€å¤šåˆ†æ10æ¡
            # æå–åŸºç¡€ä¿¡æ¯
            title = ref.get('title', '')
            platform = ref.get('platform', 'æœªçŸ¥å¹³å°')
            author = ref.get('author_name', 'æœªçŸ¥ä½œè€…')
            url = ref.get('url', '')
            view_count = ref.get('view_count', 0)

            # æå–æè¿°/æ‘˜è¦ä½œä¸ºå†…å®¹
            raw_data = ref.get('raw_data', {})
            content = raw_data.get('description', '') or raw_data.get('summary', '')

            # ç®€å•çš„ç›¸å…³æ€§ç­›é€‰ï¼ˆå¯é€‰ï¼‰
            is_relevant = (
                query_lower in title.lower() or
                query_lower in content.lower() or
                query == "*"  # "*" è¡¨ç¤ºåˆ†ææ‰€æœ‰ç´ æ
            )

            if is_relevant and content:
                results.append({
                    "source": f"[{platform}] {title} - {author}",
                    "url": url,
                    "content": f"""
**å¹³å°**: {platform}
**æ ‡é¢˜**: {title}
**ä½œè€…**: {author}
**æ’­æ”¾é‡**: {view_count:,}
**é“¾æ¥**: {url}

**å†…å®¹æè¿°**:
{content}
""",
                    "is_primary": True,  # å·²æ”¶é›†çš„è§†é¢‘æ˜¯ä¸€æ‰‹ç´ æ
                    "search_target": target
                })
                print(f"   âœ… åŒ¹é…åˆ°ç´ æ: {title[:40]}...")

        print(f"   ğŸ“¦ ä»å·²æœ‰ç´ æä¸­æå–äº† {len(results)} æ¡ç›¸å…³å†…å®¹")
        return results

    def extract_insights(self, raw_results: List[Dict[str, Any]], topic_title: str) -> List[KeyInsight]:
        """
        æ™ºèƒ½èƒå–: ä»é•¿æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æƒ…æŠ¥å¡ç‰‡

        è¾“å…¥: 20k tokens çš„åŸå§‹æ‚ä¹±æ–‡æœ¬
        è¾“å‡º: ç»“æ„åŒ–çš„ KeyInsight å¡ç‰‡
        æ¨¡å‹: Fast Model (ä½æˆæœ¬é•¿æ–‡æœ¬å¤„ç†)
        """

        insights = []

        for idx, result in enumerate(raw_results[:10], 1):  # æœ€å¤šå¤„ç†10ä¸ªæ¥æº
            source = result.get('source', 'Unknown')
            url = result.get('url', '')
            content = result.get('content', '')
            is_primary = result.get('is_primary', False)

            if not content or len(content) < 50:
                continue

            # æˆªæ–­è¿‡é•¿å†…å®¹ (æœ€å¤š15k tokens â‰ˆ 60k chars)
            if len(content) > 60000:
                content = content[:60000] + "... [truncated]"

            print(f"\nğŸ“„ Extracting insights from [{idx}] {source[:50]}...")

            user_prompt = f"""
You are an expert information extractor. Extract KEY INSIGHTS from this source about the topic: "{topic_title}".

**Source**: {source}
**URL**: {url}
**Content**:
{content}

**Task**: Extract the MOST VALUABLE insights for understanding "{topic_title}".

**Extraction Rules**:
1. **Quote Original Text**: Copy exact quotes/data (NO fabrication)
2. **Filter Noise**: Ignore ads, fluff, promotional content
3. **Find Conflicts**: Identify claims that contradict mainstream views
4. **Verify Facts**: Mark confidence (high/medium/low)

**Output**: Up to 3 KeyInsight cards for this source.

**KeyInsight Schema**:
- source: Source name/title
- url: Source URL
- is_primary: true if this is first-hand (paper/official doc), false if secondary
- quote: Exact quote or data from the source (MUST be verbatim)
- insight: Your interpretation of why this matters for "{topic_title}"
- conflict: (optional) Does this contradict common beliefs?
- confidence: high/medium/low

**Example**:
```json
{{
  "source": "OpenAI Technical Report: GPT-4",
  "url": "https://arxiv.org/abs/2303.08774",
  "is_primary": true,
  "quote": "GPT-4 achieves human-level performance on various professional benchmarks",
  "insight": "This shows AI has reached capability threshold for professional work, implying major productivity shifts",
  "conflict": "Contradicts belief that AI can only do simple tasks",
  "confidence": "high"
}}
```

**CRITICAL**:
- If the content is irrelevant, return empty list
- If no solid facts/quotes, skip it
- Quality > Quantity
"""

            try:
                class MultiInsightOutput(BaseModel):
                    insights: List[KeyInsight] = Field(default_factory=list)

                result_obj: MultiInsightOutput = get_llm_with_schema(
                    user_prompt=user_prompt,
                    response_model=MultiInsightOutput,
                    capability="creative",  # Fast model for extraction
                    system_prompt="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»é•¿æ–‡æœ¬ä¸­æå–å¯éªŒè¯çš„äº‹å®å’ŒåŸæ–‡å¼•ç”¨ã€‚

æ ¸å¿ƒåŸåˆ™ï¼š
- æ‰€æœ‰è¾“å‡ºå¿…é¡»ä½¿ç”¨ä¸­æ–‡
- åªæå–å¯éªŒè¯çš„äº‹å®ï¼Œæ‹’ç»ç¼–é€ 
- ä¿ç•™åŸæ–‡å¼•ç”¨ï¼ˆquoteå­—æ®µï¼‰ï¼Œç¡®ä¿å‡†ç¡®æ€§
- å¦‚æœå†…å®¹ä¸ç›¸å…³ï¼Œè¿”å›ç©ºåˆ—è¡¨"""
                )

                extracted = result_obj.insights
                if extracted:
                    insights.extend(extracted)
                    print(f"   âœ… Extracted {len(extracted)} insights")
                else:
                    print(f"   âš ï¸ No insights extracted (likely irrelevant)")

            except Exception as e:
                print(f"   âŒ Extraction failed: {e}")
                continue

        print(f"\nâœ… Total Insights Extracted: {len(insights)}")
        return insights


# ============================================
# Level 3: Philosopher (è®¤çŸ¥é‡æ„)
# ============================================

def deep_analysis(
    topic_brief: TopicBrief,
    insights: List[KeyInsight]
) -> DeepAnalysisReport:
    """
    Level 3: Philosopher - åŸºäºäº‹å®è¿›è¡Œé€»è¾‘å¤–ç§‘æ‰‹æœ¯

    è¾“å…¥: é«˜ä¿¡å™ªæ¯”çš„æƒ…æŠ¥å¡ç‰‡
    è¾“å‡º: æ·±åº¦ç ”æŠ¥
    æ¨¡å‹: Reasoning Model (DeepSeek-R1 / Kimi K2 / Claude 3.5 Sonnet)
    """

    # å‡†å¤‡æƒ…æŠ¥å¡ç‰‡ä¸Šä¸‹æ–‡
    insights_context = ""
    for idx, insight in enumerate(insights[:15], 1):  # æœ€å¤šä½¿ç”¨15ä¸ªæ´å¯Ÿ
        insights_context += f"""
ã€æƒ…æŠ¥å¡ç‰‡ {idx}ã€‘
æ¥æº: {insight.source}
é“¾æ¥: {insight.url}
ä¸€æ‰‹èµ„æ–™: {'æ˜¯' if insight.is_primary else 'å¦'}
åŸæ–‡å¼•ç”¨: {insight.quote}
ä»·å€¼è§£è¯»: {insight.insight}
å†²çªç‚¹: {insight.conflict if insight.conflict else 'æ— '}
å¯ä¿¡åº¦: {insight.confidence}
---
        """.strip() + "\n\n"

    user_prompt = f"""
You are a top-tier research analyst with expertise in First Principles Thinking, Dialectics, and Mental Models.

**Topic**: {topic_brief.title}
**Core Angle**: {topic_brief.core_angle}
**Rationale**: {topic_brief.rationale}

**Intelligence Cards** (Verified Facts):
{insights_context}

**Your Mission**: Transform these facts into a DEEP ANALYSIS REPORT that reveals the UNDERLYING LOGIC and CONTRARIAN INSIGHTS.

---

## ğŸ§  Thinking Framework

### 1. **First Principles Thinking** (ç¬¬ä¸€æ€§åŸç†)
- Ask "WHY" 5 times to reach the root cause
- Strip away assumptions and get to fundamental truths
- Example: "Why is RAG popular?" â†’ "Why do LLMs hallucinate?" â†’ "Why is training data limited?" â†’ ... â†’ **Root: Information is sparse in reality**

### 2. **Dialectic Method** (è¾©è¯æ³•)
- Find the OPPOSITE of mainstream views
- Construct conflicts and contradictions
- Example: Mainstream = "AI will replace humans" | Contrarian = "AI amplifies human uniqueness"

### 3. **Mental Models** (æ€ç»´æ¨¡å‹åº“)
Auto-match relevant models from:
- **Physics**: Entropy (ç†µå¢å®šå¾‹), Energy Conservation
- **Economics**: Network Effects, Marginal Cost, Supply/Demand
- **Psychology**: Loss Aversion (æŸå¤±åŒæ¶), Cognitive Bias
- **Evolution**: Selection Pressure, Adaptation

---

## ğŸ“‹ Output Structure: DeepAnalysisReport

### A. **Fact Layer** (äº‹å®å±‚)
- `hard_evidence`: List of verified facts with citations (e.g., "[Source: Arxiv] GPT-4 achieves 86% on MMLU")
- `verified_facts`: Structured list of key facts with sources

### B. **Logic Layer** (é€»è¾‘å±‚)
- `root_cause`: The FUNDAMENTAL reason (from 5-Why analysis)
- `theoretical_model`: Which mental model applies (e.g., "Network Effects", "Entropy")
- `first_principles_analysis`: The step-by-step reasoning from basics

### C. **Insight Layer** (æ´å¯Ÿå±‚)
- `mainstream_view`: What most people think
- `contrarian_view`: The counter-intuitive insight (HIGH VALUE)
- `conflict_analysis`: Where does the contradiction lie?

### D. **Narrative Layer** (å™äº‹å±‚)
- `emotional_hook`: Which deep human emotion does this touch? (Greed, Fear, Laziness, Curiosity, Pride)
- `content_strategy`: Concrete advice for the Writer Agent (e.g., "Open with shocking stat, use before/after structure")

### E. **Metadata**
- `sources_used`: List of KeyInsights used
- `confidence_score`: Overall confidence (0-1)

---

## ğŸ¯ Quality Standards

**MUST HAVE**:
âœ… Every claim must cite a source
âœ… Root cause must be non-obvious (not surface-level)
âœ… Contrarian view must be backed by logic/data
âœ… Emotional hook must be specific (not generic)

**MUST AVOID**:
âŒ Generic statements ("AI is changing the world")
âŒ Fabricated quotes or data
âŒ Obvious observations without depth
âŒ Buzzwords without substance

---

## ğŸ’¡ Example (Partial)

**Topic**: "RAG vs Long Context"

**hard_evidence**:
- "[Arxiv 2024] GPT-4 with 128k context still hallucinates on needle-in-haystack tests"
- "[Anthropic Blog] Claude 2 retrieves facts with 99% accuracy using RAG vs 87% with long context"

**root_cause**: "Information retrieval is fundamentally a search problem, not a storage problem. Long context = storing everything; RAG = indexing smartly. Physics favors indexing (lower entropy)."

**theoretical_model**: "Entropy (ç†µå¢å®šå¾‹) - Systems tend toward disorder. Long context increases disorder (noise); RAG maintains order (structured retrieval)."

**contrarian_view**: "Mainstream believes 'longer context = better'. But physics says: unlimited context creates infinite noise. RAG wins because it REDUCES entropy."

**emotional_hook**: "Fear - Developers fear RAG complexity. But long context is a trap: you're drowning in noise."

**content_strategy**: "Open with shocking stat: 'GPT-4 fails 13% of retrieval with 128k context'. Then reveal: RAG = 99% accuracy. Use analogy: Long context = hoarding; RAG = Marie Kondo."

---

**Now, analyze the intelligence cards and produce your DeepAnalysisReport.**

**ğŸ”‘ å…³é”®è¦æ±‚**:
- æ‰€æœ‰å­—æ®µå¿…é¡»ä½¿ç”¨ä¸­æ–‡è¾“å‡º
- åŒ…æ‹¬ root_cause, mainstream_view, contrarian_view, emotional_hook ç­‰æ‰€æœ‰æ–‡æœ¬å­—æ®µ
- ä¿æŒä¸“ä¸šæ€§å’Œåˆ†ææ·±åº¦
"""

    try:
        report: DeepAnalysisReport = get_llm_with_schema(
            user_prompt=user_prompt,
            response_model=DeepAnalysisReport,
            capability="reasoning",  # ğŸ”‘ Use reasoning model for deep analysis
            system_prompt="""ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„æ·±åº¦åˆ†æä¸“å®¶ï¼Œèåˆäº†ï¼š
- ç§‘å­¦å®¶çš„ä¸¥è°¨æ€§ï¼ˆéªŒè¯ä¸€åˆ‡ï¼‰
- å“²å­¦å®¶çš„æ´å¯ŸåŠ›ï¼ˆå‘ç°æ·±å±‚çœŸç†ï¼‰
- æ•…äº‹è®²è¿°è€…çš„è¡¨è¾¾åŠ›ï¼ˆæ¸…æ™°ä¼ è¾¾ï¼‰

æ ¸å¿ƒåŸåˆ™ï¼š
1. æ‰€æœ‰è¾“å‡ºå¿…é¡»ä½¿ç”¨ä¸­æ–‡
2. æ¯ä¸ªç»“è®ºå¿…é¡»æœ‰è¯æ®æ”¯æ’‘
3. è¿½æ±‚åç›´è§‰çš„æ´å¯Ÿï¼ˆè€Œéæ˜¾è€Œæ˜“è§çš„è§‚ç‚¹ï¼‰
4. ä½¿ç”¨æ€ç»´æ¨¡å‹è§£é‡Š"ä¸ºä»€ä¹ˆ"
5. è®©åˆ†æç»“æœå¯ç”¨äºå†…å®¹åˆ›ä½œ"""
        )

        # å¡«å……å…ƒæ•°æ®
        report.topic_id = topic_brief.id
        report.topic_title = topic_brief.title
        report.sources_used = insights

        print(f"\nâœ… Deep Analysis Completed:")
        print(f"   Root Cause: {report.root_cause[:80]}...")
        print(f"   Contrarian View: {report.contrarian_view[:80]}...")
        print(f"   Confidence: {report.confidence_score}")

        return report

    except Exception as e:
        print(f"âŒ Deep analysis failed: {e}")
        # è¿”å›ç©ºæŠ¥å‘Š
        return DeepAnalysisReport(
            topic_id=topic_brief.id,
            topic_title=topic_brief.title,
            hard_evidence=["Analysis failed due to error"],
            root_cause="Unable to determine",
            confidence_score=0.0
        )


# ============================================
# Main Workflow: Three-Level Rocket
# ============================================

def run_analyst(topic_brief: TopicBrief) -> DeepAnalysisReport:
    """
    å®Œæ•´çš„åˆ†ææµç¨‹: Scout â†’ Excavator â†’ Philosopher

    è¾“å…¥: TopicBrief (æ¥è‡ª Radar Agent)
    è¾“å‡º: DeepAnalysisReport (ç»™ Writer Agent)
    """

    print("\n" + "="*60)
    print("ğŸš€ ANALYST AGENT - Three-Level Rocket Launch")
    print("="*60)

    # ğŸš€ Level 1: Scout - åŠ¨æ€ä¾¦å¯Ÿè§„åˆ’
    print("\nğŸš€ Level 1: Adaptive Scout - Planning Research Strategy")
    research_plan = plan_research_strategy(topic_brief)

    # ğŸš€ Level 2: Excavator - æŒ–æ˜ä¸èƒå–
    print("\nğŸš€ Level 2: Excavator - Digging and Extracting Insights")
    processor = ContentProcessor(reference_data=topic_brief.reference_data)  # ğŸ”‘ ä¼ é€’å·²æœ‰ç´ æ

    # 2.1 æ‰§è¡Œæœç´¢è®¡åˆ’
    raw_results = processor.execute_search_plan(research_plan, topic_brief.title)

    # 2.2 æ™ºèƒ½èƒå–
    insights = processor.extract_insights(raw_results, topic_brief.title)

    if not insights:
        print("âš ï¸ No insights extracted! Falling back to basic report.")
        return DeepAnalysisReport(
            topic_id=topic_brief.id,
            topic_title=topic_brief.title,
            hard_evidence=["No data collected"],
            root_cause="Insufficient information",
            confidence_score=0.1
        )

    # ğŸš€ Level 3: Philosopher - è®¤çŸ¥é‡æ„
    print("\nğŸš€ Level 3: Philosopher - Deep Analysis & Reconstruction")
    report = deep_analysis(topic_brief, insights)

    print("\n" + "="*60)
    print("âœ… ANALYST AGENT - Mission Complete")
    print("="*60)

    return report


# ============================================
# LangGraph Node Interface
# ============================================

def analyst_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    LangGraph èŠ‚ç‚¹æ¥å£

    è¾“å…¥ state å­—æ®µ:
    - proposals: List[TopicBrief] (æ¥è‡ª Radar Agent)

    è¾“å‡º state å­—æ®µ:
    - analysis_reports: List[DeepAnalysisReport]
    """

    from core.state import RadarState

    # ğŸ”‘ ä¿®å¤: state å¯èƒ½æ˜¯ RadarState å¯¹è±¡æˆ–å­—å…¸ï¼Œå…¼å®¹å¤„ç†
    if isinstance(state, dict):
        proposals = state.get("proposals", [])
        logs = state.get("logs", [])
    else:
        # RadarState å¯¹è±¡
        proposals = state.proposals if hasattr(state, 'proposals') else []
        logs = state.logs if hasattr(state, 'logs') else []

    if not proposals:
        print("âš ï¸ No proposals to analyze")
        return {"logs": logs + ["ã€Analystã€‘è·³è¿‡: æ— é€‰é¢˜"]}

    reports = []

    for idx, proposal_item in enumerate(proposals[:3], 1):  # æœ€å¤šåˆ†æ3ä¸ªé€‰é¢˜
        print(f"\n{'='*60}")
        print(f"Analyzing Proposal {idx}/{min(len(proposals), 3)}")
        print(f"{'='*60}")

        # è½¬æ¢ä¸º TopicBrief å¯¹è±¡
        try:
            # ğŸ”‘ å…¼å®¹å¤„ç†ï¼šå¯èƒ½æ˜¯å­—å…¸æˆ–å·²ç»æ˜¯ TopicBrief å¯¹è±¡
            if isinstance(proposal_item, TopicBrief):
                topic_brief = proposal_item
            elif isinstance(proposal_item, dict):
                topic_brief = TopicBrief(**proposal_item)
            else:
                print(f"âš ï¸ Unknown proposal type: {type(proposal_item)}, skipping")
                continue

            report = run_analyst(topic_brief)
            reports.append(report.model_dump())
        except Exception as e:
            print(f"âŒ Failed to analyze proposal {idx}: {e}")
            import traceback
            traceback.print_exc()
            continue

    return {
        "analysis_reports": reports,
        "logs": logs + [f"ã€Analystã€‘å®Œæˆ {len(reports)} ä¸ªæ·±åº¦åˆ†æ"]
    }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=== Testing Analyst Agent ===\n")

    # åˆ›å»ºæµ‹è¯• TopicBrief
    test_topic = TopicBrief(
        id="test_001",
        title="AIç»˜å›¾æŠ€æœ¯",
        core_angle="Stable Diffusion vs MidJourney æŠ€æœ¯å¯¹æ¯”",
        rationale="é«˜çƒ­åº¦è¯é¢˜ï¼ŒæŠ€æœ¯æ·±åº¦è¶³å¤Ÿ",
        source_type="tech_news",
        reference_data=[{"platform": "youtube", "title": "AI Art Tutorial"}]
    )

    # è¿è¡Œåˆ†æ
    report = run_analyst(test_topic)

    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("FINAL REPORT")
    print("="*60)
    print(f"Topic: {report.topic_title}")
    print(f"\nRoot Cause: {report.root_cause}")
    print(f"\nMainstream View: {report.mainstream_view}")
    print(f"\nContrarian View: {report.contrarian_view}")
    print(f"\nEmotional Hook: {report.emotional_hook}")
    print(f"\nConfidence: {report.confidence_score}")
