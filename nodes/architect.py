from typing import Dict, Any, List, Tuple
from pydantic import BaseModel, Field
from core.state import RadarState, TopicBrief
from core.llm import ModelGateway
from core.prompts import load_prompt
import re
from difflib import SequenceMatcher

# Define a container for the list to help Instructor
class TopicBriefList(BaseModel):
    proposals: List[TopicBrief]

def run_architect(state: RadarState) -> Dict[str, Any]:
    """
    èŠ‚ç‚¹ 4: é€‰é¢˜ç­–åˆ’ (Architect) - Structured Output Version
    ğŸ”‘ P0æ”¹è¿›: æ·»åŠ ç´ æç›¸å…³æ€§è´¨é‡é—¨æ§›
    """
    print("\n--- èŠ‚ç‚¹: é€‰é¢˜ç­–åˆ’ (Node 4: Architect) ---")

    if not state.filtered_candidates:
        print("âš ï¸  æ²¡æœ‰ç´ æé€šè¿‡ç­›é€‰ï¼Œè·³è¿‡ç­–åˆ’ç¯èŠ‚ã€‚")
        return {"logs": state.logs + ["Architect skipped: No candidates"]}

    # ğŸ”‘ P0: è´¨é‡é—¨æ§› - æ£€æŸ¥ç´ æä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§
    if state.target_domains:
        user_query = state.target_domains[0]  # ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢
        candidate_titles = [item.title for item in state.filtered_candidates]

        relevance_score, matched_keywords = _calculate_relevance_score(user_query, candidate_titles)

        print(f"ğŸ“Š ç´ æç›¸å…³æ€§åˆ†æ:")
        print(f"   ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        print(f"   ç›¸å…³æ€§åˆ†æ•°: {relevance_score:.2%}")
        print(f"   åŒ¹é…å…³é”®è¯: {matched_keywords if matched_keywords else 'æ— '}")

        # é˜ˆå€¼: å¹³å‡ç›¸å…³æ€§ < 30% åˆ™æ‹’ç»ç”Ÿæˆ
        if relevance_score < 0.30:
            # æå–ç´ æå®é™…è®¨è®ºçš„ä¸»é¢˜
            actual_topics = _extract_topic_keywords(state.filtered_candidates)

            warning_msg = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âš ï¸  ç´ æè´¨é‡è­¦å‘Š: æœç´¢ç»“æœä¸æŸ¥è¯¢ä¸åŒ¹é…                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ æ‚¨çš„æŸ¥è¯¢: {user_query}
ğŸ“Š ç´ æç›¸å…³æ€§: {relevance_score:.1%} (é˜ˆå€¼: 30%)

ğŸ” æœç´¢åˆ°çš„å†…å®¹ä¸»è¦è®¨è®º:
   {', '.join(actual_topics[:5])}

ğŸ’¡ å»ºè®®:
   1. å°è¯•æ›´ç®€æ´çš„å…³é”®è¯ (å¦‚: "{matched_keywords[0] if matched_keywords else user_query.split()[0]}")
   2. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®
   3. è¯¥ä¸»é¢˜å¯èƒ½æ•°æ®ä¸è¶³ï¼ˆæ–°å…¬å¸/å°ä¼—äº§å“ï¼‰

âŒ å·²é˜»æ­¢ç”Ÿæˆä¸ç›¸å…³é€‰é¢˜ï¼Œé¿å…"ç‚¹çº¢çƒ§è‚‰ä¸ŠåœŸè±†ä¸"
"""
            print(warning_msg)

            return {
                "proposals": [],  # è¿”å›ç©ºåˆ—è¡¨
                "logs": state.logs + [
                    f"âš ï¸ Architectè´¨é‡é—¨æ§›æ‹¦æˆª: ç›¸å…³æ€§{relevance_score:.1%} < 30%",
                    f"å®é™…ä¸»é¢˜: {', '.join(actual_topics[:3])}",
                    "å»ºè®®è°ƒæ•´æœç´¢å…³é”®è¯"
                ]
            }

    llm = ModelGateway()
    prompt_cfg = load_prompt("architect_agent")
    
    # å‡†å¤‡ä¸Šä¸‹æ–‡
    candidates_summary = []
    for idx, item in enumerate(state.filtered_candidates, 1):
        clean_desc = _deep_clean_text(item.raw_data.get('description', '') or item.raw_data.get('summary', ''))
        desc_snippet = clean_desc[:3000] 
        
        candidates_summary.append(
            f"""
            ã€ç´ æ #{idx}ã€‘
            æ¥æº: {item.platform} | ç±»å‹: {item.source_type}
            æ ‡é¢˜: {item.title}
            æ•°æ®: æ’­æ”¾ {item.view_count} | äº’åŠ¨ {item.interaction}
            åšä¸»: {item.author_name}
            æ ¸å¿ƒå†…å®¹æ‘˜è¦:
            {desc_snippet}
            --------------------------------------------------
            """
        )
    
    context_str = "\n".join(candidates_summary)
    
    system_prompt = prompt_cfg["system_template"].format(
        role=prompt_cfg["role"],
        goal=prompt_cfg["goal"],
        methodology=prompt_cfg["methodology"]
    )
    
    user_prompt = f"""
    æˆ‘ä¸ºä½ ç²¾é€‰äº†ä»¥ä¸‹ {len(state.filtered_candidates)} æ¡é«˜ä»·å€¼æƒ…æŠ¥:
    
    {context_str}
    
    ä»»åŠ¡:
    è¯·åƒä¸€ä¸ªç»éªŒä¸°å¯Œçš„ç§‘æŠ€åª’ä½“ä¸»ç¼–ä¸€æ ·ï¼Œé˜…è¯»ä»¥ä¸Šæ‰€æœ‰ææ–™ï¼Œç­–åˆ’ 3 ä¸ªå…·ä½“çš„é€‰é¢˜æ–¹æ¡ˆã€‚
    
    è¦æ±‚:
    1. æ·±åº¦æ•´åˆ: ä¸è¦åªå¤è¿°ï¼Œå°è¯•å¯»æ‰¾å…³è”ã€‚
    2. æ ‡é¢˜å…š: æ ‡é¢˜è¦æå…·ç‚¹å‡»æ¬²ã€‚
    3. ä¸¥æ ¼æŒ‰ç…§ Schema è¾“å‡ºã€‚
    """
    
    try:
        print(f"ğŸ§  æ­£åœ¨é˜…è¯»å¹¶ç­–åˆ’é€‰é¢˜ (ä¸Šä¸‹æ–‡é•¿åº¦: {len(context_str)} å­—ç¬¦)...")
        
        # Magic happens here: Instructor handles validation
        result: TopicBriefList = llm.call_with_schema(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            schema_model=TopicBriefList,
            capability="creative" # Kimi K2 Thinking
        )
        
        proposals = result.proposals
        
        # Post-processing: Ensure source_type is filled if missing
        for p in proposals:
            if not p.source_type: p.source_type = "hybrid"
                
        return {
            "proposals": proposals,
            "logs": state.logs + [f"æˆåŠŸç”Ÿæˆ {len(proposals)} ä¸ªé€‰é¢˜ææ¡ˆ (Validated)."]
        }
        
    except Exception as e:
        print(f"ç­–åˆ’é˜¶æ®µå‡ºé”™: {e}")
        return {"logs": state.logs + [f"Architect failed: {e}"]}

def _calculate_relevance_score(user_query: str, candidate_titles: List[str]) -> Tuple[float, List[str]]:
    """
    ğŸ”‘ P0æ–¹æ¡ˆ: è®¡ç®—ç´ æä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§

    å‚æ•°:
        user_query: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ (å¦‚ "AIå…¬å¸manusä¸ºä»€ä¹ˆæˆåŠŸ")
        candidate_titles: å€™é€‰ç´ æçš„æ ‡é¢˜åˆ—è¡¨

    è¿”å›:
        (å¹³å‡ç›¸å…³æ€§åˆ†æ•° 0-1, åŒ¹é…åˆ°çš„å…³é”®è¯åˆ—è¡¨)
    """
    # æå–ç”¨æˆ·æŸ¥è¯¢ä¸­çš„æ ¸å¿ƒå®ä½“è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
    stopwords = {'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–',
                 'why', 'how', 'what', 'when', 'where', 'the', 'a', 'an', 'is', 'are'}

    # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼ã€æ ‡ç‚¹ï¼‰
    query_words = re.findall(r'[\w]+', user_query.lower())
    core_entities = [w for w in query_words if w not in stopwords and len(w) > 1]

    if not core_entities:
        return 1.0, []  # å¦‚æœæ— æ³•æå–å®ä½“ï¼Œé»˜è®¤é€šè¿‡

    # è®¡ç®—æ¯ä¸ªç´ ææ ‡é¢˜çš„ç›¸å…³æ€§
    relevance_scores = []
    matched_keywords = set()

    for title in candidate_titles:
        title_lower = title.lower()

        # æ–¹æ³•1: ç²¾ç¡®åŒ¹é…æ ¸å¿ƒå®ä½“
        exact_matches = sum(1 for entity in core_entities if entity in title_lower)

        # æ–¹æ³•2: æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†æ‹¼å†™å˜ä½“ï¼‰
        fuzzy_matches = 0
        for entity in core_entities:
            for word in re.findall(r'[\w]+', title_lower):
                similarity = SequenceMatcher(None, entity, word).ratio()
                if similarity > 0.8:  # 80%ç›¸ä¼¼åº¦
                    fuzzy_matches += 1
                    matched_keywords.add(entity)
                    break

        # ç»¼åˆå¾—åˆ†
        total_matches = exact_matches + fuzzy_matches * 0.8
        score = min(total_matches / len(core_entities), 1.0)
        relevance_scores.append(score)

    avg_score = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0

    return avg_score, list(matched_keywords)


def _extract_topic_keywords(candidates: List[Any]) -> List[str]:
    """
    ä»å€™é€‰ç´ æä¸­æå–å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¸»é¢˜è¯
    ç”¨äºåˆ¤æ–­ç´ æé›†ä¸­è®¨è®ºçš„æ˜¯ä»€ä¹ˆè¯é¢˜
    """
    from collections import Counter

    all_words = []
    for item in candidates:
        title = item.title if hasattr(item, 'title') else item.get('title', '')
        words = re.findall(r'[\w]+', title.lower())
        # è¿‡æ»¤åœç”¨è¯å’Œè¿‡çŸ­è¯
        stopwords = {'ai', 'the', 'a', 'an', 'is', 'are', 'for', 'to', 'in', 'on', 'çš„', 'äº†', 'å’Œ'}
        words = [w for w in words if w not in stopwords and len(w) > 2]
        all_words.extend(words)

    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(all_words)
    top_topics = [word for word, count in word_counts.most_common(5)]

    return top_topics


def _deep_clean_text(text: str) -> str:
    """Sanitize text to avoid token waste"""
    if not text: return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace('{"', '').replace('"}', '')
    return text
