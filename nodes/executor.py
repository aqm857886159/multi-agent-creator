from typing import Dict, Any, List, Optional
from datetime import datetime
import re
from urllib.parse import urlparse
from core.state import RadarState, ContentItem, LeadItem
from core.tool_registry import registry
from core.tool_loader import load_tools_from_config
from core.quality_gate import AdaptiveQualityGate, FeedbackLoopManager, FeedbackLoopGuard
from core.memory import compress_candidates_if_needed
from core.state_reducers import (
    append_reducer,
    merge_dict_reducer,
    dedupe_append_reducer,
    capped_append_reducer
)
# ğŸ”‘ P4: åé¦ˆåˆ†æå™¨
from core.feedback_analyzer import (
    analyze_result,
    get_retry_suggestion
)
# ğŸ”‘ P1: ä¸Šä¸‹æ–‡å‹ç¼©
from core.context_compressor import compress_candidates

# ğŸ”‘ P0: å€™é€‰å†…å®¹å‹ç¼©é˜ˆå€¼
CANDIDATES_COMPRESS_THRESHOLD = 100

DEFAULT_PARAMS = {
    "web_search": {"limit": 20, "depth": "advanced"},  # ğŸ”‘ 15â†’20 (ä½æˆæœ¬æ‰©å®¹)
    "youtube_search": {"limit": 15, "days": 60, "scan_limit": 50},  # ğŸ”‘ å¿«é€Ÿæ‰«æ50æ¡ï¼Œè¯¦ç»†å¤„ç†15æ¡ï¼Œæ—¶é—´æ”¾å®½åˆ°60å¤©
    "bilibili_search": {"limit": 15, "days": 60, "sort_by": "comprehensive", "fetch_size": 100},  # ğŸ”‘ æ—¶é—´æ”¾å®½åˆ°60å¤©
    "youtube_monitor": {"limit": 15, "days": 60},  # ğŸ”‘ 10â†’15ï¼Œæ—¶é—´æ”¾å®½åˆ°60å¤©
    "bilibili_monitor": {"limit": 15},  # ğŸ”‘ 10â†’15
}

# ğŸ”‘ è‡ªé€‚åº”è´¨é‡é—¨ï¼ˆå…¨å±€å•ä¾‹ï¼Œä½¿ç”¨fast modelé™ä½æˆæœ¬ï¼‰
_quality_gate = AdaptiveQualityGate(use_fast_model=True)
_feedback_manager = FeedbackLoopManager(max_retries=2, max_cost=0.5)

def run_executor(state: RadarState) -> Dict[str, Any]:
    # é™é»˜åŠ è½½å·¥å…·ï¼ˆä¸æ‰“å°æ—¥å¿—ï¼‰
    load_tools_from_config()

    # Get last planned action
    if not state.plan_scratchpad:
        return {"plan_status": "planning"} # Should not happen

    last_entry = state.plan_scratchpad[-1]
    if "tool_result" in last_entry:
        # Already executed
        return {"plan_status": "planning"}

    tool_call = last_entry.get("tool_call")
    if not tool_call:
        return {"plan_status": "planning"}
        
    tool_name = tool_call["tool_name"]
    tool_args = tool_call["arguments"]
    _apply_default_params(tool_name, tool_args)
    
    tool_def = registry.get_tool(tool_name)
    if not tool_def or not tool_def.func:
        error_msg = f"Tool {tool_name} not found or not executable."
        print(f"âŒ {error_msg}")
        last_entry["tool_result"] = {"status": "error", "error": error_msg}
        return {"plan_status": "planning", "plan_scratchpad": state.plan_scratchpad}
        
    try:
        print(f"ğŸ”¨ æ‰§è¡Œ: {tool_name}...")

        # ğŸ”‘ æ–°å¢: ä»reasoningä¸­æå–ä»»åŠ¡IDå’Œå¼•æ“ä¿¡æ¯
        reasoning = tool_call.get("reasoning", "")
        task_id = _extract_task_id(reasoning)
        engine = _extract_engine(reasoning)

        # Execute the tool wrapper
        result = tool_def.func(tool_args)

        # result is a ToolResult object
        print(f"âœ… ç»“æœ: {result.summary}")

        # ğŸ”‘ è‡ªé€‚åº”è´¨é‡æ£€æŸ¥ï¼ˆæ™ºèƒ½åˆ¤æ–­ç»“æœè´¨é‡ï¼‰
        if state.feedback_enabled and result.status == "success":
            quality_result = _run_quality_check(
                state=state,
                tool_name=tool_name,
                tool_args=tool_args,
                tool_result=result,
                reasoning=reasoning
            )

            # å¦‚æœè´¨é‡ä¸é€šè¿‡ä¸”å»ºè®®è°ƒæ•´ï¼Œè®°å½•åé¦ˆä½†ç»§ç»­ï¼ˆç”±plannerå†³å®šæ˜¯å¦é‡è¯•ï¼‰
            if not quality_result.passed:
                print(f"   âš ï¸ è´¨é‡æ£€æŸ¥: {quality_result.suggested_action} - {quality_result.reasoning[:100]}")
                if quality_result.issues:
                    for issue in quality_result.issues[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªé—®é¢˜
                        print(f"     â€¢ {issue}")

                # è®°å½•è´¨é‡æ£€æŸ¥ç»“æœåˆ°çŠ¶æ€ï¼ˆä¾›plannerå‚è€ƒï¼‰
                state.quality_checks.append({
                    "tool_name": tool_name,
                    "tool_args": tool_args,
                    "result_summary": result.summary,
                    "quality_score": quality_result.score,
                    "passed": quality_result.passed,
                    "issues": quality_result.issues,
                    "suggested_action": quality_result.suggested_action,
                    "adjustment_plan": quality_result.adjustment_plan,
                    "reasoning": quality_result.reasoning,
                    "timestamp": datetime.now().isoformat()
                })
            else:
                print(f"   âœ… è´¨é‡æ£€æŸ¥: é€šè¿‡ (åˆ†æ•°: {quality_result.score:.2f})")

        # Save result to scratchpad
        last_entry["tool_result"] = result.model_dump()

        # Ingest data into state.candidates if applicable
        topic_hint = tool_args.get("topic_hint")
        new_count = 0
        if tool_name == "web_search" and isinstance(result.data, list):
            lead_count = _ingest_leads(state, result.data, topic_hint, tool_name)
            if lead_count:
                state.logs.append(f"ã€çº¿ç´¢ã€‘{tool_name} è¿½åŠ  {lead_count} æ¡ leads")

        if result.status == "success":
            new_items = []
            if result.data and isinstance(result.data, list):
                for item in result.data:
                    try:
                        if isinstance(item, dict):
                            if topic_hint:
                                item.setdefault("raw_data", {})
                                item["raw_data"]["topic_hint"] = topic_hint
                            if "source_type" not in item:
                                item["source_type"] = tool_name
                            if "platform" not in item:
                                item["platform"] = "web"
                            if "publish_time" not in item:
                                item["publish_time"] = datetime.now().strftime("%Y%m%d")

                            # ğŸ”‘ æ–°å¢: æ ‡è®°å¼•æ“æ¥æº
                            item.setdefault("raw_data", {})
                            item["raw_data"]["engine"] = engine

                            # ğŸ”‘ æ–°å¢: æ ‡è®°æ˜¯å¦æ¥è‡ªé¡ºè—¤æ‘¸ç“œ
                            if tool_args.get("from_influencer"):
                                item["raw_data"]["from_influencer_search"] = True
                                item["raw_data"]["source_influencer"] = tool_args.get("influencer_name", "")

                            ci = ContentItem(**item)
                            new_items.append(ci)
                    except Exception:
                        pass

            if new_items:
                state.candidates.extend(new_items)
                new_count = len(new_items)

                # ğŸ”‘ æ–°å¢: æ›´æ–°å¼•æ“è¿›åº¦
                if engine in ["engine1", "engine2"]:
                    state.engine_progress[engine] = state.engine_progress.get(engine, 0) + new_count

                engine_icon = "ğŸ”´" if engine == "engine1" else "ğŸ”µ"
                print(f"   {engine_icon} +{new_count} æ¡")
                _harvest_sources(state, new_items, tool_name)
                _update_topic_progress(state, topic_hint, new_count)
                _log_collection_summary(state, tool_name, topic_hint, new_count, result.summary)
                
                # ğŸ”‘ P0: æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©å€™é€‰å†…å®¹åˆ°å¤–éƒ¨å­˜å‚¨
                _maybe_compress_candidates(state)
            else:
                _log_collection_summary(state, tool_name, topic_hint, 0, "æœªè·å–åˆ°å¯ç”¨çš„æ•°æ®")
            _mark_platform_search_done(state, tool_name)

            # ğŸ”‘ ä¿®å¤å…³é”®é—®é¢˜ 4: ç›‘æ§å®Œæˆåï¼Œæ ‡è®°ä¸ºå·²ç›‘æ§
            _mark_source_monitored(state, tool_name, tool_args)
        else:
            _log_collection_summary(state, tool_name, topic_hint, 0, f"æ‰§è¡Œå¤±è´¥: {result.summary}")

        # ğŸ”‘ æ–°å¢: æ ‡è®°ä»»åŠ¡å®Œæˆ
        if task_id:
            _mark_task_completed(state, task_id)

        return {
            "plan_status": "planning", # Go back to planner
            "plan_scratchpad": state.plan_scratchpad,
            "candidates": state.candidates,
            "leads": state.leads,
            "pending_monitors": state.pending_monitors,
            "discovered_sources": state.discovered_sources,
            "task_queue": state.task_queue,  # ğŸ”‘ æ–°å¢: è¿”å›æ›´æ–°åçš„ä»»åŠ¡é˜Ÿåˆ—
            "completed_tasks": state.completed_tasks,  # ğŸ”‘ æ–°å¢
            "engine_progress": state.engine_progress,  # ğŸ”‘ æ–°å¢
            "candidates_externalized": state.candidates_externalized  # ğŸ”‘ P0: è¿”å›å¤–éƒ¨åŒ–æ ‡è®°
        }
        
    except Exception as e:
        print(f"âŒ Execution Error: {e}")
        last_entry["tool_result"] = {"status": "error", "error": str(e)}
        
        # ğŸ”‘ P4: ä½¿ç”¨ FeedbackAnalyzer åˆ†æé”™è¯¯
        retry_suggestion = get_retry_suggestion(
            tool_name=tool_name,
            error=str(e),
            original_params=tool_args,
            state=state
        )
        
        # ğŸ”‘ P0: è®°å½•é”™è¯¯åˆ° error_historyï¼ˆManusæœ€ä½³å®è·µï¼šä¿ç•™å¤±è´¥å°è¯•ï¼‰
        error_record = {
            "tool_name": tool_name,
            "tool_args": tool_args,
            "error": str(e),
            "error_type": type(e).__name__,
            "timestamp": datetime.now().isoformat(),
            "reasoning": tool_call.get("reasoning", "")[:200],  # ä¿ç•™éƒ¨åˆ†reasoningä¾¿äºåˆ†æ
            # ğŸ”‘ P4: æ·»åŠ é‡è¯•å»ºè®®
            "retry_suggestion": retry_suggestion
        }
        state.error_history.append(error_record)
        
        # ğŸ”‘ P4: æ‰“å°é‡è¯•å»ºè®®
        if retry_suggestion.get("should_retry"):
            print(f"   ğŸ’¡ å»ºè®®: {retry_suggestion.get('reason', '')}")
            if retry_suggestion.get("adjusted_params"):
                print(f"   ğŸ”§ è°ƒæ•´å‚æ•°: {retry_suggestion.get('adjusted_params')}")
            if retry_suggestion.get("wait_seconds", 0) > 0:
                print(f"   â±ï¸ å»ºè®®ç­‰å¾…: {retry_suggestion.get('wait_seconds')}ç§’")
        else:
            print(f"   âš ï¸ ä¸å»ºè®®é‡è¯•: {retry_suggestion.get('reason', '')}")
            if retry_suggestion.get("alternative_tool"):
                print(f"   ğŸ”„ å¯å°è¯•: {retry_suggestion.get('alternative_tool')}")
        
        print(f"   ğŸ“ é”™è¯¯å·²è®°å½•åˆ° error_history (å…± {len(state.error_history)} æ¡)")
        
        return {
            "plan_status": "planning",
            "leads": state.leads,
            "pending_monitors": state.pending_monitors,
            "discovered_sources": state.discovered_sources,
            "error_history": state.error_history  # ğŸ”‘ è¿”å›æ›´æ–°åçš„é”™è¯¯å†å²
        }

def _apply_default_params(tool_name: str, tool_args: Dict[str, Any]):
    defaults = DEFAULT_PARAMS.get(tool_name)
    if not defaults:
        return
    for key, default_value in defaults.items():
        current = tool_args.get(key)
        if isinstance(default_value, (int, float)):
            if current is None or current < default_value:
                tool_args[key] = default_value
        else:
            if not current:
                tool_args[key] = default_value

def _harvest_sources(state: RadarState, items: list, source_label: Optional[str] = None):
    for ci in items:
        url = (ci.url or "").strip()
        if not url:
            continue
        lower_url = url.lower()
        
        # Track generic web domains
        domain = urlparse(url).netloc
        if domain:
            web_list = state.discovered_sources.setdefault("web", [])
            if domain not in web_list:
                web_list.append(domain)
                state.logs.append(f"ã€å‘ç°ã€‘æ–°å¢ç«™ç‚¹ {domain}")
        
        # YouTube video -> derive channel
        if ci.platform == "youtube" or "youtube.com" in lower_url:
            channel_url = None
            if ci.author_id:
                channel_url = f"https://www.youtube.com/channel/{ci.author_id}"
            else:
                channel_url = _extract_youtube_channel(lower_url)
            if channel_url:
                _enqueue_source(state, "youtube", channel_url)
        
        # Bilibili UP ä¸»
        if ci.platform == "bilibili" or "bilibili.com" in lower_url:
            mid = ci.author_id or ci.raw_data.get("author_id") if isinstance(ci.raw_data, dict) else None
            if mid:
                _enqueue_source(state, "bilibili", str(mid))

        if source_label:
            chain = ci.raw_data.get("source_chain")
            if isinstance(chain, list):
                if source_label not in chain:
                    chain.append(source_label)
            elif chain:
                ci.raw_data["source_chain"] = list({chain, source_label})
            else:
                ci.raw_data["source_chain"] = [source_label]

def _extract_youtube_channel(url: str) -> str:
    if "/channel/" in url:
        return "https://www.youtube.com" + url.split("youtube.com")[1].split("?")[0]
    if "/@" in url:
        idx = url.index("/@")
        return "https://www.youtube.com" + url[idx:].split("?")[0]
    if "/user/" in url:
        idx = url.index("/user/")
        return "https://www.youtube.com" + url[idx:].split("?")[0]
    return ""

def _enqueue_source(state: RadarState, platform: str, identifier: str):
    if not identifier:
        return
    identifier = identifier.rstrip("/")
    # Ensure dicts exist
    if platform not in state.pending_monitors:
        state.pending_monitors[platform] = []
    if platform not in state.monitoring_list:
        state.monitoring_list[platform] = []
    if platform not in state.discovered_sources:
        state.discovered_sources[platform] = []
    if platform not in state.monitored_sources:
        state.monitored_sources[platform] = []

    # ğŸ”‘ ä¿®å¤å…³é”®é—®é¢˜ 2: æ£€æŸ¥æ˜¯å¦å·²ç»ç›‘æ§è¿‡ï¼Œé¿å…é‡å¤ç›‘æ§
    if identifier in state.monitored_sources[platform]:
        return  # å·²ç»ç›‘æ§è¿‡ï¼Œè·³è¿‡

    if identifier in state.pending_monitors[platform]:
        return  # å·²ç»åœ¨å¾…ç›‘æ§é˜Ÿåˆ—ä¸­

    # ğŸ”‘ ä¿®å¤å…³é”®é—®é¢˜ 3: é™åˆ¶å¾…ç›‘æ§é˜Ÿåˆ—é•¿åº¦ï¼Œé˜²æ­¢å¤±æ§
    MAX_PENDING_PER_PLATFORM = 10
    if len(state.pending_monitors[platform]) >= MAX_PENDING_PER_PLATFORM:
        return  # é˜Ÿåˆ—å·²æ»¡ï¼Œä¸å†æ·»åŠ 

    if identifier in state.monitoring_list[platform]:
        # Already part of whitelist, ensure pending
        state.pending_monitors[platform].append(identifier)
        return
    if identifier not in state.discovered_sources[platform]:
        state.discovered_sources[platform].append(identifier)
    state.pending_monitors[platform].append(identifier)
    state.logs.append(f"ã€å‘ç°ã€‘åŠ å…¥{platform}å¾…ç›‘æ§ï¼š{identifier}")

def _update_topic_progress(state: RadarState, topic_hint: Any, delta: int):
    if delta <= 0:
        return
    topic = topic_hint or "general"
    state.topic_progress.setdefault(topic, 0)
    state.topic_progress[topic] += delta

def _log_collection_summary(state: RadarState, tool_name: str, topic_hint: Any, delta: int, summary: str):
    topic = topic_hint or "general"
    target = state.topic_targets.get(topic)
    progress = state.topic_progress.get(topic, 0)
    if target:
        msg = f"ã€é‡‡é›†ã€‘{tool_name} -> {topic} (+{delta}) ç´¯è®¡ {progress}/{target} | {summary}"
    else:
        msg = f"ã€é‡‡é›†ã€‘{tool_name} -> {topic} (+{delta}) ç´¯è®¡ {progress} | {summary}"
    state.logs.append(msg)


def _mark_platform_search_done(state: RadarState, tool_name: str):
    mapping = {
        "youtube_search": "youtube",
        "bilibili_search": "bilibili"
    }
    platform = mapping.get(tool_name)
    if platform:
        state.platform_search_progress[platform] = True


def _mark_source_monitored(state: RadarState, tool_name: str, tool_args: Dict[str, Any]):
    """
    ğŸ”‘ ä¿®å¤å…³é”®é—®é¢˜ 4: ç›‘æ§å®Œæˆåï¼Œå°†é¢‘é“æ ‡è®°ä¸ºå·²ç›‘æ§ï¼Œé¿å…é‡å¤ç›‘æ§
    """
    monitor_mapping = {
        "youtube_monitor": ("youtube", "channel_url"),
        "bilibili_monitor": ("bilibili", "user_id")
    }

    if tool_name in monitor_mapping:
        platform, arg_key = monitor_mapping[tool_name]
        identifier = tool_args.get(arg_key)
        if identifier:
            identifier = identifier.rstrip("/")
            if platform not in state.monitored_sources:
                state.monitored_sources[platform] = []
            if identifier not in state.monitored_sources[platform]:
                state.monitored_sources[platform].append(identifier)
                print(f"âœ“ æ ‡è®° {platform} é¢‘é“å·²ç›‘æ§: {identifier}")


def _ingest_leads(state: RadarState, raw_items: list, topic_hint: Any, source_tool: str) -> int:
    """
    Store generic web search hits as lightweight leads for downstream planner use.
    """
    added = 0
    seen_urls = {lead.url for lead in state.leads}
    topic = topic_hint or "general"

    for item in raw_items:
        if not isinstance(item, dict):
            continue
        title = (item.get("title") or item.get("name") or "").strip()
        url = (item.get("url") or item.get("href") or "").strip()
        snippet = (item.get("content") or item.get("summary") or item.get("description") or "").strip()
        source = item.get("source") or source_tool

        if not title and not url:
            continue
        if url and url in seen_urls:
            continue

        tags = _extract_lead_tags(title, snippet)
        lead = LeadItem(
            title=title or url,
            url=url or f"-/{hash(title)}",
            snippet=snippet[:500],
            source=source,
            topic_hint=topic,
            tags=tags
        )
        state.leads.append(lead)
        seen_urls.add(lead.url)
        added += 1

    return added


def _extract_lead_tags(title: str, snippet: str) -> List[str]:
    """
    Quick heuristics to capture potential creator names or keywords from web hits.
    """
    tags = set()
    text = f"{title} {snippet}".strip()
    if not text:
        return []

    # @handles
    tags.update(re.findall(r"@([\w\-]+)", text))
    # ã€Šä½œå“ã€‹ or â€œå¼•å·â€
    tags.update(re.findall(r"ã€Š([^ã€‹]{2,25})ã€‹", text))
    tags.update(re.findall(r"â€œ([^â€]{2,25})â€", text))

    # Split by separators to capture candidate names (limit length)
    for part in re.split(r"[|ï½œ\-â€”â€“:ï¼š]", title):
        clean = part.strip()
        if 2 <= len(clean) <= 24:
            tags.add(clean)

    return [t for t in tags if t]


def _run_quality_check(
    state: RadarState,
    tool_name: str,
    tool_args: Dict[str, Any],
    tool_result: Any,
    reasoning: str
):
    """
    è¿è¡Œè´¨é‡æ£€æŸ¥ï¼ˆæ™ºèƒ½åˆ¤æ–­ï¼‰

    æ ¹æ®å·¥å…·ç±»å‹å’Œé¢„æœŸæ„å»ºæ™ºèƒ½æ£€æŸ¥
    """
    # æ„å»ºæœŸæœ›æè¿°ï¼ˆåŸºäºreasoningå’Œå·¥å…·ç±»å‹ï¼‰
    expectation = _build_expectation(tool_name, tool_args, reasoning, state)

    # æ„å»ºä¸Šä¸‹æ–‡
    context = {
        "target_domains": state.target_domains,
        "current_candidates_count": len(state.candidates),
        "current_phase": state.current_phase,
        "recent_quality_checks": state.quality_checks[-3:] if len(state.quality_checks) > 0 else []
    }

    # è°ƒç”¨è´¨é‡é—¨
    try:
        quality_result = _quality_gate.check_quality(
            tool_name=tool_name,
            tool_params=tool_args,
            tool_result=tool_result,
            expectation=expectation,
            context=context
        )
        return quality_result
    except Exception as e:
        print(f"   âš ï¸ è´¨é‡æ£€æŸ¥å¼‚å¸¸: {e}")
        # è¿”å›é»˜è®¤é€šè¿‡
        from core.quality_gate import QualityCheckResult
        return QualityCheckResult(
            passed=True,
            confidence=0.5,
            score=0.7,
            suggested_action="continue",
            reasoning=f"è´¨é‡æ£€æŸ¥å¼‚å¸¸ï¼Œé»˜è®¤é€šè¿‡: {e}"
        )


def _build_expectation(
    tool_name: str,
    tool_args: Dict[str, Any],
    reasoning: str,
    state: RadarState
) -> str:
    """
    æ™ºèƒ½æ„å»ºæœŸæœ›æè¿°ï¼ˆé€šç”¨åŒ–ï¼‰

    åŸºäºå·¥å…·ç±»å‹ã€å‚æ•°å’Œreasoningè‡ªåŠ¨ç”ŸæˆæœŸæœ›
    """
    # æå–ä¸»é¢˜ï¼ˆä»keywordæˆ–queryå‚æ•°ï¼‰
    topic = tool_args.get("keyword") or tool_args.get("query") or ""

    # ä»reasoningä¸­æå–å¼•æ“å’Œä»»åŠ¡ç±»å‹
    if "å‘ç°åšä¸»" in reasoning or "discovery" in reasoning.lower():
        task_type = "å‘ç°ç›¸å…³åšä¸»"
    elif "é¡ºè—¤æ‘¸ç“œ" in reasoning or "influencer" in reasoning.lower():
        task_type = f"æœç´¢åšä¸»'{tool_args.get('influencer_name', 'æœªçŸ¥')}' çš„ç›¸å…³å†…å®¹"
    elif "ç›‘æ§" in reasoning or "monitor" in reasoning.lower():
        task_type = "ç›‘æ§åšä¸»çš„æœ€æ–°è§†é¢‘"
    else:
        task_type = "æœç´¢ç›¸å…³è§†é¢‘å†…å®¹"

    # æ„å»ºæœŸæœ›
    if "web_search" in tool_name:
        return f"{task_type}ï¼ŒæœŸæœ›æ‰¾åˆ°æ¨èä¼˜è´¨åšä¸»çš„æ–‡ç« ï¼Œä¸»é¢˜: {topic}"
    elif "search" in tool_name:
        platform = "YouTube" if "youtube" in tool_name else "Bilibili"
        return f"åœ¨{platform}ä¸Š{task_type}ï¼ŒæœŸæœ›è¿”å›é«˜è´¨é‡ã€ç›¸å…³æ€§å¼ºçš„è§†é¢‘ï¼Œä¸»é¢˜: {topic}"
    elif "monitor" in tool_name:
        platform = "YouTube" if "youtube" in tool_name else "Bilibili"
        return f"ç›‘æ§{platform}åšä¸»çš„æœ€æ–°å†…å®¹ï¼ŒæœŸæœ›è¿”å›è¯¥åšä¸»çš„è¿‘æœŸè§†é¢‘"
    else:
        return f"{task_type}ï¼Œä¸»é¢˜: {topic}"


def _extract_task_id(reasoning: str) -> Optional[str]:
    """ä»reasoningä¸­æå–ä»»åŠ¡ID"""
    # æ ¼å¼: [task_id] reasoning...
    match = re.match(r"\[([^\]]+)\]", reasoning)
    if match:
        return match.group(1)
    return None


def _extract_engine(reasoning: str) -> str:
    """ä»reasoningä¸­æå–å¼•æ“æ ‡è¯†"""
    if "engine1" in reasoning.lower() or "å¼•æ“1" in reasoning or "é¡ºè—¤æ‘¸ç“œ" in reasoning:
        return "engine1"
    elif "engine2" in reasoning.lower() or "å¼•æ“2" in reasoning or "å…³é”®è¯æœç´¢" in reasoning:
        return "engine2"
    return "unknown"


def _mark_task_completed(state: RadarState, task_id: str):
    """æ ‡è®°ä»»åŠ¡ä¸ºå·²å®Œæˆ"""
    for task in state.task_queue:
        if task.task_id == task_id:
            task.status = "completed"
            if task_id not in state.completed_tasks:
                state.completed_tasks.append(task_id)
            break


def _maybe_compress_candidates(state: RadarState):
    """
    ğŸ”‘ P0: æ£€æŸ¥å¹¶å‹ç¼©å€™é€‰å†…å®¹åˆ°å¤–éƒ¨å­˜å‚¨
    
    å½“å€™é€‰å†…å®¹è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œå°†å®Œæ•´æ•°æ®å­˜å‚¨åˆ°æ–‡ä»¶ç³»ç»Ÿï¼Œ
    å†…å­˜ä¸­åªä¿ç•™è½»é‡å¼•ç”¨ï¼Œå‡å°‘ LLM ä¸Šä¸‹æ–‡è´Ÿæ‹…ã€‚
    """
    if state.candidates_externalized:
        # å·²ç»å‹ç¼©è¿‡ï¼Œä¸é‡å¤å¤„ç†
        return
    
    if len(state.candidates) < CANDIDATES_COMPRESS_THRESHOLD:
        # æœªè¾¾åˆ°é˜ˆå€¼ï¼Œä¸å‹ç¼©
        return
    
    try:
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼ˆFileMemory éœ€è¦å­—å…¸æ ¼å¼ï¼‰
        candidates_dict = [c.model_dump() for c in state.candidates]
        
        # å‹ç¼©å¹¶å­˜å‚¨
        compressed, was_compressed = compress_candidates_if_needed(
            candidates_dict, 
            threshold=CANDIDATES_COMPRESS_THRESHOLD
        )
        
        if was_compressed:
            # æ›´æ–°çŠ¶æ€ï¼šç”¨å‹ç¼©å¼•ç”¨æ›¿æ¢å®Œæ•´æ•°æ®
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿ç•™åŸå§‹ ContentItem å¯¹è±¡ï¼Œä½†æ ‡è®°å·²å¤–éƒ¨åŒ–
            # å®Œæ•´æ•°æ®å·²å­˜å‚¨åˆ° data/memory/candidates/
            state.candidates_externalized = True
            print(f"   ğŸ’¾ å€™é€‰å†…å®¹å·²å¤–éƒ¨åŒ–å­˜å‚¨ ({len(state.candidates)} æ¡)")
            state.logs.append(f"ã€å­˜å‚¨ã€‘{len(state.candidates)} æ¡å€™é€‰å†…å®¹å·²å¤–éƒ¨åŒ–åˆ°æ–‡ä»¶ç³»ç»Ÿ")
    except Exception as e:
        # å‹ç¼©å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        print(f"   âš ï¸ å¤–éƒ¨åŒ–å­˜å‚¨å¤±è´¥: {e}")


def get_error_summary_for_planner(state: RadarState, limit: int = 3) -> str:
    """
    ğŸ”‘ P0: ç”Ÿæˆé”™è¯¯æ‘˜è¦ä¾› Planner å‚è€ƒ
    
    è¿”å›æœ€è¿‘çš„é”™è¯¯è®°å½•æ‘˜è¦ï¼Œå¸®åŠ© LLM é¿å…é‡å¤çŠ¯é”™ã€‚
    """
    if not state.error_history:
        return ""
    
    recent_errors = state.error_history[-limit:]
    summary_lines = ["ã€æœ€è¿‘é”™è¯¯è®°å½•ã€‘"]
    
    for err in recent_errors:
        tool = err.get("tool_name", "unknown")
        error_type = err.get("error_type", "Error")
        error_msg = err.get("error", "")[:100]
        summary_lines.append(f"- {tool}: {error_type} - {error_msg}")
    
    summary_lines.append("è¯·é¿å…é‡å¤ä¸Šè¿°å¤±è´¥çš„æ“ä½œã€‚")
    return "\n".join(summary_lines)


# ============ P3: Reducer è¾…åŠ©å‡½æ•° ============

def _dedupe_candidates(existing: List[ContentItem], new_items: List[ContentItem]) -> List[ContentItem]:
    """
    ğŸ”‘ P3: ä½¿ç”¨ Reducer æ¨¡å¼å»é‡å€™é€‰å†…å®¹
    
    æŒ‰ URL å»é‡ï¼Œé¿å…é‡å¤æ·»åŠ ç›¸åŒå†…å®¹
    """
    existing_urls = set(c.url for c in existing)
    unique_new = [item for item in new_items if item.url not in existing_urls]
    return unique_new


def _safe_extend_candidates(state: RadarState, new_items: List[ContentItem]) -> int:
    """
    ğŸ”‘ P3: å®‰å…¨æ‰©å±•å€™é€‰å†…å®¹åˆ—è¡¨
    
    ä½¿ç”¨ Reducer æ¨¡å¼ï¼š
    1. è‡ªåŠ¨å»é‡
    2. è¿”å›å®é™…æ·»åŠ æ•°é‡
    """
    unique_items = _dedupe_candidates(state.candidates, new_items)
    state.candidates.extend(unique_items)
    return len(unique_items)


def _safe_append_error(state: RadarState, error_record: Dict[str, Any], max_errors: int = 50):
    """
    ğŸ”‘ P3: å®‰å…¨è¿½åŠ é”™è¯¯è®°å½•
    
    ä½¿ç”¨ capped_append_reducer æ¨¡å¼ï¼Œé™åˆ¶æœ€å¤§æ•°é‡
    """
    state.error_history = capped_append_reducer(
        state.error_history, 
        [error_record], 
        max_size=max_errors
    )


def _safe_merge_progress(state: RadarState, engine: str, count: int):
    """
    ğŸ”‘ P3: å®‰å…¨åˆå¹¶å¼•æ“è¿›åº¦
    
    ä½¿ç”¨ merge_dict_reducer æ¨¡å¼
    """
    current = state.engine_progress.get(engine, 0)
    state.engine_progress = merge_dict_reducer(
        state.engine_progress,
        {engine: current + count}
    )
